# **实验 1：验证视觉信息在多语言表征对齐中的因果作用**

## **项目背景**

在多语言视觉-语言模型（Multilingual Vision-Language Models，简称 MLLMs）中，视觉信息是否能显著增强并对齐不同语言的表征是一个重要问题。本实验旨在验证图像作为外部证据锚点，如何通过视觉信息增强并对齐多语言文本中的表征，特别是验证 **视觉信息** 如何影响不同语言对同一具体概念的对齐效果。

## **实验目标**

**核心假设：**
视觉信息（图像）作为一种外部证据锚点，能够因果性地增强并对齐不同语言（如中英文）中对同一具体概念的表征。我们假设这种对齐是选择性的，主要作用于可被视觉锚定的概念（如物体、属性）。我们通过三组对照实验来验证这一假设。

1. **验证目标：**

   * 提供正确的视觉上下文（图像）是否能显著拉近多语言概念的表征距离。
   * 提供错误的视觉上下文（不相关的图像）是否能破坏或减弱对齐效果。
   * 计算不同条件下的 **余弦距离**（Cosine Distance）来量化对齐效果。

2. **实验设计：**
   实验将使用多模态大语言模型（如 LLaVA），并以 COCO 2017 数据集中的图像和多语言文本为基础，进行以下实验设计：

   * **三组对照实验：**

     * **无锚定基线（Baseline Condition）：** 仅使用文本数据进行对齐计算。
     * **正确锚定（Correct Anchoring）：** 使用图像和文本输入，计算文本和图像的对齐效果。
     * **错误锚定（Mismatched Anchoring）：** 使用不相关的图像和文本进行计算，测试是否图像的语义内容影响对齐效果。

## **项目结构与模块化**

为了确保代码的可维护性和可扩展性，项目分为以下几个模块：

### **1. 数据准备模块（`/data`）**

* **功能：** 负责加载图像数据（如 COCO 数据集），筛选目标物体类别（如狗、汽车），并提供多语言的描述文本。支持英文到目标语言（如中文）的自动翻译。

### **2. 模型加载模块（`/models`）**

* **功能：** 使用 Hugging Face 提供的 `from_pretrained` 方法加载预训练的 LLaVA 模型及其处理器，并确保模型能够支持图像和多语言文本的输入。

### **3. 实验设计与执行模块（`/experiment`）**

* **功能：** 负责控制实验流程，包括创建三组对照实验（基线、正确锚定、错误锚定），并计算每个实验组的对齐效果（使用余弦距离度量）。

### **4. 结果分析与评估模块（`/analysis`）**

* **功能：** 对实验结果进行统计分析，计算每组实验的余弦距离并进行显著性检验。

### **5. 可视化模块（`/visualization`）**

* **功能：** 将实验结果进行可视化，绘制图表展示不同实验组之间的对齐效果差异。

### **6. 主程序（`main.py`）**

* **功能：** 主程序负责协调所有模块的执行，从配置文件中读取设置并控制实验的整体执行。

## **实验设计**

### **1. 实验组设计**

* **无锚定基线（Baseline Condition）：**

  * 不使用图像，仅根据文本描述计算英文和中文之间的余弦距离。
* **正确锚定（Correct Anchoring）：**

  * 使用图像和对应的文本描述，计算图像和多语言描述之间的余弦距离。
* **错误锚定（Mismatched Anchoring）：**

  * 使用图像和无关的文本描述（如汽车图像与“狗”的描述），计算余弦距离。

### **2. 输入数据**

* **图像数据：** 使用 COCO 2017 数据集中的图像。
* **文本描述：** 为每个图像提供多语言描述（例如，英文和中文）。
  这些文本描述将由 `captions_dict` 提供，格式如下：

  ```python
  captions_dict = {
      'en': "A dog is playing in the park.",
      'zh': "一只狗在公园里玩耍。",
      'fr': "Un chien joue dans le parc."
  }
  ```

### **3. 输出指标**

* **余弦距离（Cosine Distance）：**
  余弦距离作为衡量对齐效果的标准，距离越小表示表征越接近，对齐效果越好。
* **统计显著性：**
  使用 t 检验（或其他统计方法）对不同实验组的对齐效果进行显著性分析。

## **TODO List**

### **1. 数据准备**

* [ ] **加载图像数据：** 从 COCO 2017 数据集加载图像。
* [ ] **筛选目标物体类别：** 选择如 `dog`、`car` 等具体物体进行实验。
* [ ] **多语言描述生成：**

  * 通过翻译 API（如 DeepL 或 Google Translate）翻译英文描述成其他语言（如中文、法文）。
  * 确保不同语言的翻译语义一致性。

### **2. 模型加载**

* [ ] **加载 LLaVA 模型：** 使用 Hugging Face 的 `from_pretrained` 方法加载 LLaVA 或其他多模态模型。
* [ ] **处理图像和文本输入：** 配置模型的处理器，以支持图像和多语言文本输入。

### **3. 实验设计**

* [ ] **无锚定基线实验：** 计算并返回基线实验组中多语言描述的余弦距离。
* [ ] **正确锚定实验：** 计算图像和多语言描述之间的余弦距离。
* [ ] **错误锚定实验：** 使用不相关图像和文本计算余弦距离。

### **4. 结果分析**

* [ ] **计算余弦距离：** 在每个实验组中计算图像和文本之间的余弦距离。
* [ ] **统计显著性：** 使用 t 检验等方法分析不同实验组之间的显著性差异。

### **5. 可视化**

* [ ] **绘制结果图：** 使用 Matplotlib 绘制不同实验组之间的对齐效果图表。

### **6. 文档与报告**

* [ ] **撰写实验报告：** 总结实验方法、数据、结果和分析。
* [ ] **代码注释与文档：** 适当注释代码，确保代码的可读性与可维护性。

---

## **实验执行流程**

1. **数据加载与预处理：**

   * 加载图像和文本描述数据。
   * 使用翻译 API 进行多语言翻译。

2. **模型加载：**

   * 使用预训练的 LLaVA 模型进行推理。

3. **实验执行：**

   * 运行三组对照实验（基线、正确锚定、错误锚定）。

4. **结果分析：**

   * 计算余弦距离，分析对齐效果。

5. **结果可视化：**

   * 绘制图表并展示实验结果。

6. **实验报告：**

   * 撰写实验报告，分析结果并得出结论。

---

### **最终期望结果**

我们预期会观察到一个清晰的、具有统计显著性的排序：

* **Dist_anchored < Dist_baseline < Dist_mismatched**

  * **Dist_anchored < Dist_baseline**：证明正确的视觉上下文可以显著增强多语言概念的对齐。
  * **Dist_baseline < Dist_mismatched**：证明错误的视觉上下文（与文本不相关的图像）不仅无法增强对齐，反而会破坏对齐效果。

---

## **结语**

这个实验将帮助我们深入理解视觉信息在多语言表征对齐中的作用，尤其是验证视觉内容是否仅在语义一致时才有效地锚定和组织多语言表征。通过精心设计的对照实验，我们能够清晰地展示视觉信息对多语言模型的因果影响。
