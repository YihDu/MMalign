===== GPU MONITOR START =====
TIME: Thu Nov 13 09:12:30 PM CST 2025

----------------------------------------
TIME: Thu Nov 13 09:12:31 PM CST 2025
Thu Nov 13 21:12:31 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   32C    P0             76W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   32C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0          -     -      -      -      -      -      -      -    -              
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:12:34 PM CST 2025
Thu Nov 13 21:12:34 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   32C    P0             76W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   32C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0          -     -      -      -      -      -      -      -    -              
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:12:39 PM CST 2025
Thu Nov 13 21:12:39 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   32C    P0             76W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   32C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0          -     -      -      -      -      -      -      -    -              
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:12:43 PM CST 2025
Thu Nov 13 21:12:43 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   32C    P0             76W /  700W |       4MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   32C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C      -      -      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:12:47 PM CST 2025
Thu Nov 13 21:12:48 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   33C    P0            119W /  700W |   10998MiB / 143771MiB |      8%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   32C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                13862MiB |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C      7      0      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:12:53 PM CST 2025
Thu Nov 13 21:12:53 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   34C    P0            119W /  700W |   16590MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   32C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                16580MiB |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C      0      0      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:12:57 PM CST 2025
Thu Nov 13 21:12:57 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   34C    P0            119W /  700W |   16590MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   32C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                16580MiB |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C      0      0      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:13:01 PM CST 2025
Thu Nov 13 21:13:01 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   34C    P0            119W /  700W |   16590MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   32C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                16580MiB |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C      0      0      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:13:07 PM CST 2025
Thu Nov 13 21:13:07 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   34C    P0            119W /  700W |   16590MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   32C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                16580MiB |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C      0      0      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:13:11 PM CST 2025
Thu Nov 13 21:13:11 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   34C    P0            119W /  700W |   16590MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   32C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                16580MiB |
+-----------------------------------------------------------------------------------------+


===== Batch 0 =====
QIDs: [669, 669, 669, 669, 669, 669, 669, 669, 669, 669]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [531, 245, 491, 536, 450, 520, 529, 166, 561, 567]
image sizes: [(276, 269), (276, 269), (276, 269), (276, 269), (276, 269), (276, 269), (276, 269), (276, 269), (276, 269), (276, 269)]
input_ids shape: torch.Size([10, 394])
attention_mask shape: torch.Size([10, 394])

Processes:
vision_start count = 10, vision_end count = 10
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C      0      0      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              
NaN in input_ids: False
Sequence length = 394

----------------------------------------
TIME: Thu Nov 13 09:13:15 PM CST 2025
Thu Nov 13 21:13:15 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   36C    P0            203W /  700W |   19574MiB / 143771MiB |     64%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+


===== Batch 1 =====
QIDs: [1356, 1356, 1356, 1356, 1356, 1356, 1356, 1356, 1356, 1356]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [186, 105, 157, 191, 129, 162, 167, 101, 177, 233]
image sizes: [(490, 600), (490, 600), (490, 600), (490, 600), (490, 600), (490, 600), (490, 600), (490, 600), (490, 600), (490, 600)]
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   32C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                19564MiB |
+-----------------------------------------------------------------------------------------+
input_ids shape: torch.Size([10, 500])
attention_mask shape: torch.Size([10, 500])

Processes:
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 500
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C      3      0      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              



===== Batch 2 =====
QIDs: [677, 677, 677, 677, 677, 677, 677, 677, 677, 677]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [486, 196, 440, 468, 371, 417, 438, 141, 438, 449]
image sizes: [(488, 532), (488, 532), (488, 532), (488, 532), (488, 532), (488, 532), (488, 532), (488, 532), (488, 532), (488, 532)]
input_ids shape: torch.Size([10, 594])
attention_mask shape: torch.Size([10, 594])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 594
----------------------------------------
TIME: Thu Nov 13 09:13:19 PM CST 2025


===== Batch 3 =====
QIDs: [736, 736, 736, 736, 736, 736, 736, 736, 736, 736]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [330, 168, 338, 354, 305, 328, 266, 120, 387, 368]
image sizes: [(470, 366), (470, 366), (470, 366), (470, 366), (470, 366), (470, 366), (470, 366), (470, 366), (470, 366), (470, 366)]
Thu Nov 13 21:13:19 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   35C    P0            120W /  700W |   24300MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
input_ids shape: torch.Size([10, 417])
attention_mask shape: torch.Size([10, 417])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 417
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   32C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                24290MiB |
+-----------------------------------------------------------------------------------------+


===== Batch 4 =====
QIDs: [52, 52, 52, 52, 52, 52, 52, 52, 52, 52]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [240, 130, 236, 223, 228, 213, 207, 94, 232, 216]
image sizes: [(1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560)]

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C      9      1      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:13:25 PM CST 2025
Thu Nov 13 21:13:25 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   35C    P0            119W /  700W |   24300MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
input_ids shape: torch.Size([10, 6432])
attention_mask shape: torch.Size([10, 6432])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 6432
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   32C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                25800MiB |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C     33     11      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:13:29 PM CST 2025
Thu Nov 13 21:13:30 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   54C    P0            687W /  700W |   46656MiB / 143771MiB |     98%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   32C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                46646MiB |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C     93     31      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:13:32 PM CST 2025
Thu Nov 13 21:13:33 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   38C    P0            122W /  700W |   65312MiB / 143771MiB |      9%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   32C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                65302MiB |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C      9      0      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:13:37 PM CST 2025
Thu Nov 13 21:13:38 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   37C    P0            121W /  700W |   65312MiB / 143771MiB |     10%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   32C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                65302MiB |
+-----------------------------------------------------------------------------------------+


===== Batch 5 =====
QIDs: [610, 610, 610, 610, 610, 610, 610, 610, 610, 610]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']

text lens: [707, 621, 724, 716, 680, 779, 691, 581, 736, 719]
Processes:
image sizes: [(750, 271), (750, 271), (750, 271), (750, 271), (750, 271), (750, 271), (750, 271), (750, 271), (750, 271), (750, 271)]
input_ids shape: torch.Size([10, 849])
attention_mask shape: torch.Size([10, 849])
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
vision_start count = 10, vision_end count = 10
#NaN in input_ids: False
 Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C      8      0      -      -      -      -    python         
Sequence length = 849
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:13:41 PM CST 2025


===== Batch 6 =====
QIDs: [1129, 1129, 1129, 1129, 1129, 1129, 1129, 1129, 1129, 1129]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [225, 130, 249, 264, 240, 253, 259, 104, 277, 253]
image sizes: [(1166, 622), (1166, 622), (1166, 622), (1166, 622), (1166, 622), (1166, 622), (1166, 622), (1166, 622), (1166, 622), (1166, 622)]
Thu Nov 13 21:13:42 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   36C    P0            120W /  700W |   67776MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   32C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                67766MiB |
+-----------------------------------------------------------------------------------------+
input_ids shape: torch.Size([10, 1100])
attention_mask shape: torch.Size([10, 1100])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1100

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C     20      7      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              



===== Batch 7 =====
QIDs: [27, 27, 27, 27, 27, 27, 27, 27, 27, 27]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [397, 238, 416, 404, 389, 378, 351, 180, 414, 408]
image sizes: [(375, 184), (375, 184), (375, 184), (375, 184), (375, 184), (375, 184), (375, 184), (375, 184), (375, 184), (375, 184)]
----------------------------------------
input_ids shape: torch.Size([10, 347])
attention_mask shape: torch.Size([10, 347])
vision_start count = 10, vision_end count = 10
TIME: Thu Nov 13 09:13:46 PM CST 2025
NaN in input_ids: False
Sequence length = 347
Thu Nov 13 21:13:46 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   38C    P0            198W /  700W |   67776MiB / 143771MiB |     20%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+


===== Batch 8 =====
QIDs: [557, 557, 557, 557, 557, 557, 557, 557, 557, 557]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [244, 192, 241, 243, 266, 238, 241, 162, 270, 245]
image sizes: [(243, 229), (243, 229), (243, 229), (243, 229), (243, 229), (243, 229), (243, 229), (243, 229), (243, 229), (243, 229)]
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   32C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
input_ids shape: torch.Size([10, 263])
attention_mask shape: torch.Size([10, 263])
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 263
|    0   N/A  N/A          562495      C   python                                67766MiB |
+-----------------------------------------------------------------------------------------+

Processes:


===== Batch 9 =====
QIDs: [981, 981, 981, 981, 981, 981, 981, 981, 981, 981]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [123, 71, 121, 126, 108, 123, 97, 48, 123, 108]
image sizes: [(340, 292), (340, 292), (340, 292), (340, 292), (340, 292), (340, 292), (340, 292), (340, 292), (340, 292), (340, 292)]
input_ids shape: torch.Size([10, 203])
attention_mask shape: torch.Size([10, 203])
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      % vision_start count = 10, vision_end count = 10
     %      %      %      %      %    name 
    0     562495     C     11      2      -      -      -      -    python         
 NaN in input_ids: False
Sequence length = 203
   1          -     -      -      - 

===== Batch 10 =====
QIDs: [1202, 1202, 1202, 1202, 1202, 1202, 1202, 1202, 1202, 1202]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [592, 424, 582, 600, 474, 554, 502, 347, 625, 597]
image sizes: [(889, 101), (889, 101), (889, 101), (889, 101), (889, 101), (889, 101), (889, 101), (889, 101), (889, 101), (889, 101)]
     -      -      -      -    -              
input_ids shape: torch.Size([10, 510])

attention_mask shape: torch.Size([10, 510])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 510


===== Batch 11 =====
QIDs: [335, 335, 335, 335, 335, 335, 335, 335, 335, 335]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [264, 149, 264, 270, 270, 267, 262, 115, 291, 276]
image sizes: [(308, 480), (308, 480), (308, 480), (308, 480), (308, 480), (308, 480), (308, 480), (308, 480), (308, 480), (308, 480)]
input_ids shape: torch.Size([10, 361])
attention_mask shape: torch.Size([10, 361])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 361
----------------------------------------


===== Batch 12 =====
QIDs: [1725, 1725, 1725, 1725, 1725, 1725, 1725, 1725, 1725, 1725]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [830, 460, 853, 977, 742, 869, 680, 263, 922, 922]
image sizes: [(1282, 1180), (1282, 1180), (1282, 1180), (1282, 1180), (1282, 1180), (1282, 1180), (1282, 1180), (1282, 1180), (1282, 1180), (1282, 1180)]
TIME: Thu Nov 13 09:13:52 PM CST 2025
Thu Nov 13 21:13:52 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   36C    P0            120W /  700W |   67778MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
input_ids shape: torch.Size([10, 2374])
attention_mask shape: torch.Size([10, 2374])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2374
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   32C    P0             73W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                67768MiB |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C     50     18      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:13:56 PM CST 2025
Thu Nov 13 21:13:56 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   37C    P0            122W /  700W |   67778MiB / 143771MiB |      9%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   32C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                67768MiB |
+-----------------------------------------------------------------------------------------+


===== Batch 13 =====
QIDs: [78, 78, 78, 78, 78, 78, 78, 78, 78, 78]

Processes:
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [696, 479, 676, 704, 620, 704, 655, 406, 664, 712]
image sizes: [(652, 177), (652, 177), (652, 177), (652, 177), (652, 177), (652, 177), (652, 177), (652, 177), (652, 177), (652, 177)]
input_ids shape: torch.Size([10, 623])
attention_mask shape: torch.Size([10, 623])
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      % vision_start count = 10, vision_end count = 10
     % NaN in input_ids: False
Sequence length = 623
     %      %    name 
    0     562495     C      7      0      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              



===== Batch 14 =====
QIDs: [1108, 1108, 1108, 1108, 1108, 1108, 1108, 1108, 1108, 1108]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [706, 401, 684, 713, 613, 690, 694, 275, 705, 761]
image sizes: [(766, 205), (766, 205), (766, 205), (766, 205), (766, 205), (766, 205), (766, 205), (766, 205), (766, 205), (766, 205)]
----------------------------------------
TIME: Thu Nov 13 09:14:00 PM CST 2025
input_ids shape: torch.Size([10, 648])
attention_mask shape: torch.Size([10, 648])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 648
Thu Nov 13 21:14:01 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   39C    P0            278W /  700W |   67778MiB / 143771MiB |     71%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   32C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                67768MiB |
+-----------------------------------------------------------------------------------------+


===== Batch 15 =====
QIDs: [709, 709, 709, 709, 709, 709, 709, 709, 709, 709]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [571, 219, 510, 546, 443, 503, 508, 169, 575, 540]
image sizes: [(1218, 760), (1218, 760), (1218, 760), (1218, 760), (1218, 760), (1218, 760), (1218, 760), (1218, 760), (1218, 760), (1218, 760)]

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C     20      5      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              
input_ids shape: torch.Size([10, 1488])

attention_mask shape: torch.Size([10, 1488])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1488
----------------------------------------
TIME: Thu Nov 13 09:14:05 PM CST 2025
Thu Nov 13 21:14:05 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   37C    P0            122W /  700W |   67778MiB / 143771MiB |     10%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+


===== Batch 16 =====
QIDs: [1672, 1672, 1672, 1672, 1672, 1672, 1672, 1672, 1672, 1672]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [513, 260, 437, 442, 405, 430, 434, 160, 499, 458]
image sizes: [(302, 208), (302, 208), (302, 208), (302, 208), (302, 208), (302, 208), (302, 208), (302, 208), (302, 208), (302, 208)]
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   32C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                67768MiB |
input_ids shape: torch.Size([10, 355])
attention_mask shape: torch.Size([10, 355])
vision_start count = 10, vision_end count = 10
+-----------------------------------------------------------------------------------------+
NaN in input_ids: False
Sequence length = 355

Processes:


===== Batch 17 =====
QIDs: [175, 175, 175, 175, 175, 175, 175, 175, 175, 175]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [309, 145, 316, 285, 283, 295, 281, 116, 316, 329]
image sizes: [(1024, 324), (1024, 324), (1024, 324), (1024, 324), (1024, 324), (1024, 324), (1024, 324), (1024, 324), (1024, 324), (1024, 324)]
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C     11      2      -      -      -      -    python         input_ids shape: torch.Size([10, 629])
attention_mask shape: torch.Size([10, 629])

    1          -     - vision_start count = 10, vision_end count = 10
     - NaN in input_ids: False
Sequence length = 629
     -      -      -      -      -    -              



===== Batch 18 =====
QIDs: [19, 19, 19, 19, 19, 19, 19, 19, 19, 19]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1314, 871, 1406, 1382, 1242, 1398, 1239, 685, 1526, 1477]
image sizes: [(736, 145), (736, 145), (736, 145), (736, 145), (736, 145), (736, 145), (736, 145), (736, 145), (736, 145), (736, 145)]
input_ids shape: torch.Size([10, 1022])
attention_mask shape: torch.Size([10, 1022])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1022
----------------------------------------
TIME: Thu Nov 13 09:14:11 PM CST 2025
Thu Nov 13 21:14:11 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   38C    P0            332W /  700W |   67778MiB / 143771MiB |      9%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   32C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                67768MiB |
+-----------------------------------------------------------------------------------------+


===== Batch 19 =====
QIDs: [1406, 1406, 1406, 1406, 1406, 1406, 1406, 1406, 1406, 1406]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [313, 260, 302, 304, 276, 309, 290, 236, 308, 315]
image sizes: [(407, 207), (407, 207), (407, 207), (407, 207), (407, 207), (407, 207), (407, 207), (407, 207), (407, 207), (407, 207)]

Processes:
input_ids shape: torch.Size([10, 350])
attention_mask shape: torch.Size([10, 350])
# gpu         pid   type     sm    mem    enc    dec vision_start count = 10, vision_end count = 10
   jpg NaN in input_ids: False
Sequence length = 350
   ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C     12      1      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              



===== Batch 20 =====
QIDs: [1066, 1066, 1066, 1066, 1066, 1066, 1066, 1066, 1066, 1066]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [2947, 1392, 2840, 3177, 2528, 2802, 2385, 884, 3017, 3119]
image sizes: [(866, 532), (866, 532), (866, 532), (866, 532), (866, 532), (866, 532), (866, 532), (866, 532), (866, 532), (866, 532)]
input_ids shape: torch.Size([10, 1972])
attention_mask shape: torch.Size([10, 1972])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1972
----------------------------------------
TIME: Thu Nov 13 09:14:15 PM CST 2025
Thu Nov 13 21:14:15 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   38C    P0            454W /  700W |   67778MiB / 143771MiB |     10%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             73W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                67768MiB |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C      9      0      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              


===== Batch 21 =====

QIDs: [1326, 1326, 1326, 1326, 1326, 1326, 1326, 1326, 1326, 1326]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [188, 102, 181, 189, 199, 196, 163, 87, 228, 169]
image sizes: [(1192, 1276), (1192, 1276), (1192, 1276), (1192, 1276), (1192, 1276), (1192, 1276), (1192, 1276), (1192, 1276), (1192, 1276), (1192, 1276)]
input_ids shape: torch.Size([10, 2113])
attention_mask shape: torch.Size([10, 2113])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2113
----------------------------------------
TIME: Thu Nov 13 09:14:20 PM CST 2025
Thu Nov 13 21:14:21 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   39C    P0            363W /  700W |   67778MiB / 143771MiB |     10%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                67768MiB |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu 

===== Batch 22 =====
        pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name QIDs: [719, 719, 719, 719, 719, 719, 719, 719, 719, 719]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [306, 173, 336, 388, 293, 357, 323, 129, 361, 355]

    0     562495     C      9      0      -      -      -      -    python         
    1          -     -      -      -      -      -      -      - image sizes: [(500, 441), (500, 441), (500, 441), (500, 441), (500, 441), (500, 441), (500, 441), (500, 441), (500, 441), (500, 441)]
   -              
input_ids shape: torch.Size([10, 502])

attention_mask shape: torch.Size([10, 502])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 502


===== Batch 23 =====
QIDs: [242, 242, 242, 242, 242, 242, 242, 242, 242, 242]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [246, 142, 201, 245, 203, 220, 216, 112, 265, 263]
image sizes: [(636, 368), (636, 368), (636, 368), (636, 368), (636, 368), (636, 368), (636, 368), (636, 368), (636, 368), (636, 368)]
input_ids shape: torch.Size([10, 449])
attention_mask shape: torch.Size([10, 449])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 449
----------------------------------------
TIME: Thu Nov 13 09:14:26 PM CST 2025
Thu Nov 13 21:14:26 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   38C    P0            258W /  700W |   67778MiB / 143771MiB |      9%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+


===== Batch 24 =====
QIDs: [1507, 1507, 1507, 1507, 1507, 1507, 1507, 1507, 1507, 1507]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [2042, 1075, 1913, 2202, 1972, 1986, 1831, 659, 2281, 2127]
image sizes: [(1022, 649), (1022, 649), (1022, 649), (1022, 649), (1022, 649), (1022, 649), (1022, 649), (1022, 649), (1022, 649), (1022, 649)]
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                67768MiB |
+-----------------------------------------------------------------------------------------+

Processes:
input_ids shape: torch.Size([10, 1798])
attention_mask shape: torch.Size([10, 1798])
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx vision_start count = 10, vision_end count = 10
          # NaN in input_ids: False
Sequence length = 1798
   C/G      %      %      %      %      %      %    name 
    0     562495     C      9      1      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:14:31 PM CST 2025
Thu Nov 13 21:14:31 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   37C    P0            121W /  700W |   67778MiB / 143771MiB |     10%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             73W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                67768MiB |
+-----------------------------------------------------------------------------------------+


===== Batch 25 =====
QIDs: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500]

Processes:
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1025, 486, 954, 924, 794, 929, 923, 331, 1028, 903]
image sizes: [(370, 306), (370, 306), (370, 306), (370, 306), (370, 306), (370, 306), (370, 306), (370, 306), (370, 306), (370, 306)]
input_ids shape: torch.Size([10, 684])
# gpu         pid   type     sm    mem    enc    dec attention_mask shape: torch.Size([10, 684])
   jpg vision_start count = 10, vision_end count = 10
   ofa    command 
# Idx           #    C/G      %      %      %      %      %      % NaN in input_ids: False
Sequence length = 684
   name 
    0     562495     C      6      0      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              



===== Batch 26 =====
QIDs: [509, 509, 509, 509, 509, 509, 509, 509, 509, 509]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [259, 206, 250, 283, 230, 249, 230, 179, 281, 278]
image sizes: [(562, 226), (562, 226), (562, 226), (562, 226), (562, 226), (562, 226), (562, 226), (562, 226), (562, 226), (562, 226)]
----------------------------------------
input_ids shape: torch.Size([10, 370])
attention_mask shape: torch.Size([10, 370])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 370
TIME: Thu Nov 13 09:14:35 PM CST 2025


===== Batch 27 =====
QIDs: [1588, 1588, 1588, 1588, 1588, 1588, 1588, 1588, 1588, 1588]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [941, 673, 890, 927, 817, 887, 821, 604, 936, 885]
image sizes: [(906, 798), (906, 798), (906, 798), (906, 798), (906, 798), (906, 798), (906, 798), (906, 798), (906, 798), (906, 798)]
Thu Nov 13 21:14:36 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   37C    P0            121W /  700W |   67778MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
input_ids shape: torch.Size([10, 1418])
attention_mask shape: torch.Size([10, 1418])
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1418
|    0   N/A  N/A          562495      C   python                                67768MiB |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C     43     14      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              



===== Batch 28 =====
QIDs: [43, 43, 43, 43, 43, 43, 43, 43, 43, 43]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [307, 156, 368, 318, 305, 320, 309, 128, 362, 325]
image sizes: [(1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560)]
----------------------------------------
TIME: Thu Nov 13 09:14:41 PM CST 2025
Thu Nov 13 21:14:41 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   37C    P0            120W /  700W |   67778MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             73W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                67768MiB |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C      0      0      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              

input_ids shape: torch.Size([10, 6492])
attention_mask shape: torch.Size([10, 6492])
----------------------------------------
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 6492
TIME: Thu Nov 13 09:14:46 PM CST 2025
Thu Nov 13 21:14:46 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   52C    P0            360W /  700W |   67778MiB / 143771MiB |     98%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             73W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                67768MiB |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C     97     30      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:14:50 PM CST 2025
Thu Nov 13 21:14:51 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   40C    P0            136W /  700W |   86608MiB / 143771MiB |     10%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             73W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C      9      0      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:14:55 PM CST 2025
Thu Nov 13 21:14:55 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   38C    P0            121W /  700W |   86608MiB / 143771MiB |     11%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             73W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C     10      0      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              



===== Batch 29 =====
QIDs: [445, 445, 445, 445, 445, 445, 445, 445, 445, 445]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [327, 121, 309, 314, 248, 311, 372, 108, 381, 320]
image sizes: [(506, 578), (506, 578), (506, 578), (506, 578), (506, 578), (506, 578), (506, 578), (506, 578), (506, 578), (506, 578)]
----------------------------------------
input_ids shape: torch.Size([10, 619])
attention_mask shape: torch.Size([10, 619])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 619
TIME: Thu Nov 13 09:15:00 PM CST 2025
Thu Nov 13 21:15:00 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   38C    P0            298W /  700W |   86608MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             73W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+


===== Batch 30 =====
QIDs: [1388, 1388, 1388, 1388, 1388, 1388, 1388, 1388, 1388, 1388]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [178, 110, 173, 184, 186, 168, 173, 98, 193, 186]
image sizes: [(625, 396), (625, 396), (625, 396), (625, 396), (625, 396), (625, 396), (625, 396), (625, 396), (625, 396), (625, 396)]

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg input_ids shape: torch.Size([10, 443])
attention_mask shape: torch.Size([10, 443])
   ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C      6      0      -      -      -      -    python         
    1          -     -      -      - vision_start count = 10, vision_end count = 10
     - NaN in input_ids: False
Sequence length = 443
     -      -      -    -              



===== Batch 31 =====
QIDs: [1593, 1593, 1593, 1593, 1593, 1593, 1593, 1593, 1593, 1593]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [465, 324, 425, 471, 404, 428, 436, 276, 449, 421]
image sizes: [(870, 273), (870, 273), (870, 273), (870, 273), (870, 273), (870, 273), (870, 273), (870, 273), (870, 273), (870, 273)]
input_ids shape: torch.Size([10, 587])
attention_mask shape: torch.Size([10, 587])
----------------------------------------
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 587
TIME: Thu Nov 13 09:15:05 PM CST 2025


===== Batch 32 =====
QIDs: [482, 482, 482, 482, 482, 482, 482, 482, 482, 482]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [185, 119, 171, 177, 156, 171, 142, 97, 209, 168]
image sizes: [(555, 293), (555, 293), (555, 293), (555, 293), (555, 293), (555, 293), (555, 293), (555, 293), (555, 293), (555, 293)]
Thu Nov 13 21:15:05 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
input_ids shape: torch.Size([10, 345])
attention_mask shape: torch.Size([10, 345])
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   37C    P0            274W /  700W |   86608MiB / 143771MiB |      5%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 345
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             73W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+

Processes:


===== Batch 33 =====
QIDs: [1266, 1266, 1266, 1266, 1266, 1266, 1266, 1266, 1266, 1266]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [124, 66, 102, 120, 103, 106, 122, 50, 115, 108]
image sizes: [(330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260)]
input_ids shape: torch.Size([10, 205])
attention_mask shape: torch.Size([10, 205])
#vision_start count = 10, vision_end count = 10
NaN in input_ids: False
 gpu         pid Sequence length = 205
  type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C     17      4      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              


===== Batch 34 =====

QIDs: [1392, 1392, 1392, 1392, 1392, 1392, 1392, 1392, 1392, 1392]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [280, 177, 276, 293, 248, 270, 243, 143, 302, 287]
image sizes: [(1009, 472), (1009, 472), (1009, 472), (1009, 472), (1009, 472), (1009, 472), (1009, 472), (1009, 472), (1009, 472), (1009, 472)]
input_ids shape: torch.Size([10, 803])
attention_mask shape: torch.Size([10, 803])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 803
----------------------------------------
TIME: Thu Nov 13 09:15:10 PM CST 2025
Thu Nov 13 21:15:10 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   37C    P0            141W /  700W |   86608MiB / 143771MiB |     10%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             73W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+


===== Batch 35 =====
QIDs: [676, 676, 676, 676, 676, 676, 676, 676, 676, 676]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [944, 553, 943, 1023, 939, 977, 814, 336, 1038, 1027]
image sizes: [(1346, 716), (1346, 716), (1346, 716), (1346, 716), (1346, 716), (1346, 716), (1346, 716), (1346, 716), (1346, 716), (1346, 716)]
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+

Processes:
input_ids shape: torch.Size([10, 1791])
attention_mask shape: torch.Size([10, 1791])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1791
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C      2      0      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:15:15 PM CST 2025
Thu Nov 13 21:15:15 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|


===== Batch 36 =====
QIDs: [616, 616, 616, 616, 616, 616, 616, 616, 616, 616]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1023, 560, 934, 1252, 790, 984, 1028, 330, 1126, 911]
image sizes: [(482, 235), (482, 235), (482, 235), (482, 235), (482, 235), (482, 235), (482, 235), (482, 235), (482, 235), (482, 235)]
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   37C    P0            121W /  700W |   86608MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
input_ids shape: torch.Size([10, 716])
attention_mask shape: torch.Size([10, 716])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 716
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C     14      4      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              


===== Batch 37 =====
QIDs: [345, 345, 345, 345, 345, 345, 345, 345, 345, 345]

LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [130, 100, 120, 121, 114, 116, 127, 88, 169, 123]
image sizes: [(321, 184), (321, 184), (321, 184), (321, 184), (321, 184), (321, 184), (321, 184), (321, 184), (321, 184), (321, 184)]
input_ids shape: torch.Size([10, 214])
attention_mask shape: torch.Size([10, 214])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 214


===== Batch 38 =====
QIDs: [535, 535, 535, 535, 535, 535, 535, 535, 535, 535]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [891, 848, 892, 891, 878, 894, 877, 834, 889, 894]
image sizes: [(613, 177), (613, 177), (613, 177), (613, 177), (613, 177), (613, 177), (613, 177), (613, 177), (613, 177), (613, 177)]
input_ids shape: torch.Size([10, 622])
attention_mask shape: torch.Size([10, 622])
----------------------------------------
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 622
TIME: Thu Nov 13 09:15:20 PM CST 2025
Thu Nov 13 21:15:20 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   37C    P0            267W /  700W |   86608MiB / 143771MiB |     10%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+


===== Batch 39 =====
QIDs: [346, 346, 346, 346, 346, 346, 346, 346, 346, 346]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [139, 101, 145, 149, 139, 131, 135, 73, 140, 154]
image sizes: [(1548, 1118), (1548, 1118), (1548, 1118), (1548, 1118), (1548, 1118), (1548, 1118), (1548, 1118), (1548, 1118), (1548, 1118), (1548, 1118)]
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             73W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+

Processes:
input_ids shape: torch.Size([10, 2294])
attention_mask shape: torch.Size([10, 2294])
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
vision_start count = 10, vision_end count = 10
# Idx           #    C/G      %      %      %      % NaN in input_ids: False
     %      %    name 
    0     562495     C      2      0      -      -      -      -    python         
    1          -     - Sequence length = 2294
     -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:15:25 PM CST 2025
Thu Nov 13 21:15:25 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   39C    P0            363W /  700W |   86608MiB / 143771MiB |     10%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             73W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C      9      0      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              



===== Batch 40 =====
QIDs: [1395, 1395, 1395, 1395, 1395, 1395, 1395, 1395, 1395, 1395]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [335, 222, 365, 301, 318, 312, 300, 159, 444, 391]
image sizes: [(439, 652), (439, 652), (439, 652), (439, 652), (439, 652), (439, 652), (439, 652), (439, 652), (439, 652), (439, 652)]
input_ids shape: torch.Size([10, 602])
attention_mask shape: torch.Size([10, 602])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 602
----------------------------------------
TIME: Thu Nov 13 09:15:30 PM CST 2025


===== Batch 41 =====
QIDs: [1316, 1316, 1316, 1316, 1316, 1316, 1316, 1316, 1316, 1316]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [233, 140, 241, 287, 247, 240, 276, 108, 245, 249]
image sizes: [(136, 240), (136, 240), (136, 240), (136, 240), (136, 240), (136, 240), (136, 240), (136, 240), (136, 240), (136, 240)]
Thu Nov 13 21:15:30 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
input_ids shape: torch.Size([10, 235])
attention_mask shape: torch.Size([10, 235])
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   37C    P0            120W /  700W |   86608MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 235
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             73W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+


===== Batch 42 =====
QIDs: [455, 455, 455, 455, 455, 455, 455, 455, 455, 455]

Processes:
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [547, 234, 498, 500, 475, 476, 529, 192, 538, 445]
image sizes: [(1776, 388), (1776, 388), (1776, 388), (1776, 388), (1776, 388), (1776, 388), (1776, 388), (1776, 388), (1776, 388), (1776, 388)]
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C      8      1      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              

input_ids shape: torch.Size([10, 1210])
attention_mask shape: torch.Size([10, 1210])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1210
----------------------------------------
TIME: Thu Nov 13 09:15:34 PM CST 2025


===== Batch 43 =====
QIDs: [545, 545, 545, 545, 545, 545, 545, 545, 545, 545]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [569, 525, 566, 584, 544, 575, 542, 512, 567, 587]
image sizes: [(601, 186), (601, 186), (601, 186), (601, 186), (601, 186), (601, 186), (601, 186), (601, 186), (601, 186), (601, 186)]
input_ids shape: torch.Size([10, 530])
Thu Nov 13 21:15:35 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
attention_mask shape: torch.Size([10, 530])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 530
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   39C    P0            127W /  700W |   86608MiB / 143771MiB |      3%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             73W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+


===== Batch 44 =====
QIDs: [1655, 1655, 1655, 1655, 1655, 1655, 1655, 1655, 1655, 1655]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [347, 284, 355, 373, 327, 369, 338, 246, 344, 416]

Processes:
image sizes: [(434, 206), (434, 206), (434, 206), (434, 206), (434, 206), (434, 206), (434, 206), (434, 206), (434, 206), (434, 206)]
input_ids shape: torch.Size([10, 355])
attention_mask shape: torch.Size([10, 355])
# gpu         pid   type vision_start count = 10, vision_end count = 10
    sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      % NaN in input_ids: False
Sequence length = 355
     %    name 
    0     562495     C     11      2      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              



===== Batch 45 =====
QIDs: [475, 475, 475, 475, 475, 475, 475, 475, 475, 475]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [430, 201, 404, 428, 424, 402, 341, 144, 429, 426]
image sizes: [(534, 319), (534, 319), (534, 319), (534, 319), (534, 319), (534, 319), (534, 319), (534, 319), (534, 319), (534, 319)]
input_ids shape: torch.Size([10, 426])
attention_mask shape: torch.Size([10, 426])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 426
----------------------------------------
TIME: Thu Nov 13 09:15:39 PM CST 2025


===== Batch 46 =====
QIDs: [1573, 1573, 1573, 1573, 1573, 1573, 1573, 1573, 1573, 1573]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [115, 94, 128, 132, 120, 128, 143, 72, 150, 156]
image sizes: [(329, 391), (329, 391), (329, 391), (329, 391), (329, 391), (329, 391), (329, 391), (329, 391), (329, 391), (329, 391)]
Thu Nov 13 21:15:39 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   37C    P0            236W /  700W |   86608MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
input_ids shape: torch.Size([10, 274])
attention_mask shape: torch.Size([10, 274])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 274
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             73W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+


===== Batch 47 =====
QIDs: [1092, 1092, 1092, 1092, 1092, 1092, 1092, 1092, 1092, 1092]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [580, 255, 561, 581, 452, 546, 528, 189, 616, 570]
image sizes: [(753, 91), (753, 91), (753, 91), (753, 91), (753, 91), (753, 91), (753, 91), (753, 91), (753, 91), (753, 91)]
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+
input_ids shape: torch.Size([10, 399])
attention_mask shape: torch.Size([10, 399])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 399

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C     12      2      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              



===== Batch 48 =====
QIDs: [385, 385, 385, 385, 385, 385, 385, 385, 385, 385]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [169, 97, 167, 165, 173, 168, 175, 84, 191, 169]
image sizes: [(1842, 812), (1842, 812), (1842, 812), (1842, 812), (1842, 812), (1842, 812), (1842, 812), (1842, 812), (1842, 812), (1842, 812)]
----------------------------------------
input_ids shape: torch.Size([10, 2047])
attention_mask shape: torch.Size([10, 2047])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2047
TIME: Thu Nov 13 09:15:44 PM CST 2025
Thu Nov 13 21:15:44 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   53C    P0            640W /  700W |   86608MiB / 143771MiB |     92%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C     57     18      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              



===== Batch 49 =====
QIDs: [1269, 1269, 1269, 1269, 1269, 1269, 1269, 1269, 1269, 1269]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [522, 256, 454, 588, 418, 443, 413, 161, 468, 495]
image sizes: [(330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260)]
input_ids shape: torch.Size([10, 384])
attention_mask shape: torch.Size([10, 384])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 384
----------------------------------------
TIME: Thu Nov 13 09:15:48 PM CST 2025


===== Batch 50 =====
QIDs: [83, 83, 83, 83, 83, 83, 83, 83, 83, 83]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [218, 167, 216, 245, 205, 221, 202, 151, 217, 209]
image sizes: [(423, 244), (423, 244), (423, 244), (423, 244), (423, 244), (423, 244), (423, 244), (423, 244), (423, 244), (423, 244)]
input_ids shape: torch.Size([10, 318])
Thu Nov 13 21:15:49 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
attention_mask shape: torch.Size([10, 318])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 318
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   37C    P0            121W /  700W |   86608MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+


===== Batch 51 =====
QIDs: [960, 960, 960, 960, 960, 960, 960, 960, 960, 960]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [933, 339, 982, 884, 713, 833, 831, 247, 1137, 810]
image sizes: [(541, 312), (541, 312), (541, 312), (541, 312), (541, 312), (541, 312), (541, 312), (541, 312), (541, 312), (541, 312)]
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
input_ids shape: torch.Size([10, 760])
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
attention_mask shape: torch.Size([10, 760])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 760
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C     25      7      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              



===== Batch 52 =====
QIDs: [1048, 1048, 1048, 1048, 1048, 1048, 1048, 1048, 1048, 1048]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1150, 552, 1109, 1212, 970, 1055, 995, 354, 1204, 1150]
image sizes: [(1394, 702), (1394, 702), (1394, 702), (1394, 702), (1394, 702), (1394, 702), (1394, 702), (1394, 702), (1394, 702), (1394, 702)]
input_ids shape: torch.Size([10, 1833])
attention_mask shape: torch.Size([10, 1833])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1833
----------------------------------------
TIME: Thu Nov 13 09:15:54 PM CST 2025
Thu Nov 13 21:15:54 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   38C    P0            383W /  700W |   86608MiB / 143771MiB |     10%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+

Processes:


===== Batch 53 =====
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command QIDs: [358, 358, 358, 358, 358, 358, 358, 358, 358, 358]

# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C      9      0      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [73, 60, 86, 79, 73, 84, 94, 47, 96, 102]
image sizes: [(601, 382), (601, 382), (601, 382), (601, 382), (601, 382), (601, 382), (601, 382), (601, 382), (601, 382), (601, 382)]

input_ids shape: torch.Size([10, 368])
attention_mask shape: torch.Size([10, 368])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 368


===== Batch 54 =====
QIDs: [307, 307, 307, 307, 307, 307, 307, 307, 307, 307]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [541, 239, 503, 524, 453, 471, 434, 188, 488, 538]
image sizes: [(316, 233), (316, 233), (316, 233), (316, 233), (316, 233), (316, 233), (316, 233), (316, 233), (316, 233), (316, 233)]
----------------------------------------
input_ids shape: torch.Size([10, 356])
attention_mask shape: torch.Size([10, 356])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 356


===== Batch 55 =====
QIDs: [190, 190, 190, 190, 190, 190, 190, 190, 190, 190]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [326, 149, 308, 328, 304, 358, 311, 114, 333, 294]
image sizes: [(330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260)]
TIME: Thu Nov 13 09:15:59 PM CST 2025
input_ids shape: torch.Size([10, 334])
Thu Nov 13 21:16:00 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
attention_mask shape: torch.Size([10, 334])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 334
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   37C    P0            204W /  700W |   86608MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+

Processes:


===== Batch 56 =====
QIDs: [1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [505, 401, 506, 518, 478, 511, 426, 341, 502, 541]
image sizes: [(508, 213), (508, 213), (508, 213), (508, 213), (508, 213), (508, 213), (508, 213), (508, 213), (508, 213), (508, 213)]
input_ids shape: torch.Size([10, 469])
attention_mask shape: torch.Size([10, 469])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 469
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C     18      3      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              



===== Batch 57 =====
QIDs: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [238, 172, 230, 229, 226, 211, 248, 138, 214, 261]
image sizes: [(374, 152), (374, 152), (374, 152), (374, 152), (374, 152), (374, 152), (374, 152), (374, 152), (374, 152), (374, 152)]
input_ids shape: torch.Size([10, 259])
attention_mask shape: torch.Size([10, 259])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 259


===== Batch 58 =====
QIDs: [1644, 1644, 1644, 1644, 1644, 1644, 1644, 1644, 1644, 1644]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [781, 517, 767, 792, 687, 821, 734, 461, 791, 803]
image sizes: [(464, 429), (464, 429), (464, 429), (464, 429), (464, 429), (464, 429), (464, 429), (464, 429), (464, 429), (464, 429)]
input_ids shape: torch.Size([10, 778])
attention_mask shape: torch.Size([10, 778])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 778
----------------------------------------
TIME: Thu Nov 13 09:16:03 PM CST 2025
Thu Nov 13 21:16:03 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   37C    P0            306W /  700W |   86608MiB / 143771MiB |     10%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+


===== Batch 59 =====
QIDs: [1242, 1242, 1242, 1242, 1242, 1242, 1242, 1242, 1242, 1242]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [577, 259, 542, 610, 442, 515, 455, 168, 589, 543]
image sizes: [(1000, 562), (1000, 562), (1000, 562), (1000, 562), (1000, 562), (1000, 562), (1000, 562), (1000, 562), (1000, 562), (1000, 562)]

Processes:
input_ids shape: torch.Size([10, 995])
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa attention_mask shape: torch.Size([10, 995])
   command 
# Idx           # vision_start count = 10, vision_end count = 10
   C/G NaN in input_ids: False
Sequence length = 995
     %      %      %      %      %      %    name 
    0     562495     C      3      0      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              



===== Batch 60 =====
QIDs: [1634, 1634, 1634, 1634, 1634, 1634, 1634, 1634, 1634, 1634]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [687, 508, 587, 597, 657, 575, 680, 494, 633, 609]
image sizes: [(308, 39), (308, 39), (308, 39), (308, 39), (308, 39), (308, 39), (308, 39), (308, 39), (308, 39), (308, 39)]
input_ids shape: torch.Size([10, 455])
attention_mask shape: torch.Size([10, 455])
----------------------------------------
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 455
TIME: Thu Nov 13 09:16:09 PM CST 2025
Thu Nov 13 21:16:09 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   37C    P0            213W /  700W |   86608MiB / 143771MiB |     10%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+


===== Batch 61 =====
QIDs: [250, 250, 250, 250, 250, 250, 250, 250, 250, 250]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [181, 110, 201, 208, 182, 183, 182, 84, 197, 172]
image sizes: [(356, 222), (356, 222), (356, 222), (356, 222), (356, 222), (356, 222), (356, 222), (356, 222), (356, 222), (356, 222)]
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+
input_ids shape: torch.Size([10, 250])
attention_mask shape: torch.Size([10, 250])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 250

Processes:


===== Batch 62 =====
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
#QIDs: [366, 366, 366, 366, 366, 366, 366, 366, 366, 366]
 Idx           #    C/G      % LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [302, 182, 104, 306, 269, 118, 107, 132, 323, 259]
image sizes: [(283, 143), (283, 143), (283, 143), (283, 143), (283, 143), (283, 143), (283, 143), (283, 143), (283, 143), (283, 143)]
     %      %      %      %      %    name 
    0     562495     C      4      0      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              
input_ids shape: torch.Size([10, 225])
attention_mask shape: torch.Size([10, 225])

vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 225


===== Batch 63 =====
QIDs: [1432, 1432, 1432, 1432, 1432, 1432, 1432, 1432, 1432, 1432]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [447, 269, 429, 474, 454, 409, 473, 215, 476, 449]
image sizes: [(415, 252), (415, 252), (415, 252), (415, 252), (415, 252), (415, 252), (415, 252), (415, 252), (415, 252), (415, 252)]
input_ids shape: torch.Size([10, 460])
attention_mask shape: torch.Size([10, 460])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 460


===== Batch 64 =====
----------------------------------------
QIDs: [209, 209, 209, 209, 209, 209, 209, 209, 209, 209]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [282, 126, 301, 296, 275, 301, 238, 106, 290, 241]
image sizes: [(330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260)]
TIME: Thu Nov 13 09:16:13 PM CST 2025
input_ids shape: torch.Size([10, 285])
attention_mask shape: torch.Size([10, 285])
Thu Nov 13 21:16:13 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 285
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   37C    P0            126W /  700W |   86608MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+


===== Batch 65 =====
QIDs: [579, 579, 579, 579, 579, 579, 579, 579, 579, 579]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [313, 203, 324, 339, 291, 317, 308, 156, 339, 316]
image sizes: [(225, 202), (225, 202), (225, 202), (225, 202), (225, 202), (225, 202), (225, 202), (225, 202), (225, 202), (225, 202)]
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+
input_ids shape: torch.Size([10, 289])
attention_mask shape: torch.Size([10, 289])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 289


===== Batch 66 =====

Processes:
QIDs: [221, 221, 221, 221, 221, 221, 221, 221, 221, 221]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [65, 54, 65, 69, 60, 63, 60, 42, 72, 66]
image sizes: [(956, 694), (956, 694), (956, 694), (956, 694), (956, 694), (956, 694), (956, 694), (956, 694), (956, 694), (956, 694)]
input_ids shape: torch.Size([10, 912])
attention_mask shape: torch.Size([10, 912])
vision_start count = 10, vision_end count = 10
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      % NaN in input_ids: False
   name 
    0     562495     C     21      4      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              
Sequence length = 912

----------------------------------------
TIME: Thu Nov 13 09:16:17 PM CST 2025


===== Batch 67 =====
QIDs: [956, 956, 956, 956, 956, 956, 956, 956, 956, 956]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [163, 105, 161, 173, 150, 167, 148, 69, 169, 147]
image sizes: [(644, 232), (644, 232), (644, 232), (644, 232), (644, 232), (644, 232), (644, 232), (644, 232), (644, 232), (644, 232)]
input_ids shape: torch.Size([10, 292])
Thu Nov 13 21:16:17 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
attention_mask shape: torch.Size([10, 292])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 292
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   37C    P0            121W /  700W |   86608MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+


===== Batch 68 =====
QIDs: [826, 826, 826, 826, 826, 826, 826, 826, 826, 826]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [332, 228, 318, 322, 383, 306, 280, 191, 348, 286]
image sizes: [(429, 583), (429, 583), (429, 583), (429, 583), (429, 583), (429, 583), (429, 583), (429, 583), (429, 583), (429, 583)]
input_ids shape: torch.Size([10, 557])

Processes:
attention_mask shape: torch.Size([10, 557])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 557
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C     12      3      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              



===== Batch 69 =====
QIDs: [923, 923, 923, 923, 923, 923, 923, 923, 923, 923]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [696, 612, 680, 683, 653, 685, 658, 583, 722, 686]
image sizes: [(924, 765), (924, 765), (924, 765), (924, 765), (924, 765), (924, 765), (924, 765), (924, 765), (924, 765), (924, 765)]
input_ids shape: torch.Size([10, 1313])
attention_mask shape: torch.Size([10, 1313])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1313
----------------------------------------
TIME: Thu Nov 13 09:16:22 PM CST 2025
Thu Nov 13 21:16:23 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   37C    P0            121W /  700W |   86608MiB / 143771MiB |     10%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+


===== Batch 70 =====
QIDs: [1260, 1260, 1260, 1260, 1260, 1260, 1260, 1260, 1260, 1260]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [272, 140, 282, 276, 278, 281, 282, 124, 289, 243]
image sizes: [(842, 502), (842, 502), (842, 502), (842, 502), (842, 502), (842, 502), (842, 502), (842, 502), (842, 502), (842, 502)]
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+
input_ids shape: torch.Size([10, 740])
attention_mask shape: torch.Size([10, 740])

Processes:
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 740
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C      4      0      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              



===== Batch 71 =====
QIDs: [1278, 1278, 1278, 1278, 1278, 1278, 1278, 1278, 1278, 1278]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [169, 101, 171, 182, 166, 162, 154, 87, 179, 198]
image sizes: [(846, 364), (846, 364), (846, 364), (846, 364), (846, 364), (846, 364), (846, 364), (846, 364), (846, 364), (846, 364)]
input_ids shape: torch.Size([10, 521])
attention_mask shape: torch.Size([10, 521])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 521
----------------------------------------
TIME: Thu Nov 13 09:16:28 PM CST 2025
Thu Nov 13 21:16:28 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   37C    P0            284W /  700W |   86608MiB / 143771MiB |     10%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+


===== Batch 72 =====
QIDs: [359, 359, 359, 359, 359, 359, 359, 359, 359, 359]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [328, 188, 308, 321, 288, 320, 262, 138, 339, 303]
image sizes: [(286, 110), (286, 110), (286, 110), (286, 110), (286, 110), (286, 110), (286, 110), (286, 110), (286, 110), (286, 110)]
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+
input_ids shape: torch.Size([10, 223])
attention_mask shape: torch.Size([10, 223])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 223

Processes:


===== Batch 73 =====
QIDs: [818, 818, 818, 818, 818, 818, 818, 818, 818, 818]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [392, 224, 388, 389, 335, 370, 318, 157, 414, 396]
image sizes: [(1221, 373), (1221, 373), (1221, 373), (1221, 373), (1221, 373), (1221, 373), (1221, 373), (1221, 373), (1221, 373), (1221, 373)]
input_ids shape: torch.Size([10, 804])
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name attention_mask shape: torch.Size([10, 804])

    0     562495     C      9      1      -      -      -      -    python         
 vision_start count = 10, vision_end count = 10
   1          -     -      -      - NaN in input_ids: False
Sequence length = 804
     -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:16:32 PM CST 2025


===== Batch 74 =====
QIDs: [1493, 1493, 1493, 1493, 1493, 1493, 1493, 1493, 1493, 1493]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [297, 157, 298, 344, 254, 288, 245, 113, 323, 282]
image sizes: [(472, 480), (472, 480), (472, 480), (472, 480), (472, 480), (472, 480), (472, 480), (472, 480), (472, 480), (472, 480)]
input_ids shape: torch.Size([10, 470])
attention_mask shape: torch.Size([10, 470])
Thu Nov 13 21:16:32 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 470
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   37C    P0            128W /  700W |   86608MiB / 143771MiB |     39%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+


===== Batch 75 =====
QIDs: [1169, 1169, 1169, 1169, 1169, 1169, 1169, 1169, 1169, 1169]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [305, 164, 305, 308, 251, 302, 276, 92, 320, 288]
image sizes: [(445, 103), (445, 103), (445, 103), (445, 103), (445, 103), (445, 103), (445, 103), (445, 103), (445, 103), (445, 103)]

Processes:
input_ids shape: torch.Size([10, 251])
attention_mask shape: torch.Size([10, 251])
# gpu         pid   type     sm    mem    enc    dec vision_start count = 10, vision_end count = 10
   jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
NaN in input_ids: False
Sequence length = 251
    0     562495     C     14      3      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              



===== Batch 76 =====
QIDs: [1436, 1436, 1436, 1436, 1436, 1436, 1436, 1436, 1436, 1436]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [281, 136, 312, 260, 254, 306, 300, 109, 313, 268]
image sizes: [(351, 384), (351, 384), (351, 384), (351, 384), (351, 384), (351, 384), (351, 384), (351, 384), (351, 384), (351, 384)]
input_ids shape: torch.Size([10, 372])
attention_mask shape: torch.Size([10, 372])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 372


===== Batch 77 =====
QIDs: [89, 89, 89, 89, 89, 89, 89, 89, 89, 89]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [553, 306, 538, 536, 449, 518, 445, 243, 530, 470]
image sizes: [(433, 369), (433, 369), (433, 369), (433, 369), (433, 369), (433, 369), (433, 369), (433, 369), (433, 369), (433, 369)]
----------------------------------------
TIME: Thu Nov 13 09:16:37 PM CST 2025
input_ids shape: torch.Size([10, 517])
attention_mask shape: torch.Size([10, 517])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 517
Thu Nov 13 21:16:37 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   39C    P0            252W /  700W |   86608MiB / 143771MiB |     10%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+


===== Batch 78 =====
QIDs: [1098, 1098, 1098, 1098, 1098, 1098, 1098, 1098, 1098, 1098]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [936, 434, 937, 960, 776, 903, 836, 312, 1017, 902]
image sizes: [(567, 142), (567, 142), (567, 142), (567, 142), (567, 142), (567, 142), (567, 142), (567, 142), (567, 142), (567, 142)]
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
input_ids shape: torch.Size([10, 632])
attention_mask shape: torch.Size([10, 632])
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
|    0   N/A  N/A          562495      C   python                                86598MiB |
Sequence length = 632
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C     22      6      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              



===== Batch 79 =====
QIDs: [1370, 1370, 1370, 1370, 1370, 1370, 1370, 1370, 1370, 1370]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [570, 320, 609, 581, 591, 561, 549, 222, 586, 594]
image sizes: [(382, 470), (382, 470), (382, 470), (382, 470), (382, 470), (382, 470), (382, 470), (382, 470), (382, 470), (382, 470)]
input_ids shape: torch.Size([10, 552])
attention_mask shape: torch.Size([10, 552])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 552
----------------------------------------


===== Batch 80 =====
QIDs: [922, 922, 922, 922, 922, 922, 922, 922, 922, 922]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [397, 288, 463, 400, 372, 386, 404, 190, 469, 401]
image sizes: [(327, 152), (327, 152), (327, 152), (327, 152), (327, 152), (327, 152), (327, 152), (327, 152), (327, 152), (327, 152)]
TIME: Thu Nov 13 09:16:42 PM CST 2025
input_ids shape: torch.Size([10, 363])
attention_mask shape: torch.Size([10, 363])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 363
Thu Nov 13 21:16:42 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   37C    P0            204W /  700W |   86608MiB / 143771MiB |     10%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+


===== Batch 81 =====
QIDs: [24, 24, 24, 24, 24, 24, 24, 24, 24, 24]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [210, 154, 218, 207, 187, 212, 197, 146, 205, 216]
image sizes: [(339, 150), (339, 150), (339, 150), (339, 150), (339, 150), (339, 150), (339, 150), (339, 150), (339, 150), (339, 150)]
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+
input_ids shape: torch.Size([10, 234])
attention_mask shape: torch.Size([10, 234])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 234


===== Batch 82 =====
QIDs: [290, 290, 290, 290, 290, 290, 290, 290, 290, 290]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [954, 513, 893, 888, 784, 886, 851, 374, 940, 894]
image sizes: [(580, 572), (580, 572), (580, 572), (580, 572), (580, 572), (580, 572), (580, 572), (580, 572), (580, 572), (580, 572)]

Processes:
input_ids shape: torch.Size([10, 955])
attention_mask shape: torch.Size([10, 955])
# gpu         pid   type     sm    mem vision_start count = 10, vision_end count = 10
   enc    dec NaN in input_ids: False
   jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C     21      4      -      -      -      -    python         
    1          -     -      - Sequence length = 955
     -      -      -      -      -    -              



===== Batch 83 =====
QIDs: [1598, 1598, 1598, 1598, 1598, 1598, 1598, 1598, 1598, 1598]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [836, 766, 835, 825, 807, 838, 830, 729, 850, 826]
image sizes: [(803, 1077), (803, 1077), (803, 1077), (803, 1077), (803, 1077), (803, 1077), (803, 1077), (803, 1077), (803, 1077), (803, 1077)]
----------------------------------------
TIME: Thu Nov 13 09:16:47 PM CST 2025
input_ids shape: torch.Size([10, 1696])
attention_mask shape: torch.Size([10, 1696])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1696
Thu Nov 13 21:16:48 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   50C    P0            475W /  700W |   86608MiB / 143771MiB |     91%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C     44     14      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              



===== Batch 84 =====
QIDs: [1631, 1631, 1631, 1631, 1631, 1631, 1631, 1631, 1631, 1631]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [144, 123, 153, 144, 136, 138, 122, 99, 151, 147]
image sizes: [(1521, 352), (1521, 352), (1521, 352), (1521, 352), (1521, 352), (1521, 352), (1521, 352), (1521, 352), (1521, 352), (1521, 352)]
input_ids shape: torch.Size([10, 813])
attention_mask shape: torch.Size([10, 813])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 813
----------------------------------------
TIME: Thu Nov 13 09:16:53 PM CST 2025


===== Batch 85 =====
Thu Nov 13 21:16:53 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
QIDs: [999, 999, 999, 999, 999, 999, 999, 999, 999, 999]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [671, 425, 629, 612, 641, 632, 617, 324, 601, 663]
image sizes: [(443, 137), (443, 137), (443, 137), (443, 137), (443, 137), (443, 137), (443, 137), (443, 137), (443, 137), (443, 137)]
input_ids shape: torch.Size([10, 593])
attention_mask shape: torch.Size([10, 593])
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   37C    P0            122W /  700W |   86608MiB / 143771MiB |      1%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 593
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+

Processes:


===== Batch 86 =====
QIDs: [589, 589, 589, 589, 589, 589, 589, 589, 589, 589]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [182, 146, 170, 232, 215, 184, 170, 158, 212, 174]
image sizes: [(370, 103), (370, 103), (370, 103), (370, 103), (370, 103), (370, 103), (370, 103), (370, 103), (370, 103), (370, 103)]
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C     16      4      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              
input_ids shape: torch.Size([10, 214])
attention_mask shape: torch.Size([10, 214])

vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 214


===== Batch 87 =====
QIDs: [1465, 1465, 1465, 1465, 1465, 1465, 1465, 1465, 1465, 1465]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [243, 172, 202, 208, 529, 203, 214, 161, 195, 222]
image sizes: [(200, 119), (200, 119), (200, 119), (200, 119), (200, 119), (200, 119), (200, 119), (200, 119), (200, 119), (200, 119)]
input_ids shape: torch.Size([10, 278])
attention_mask shape: torch.Size([10, 278])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 278


===== Batch 88 =====
QIDs: [182, 182, 182, 182, 182, 182, 182, 182, 182, 182]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [246, 118, 250, 233, 171, 273, 307, 93, 302, 194]
image sizes: [(590, 290), (590, 290), (590, 290), (590, 290), (590, 290), (590, 290), (590, 290), (590, 290), (590, 290), (590, 290)]
input_ids shape: torch.Size([10, 436])
attention_mask shape: torch.Size([10, 436])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 436
----------------------------------------


===== Batch 89 =====
QIDs: [313, 313, 313, 313, 313, 313, 313, 313, 313, 313]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [282, 144, 280, 262, 277, 308, 305, 113, 306, 293]
image sizes: [(317, 240), (317, 240), (317, 240), (317, 240), (317, 240), (317, 240), (317, 240), (317, 240), (317, 240), (317, 240)]
input_ids shape: torch.Size([10, 320])
attention_mask shape: torch.Size([10, 320])
vision_start count = 10, vision_end count = 10
TIME: Thu Nov 13 09:16:59 PM CST 2025
NaN in input_ids: False
Sequence length = 320
Thu Nov 13 21:16:59 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|


===== Batch 90 =====
QIDs: [1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [553, 319, 617, 750, 575, 664, 572, 222, 608, 599]
image sizes: [(428, 366), (428, 366), (428, 366), (428, 366), (428, 366), (428, 366), (428, 366), (428, 366), (428, 366), (428, 366)]
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   37C    P0            192W /  700W |   86608MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
input_ids shape: torch.Size([10, 593])
attention_mask shape: torch.Size([10, 593])
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 593
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C     21      5      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              



===== Batch 91 =====
QIDs: [1285, 1285, 1285, 1285, 1285, 1285, 1285, 1285, 1285, 1285]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [524, 247, 515, 539, 461, 467, 569, 179, 463, 451]
image sizes: [(1134, 618), (1134, 618), (1134, 618), (1134, 618), (1134, 618), (1134, 618), (1134, 618), (1134, 618), (1134, 618), (1134, 618)]
input_ids shape: torch.Size([10, 1271])
attention_mask shape: torch.Size([10, 1271])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1271
----------------------------------------
TIME: Thu Nov 13 09:17:03 PM CST 2025
Thu Nov 13 21:17:03 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   38C    P0            351W /  700W |   86608MiB / 143771MiB |     10%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+

Processes:


===== Batch 92 =====
QIDs: [1122, 1122, 1122, 1122, 1122, 1122, 1122, 1122, 1122, 1122]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [143, 78, 135, 150, 123, 130, 119, 58, 147, 128]
#image sizes: [(508, 442), (508, 442), (508, 442), (508, 442), (508, 442), (508, 442), (508, 442), (508, 442), (508, 442), (508, 442)]
 gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C      9      0      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              
input_ids shape: torch.Size([10, 382])
attention_mask shape: torch.Size([10, 382])

vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 382


===== Batch 93 =====
QIDs: [855, 855, 855, 855, 855, 855, 855, 855, 855, 855]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [451, 268, 437, 440, 445, 420, 410, 203, 459, 477]
image sizes: [(1442, 208), (1442, 208), (1442, 208), (1442, 208), (1442, 208), (1442, 208), (1442, 208), (1442, 208), (1442, 208), (1442, 208)]
input_ids shape: torch.Size([10, 638])
attention_mask shape: torch.Size([10, 638])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 638
----------------------------------------
TIME: Thu Nov 13 09:17:07 PM CST 2025


===== Batch 94 =====
QIDs: [1418, 1418, 1418, 1418, 1418, 1418, 1418, 1418, 1418, 1418]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [241, 173, 233, 233, 230, 232, 236, 152, 221, 222]
image sizes: [(301, 249), (301, 249), (301, 249), (301, 249), (301, 249), (301, 249), (301, 249), (301, 249), (301, 249), (301, 249)]
Thu Nov 13 21:17:08 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   37C    P0            123W /  700W |   86608MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
input_ids shape: torch.Size([10, 280])
attention_mask shape: torch.Size([10, 280])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 280
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+


===== Batch 95 =====
QIDs: [245, 245, 245, 245, 245, 245, 245, 245, 245, 245]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [258, 116, 274, 265, 297, 252, 356, 100, 317, 227]
image sizes: [(891, 1090), (891, 1090), (891, 1090), (891, 1090), (891, 1090), (891, 1090), (891, 1090), (891, 1090), (891, 1090), (891, 1090)]

Processes:
input_ids shape: torch.Size([10, 1508])
attention_mask shape: torch.Size([10, 1508])
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
vision_start count = 10, vision_end count = 10
    0     562495     C      8      1      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              
NaN in input_ids: False
Sequence length = 1508

----------------------------------------
TIME: Thu Nov 13 09:17:13 PM CST 2025
Thu Nov 13 21:17:13 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   37C    P0            123W /  700W |   86608MiB / 143771MiB |     10%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+


===== Batch 96 =====
QIDs: [231, 231, 231, 231, 231, 231, 231, 231, 231, 231]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [364, 211, 372, 454, 298, 385, 413, 177, 485, 451]
image sizes: [(462, 142), (462, 142), (462, 142), (462, 142), (462, 142), (462, 142), (462, 142), (462, 142), (462, 142), (462, 142)]
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+
input_ids shape: torch.Size([10, 335])
attention_mask shape: torch.Size([10, 335])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 335

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C      6      0      -      -      -      -    python         
    1          -     -      -      -      -      -      - 

===== Batch 97 =====
QIDs: [1636, 1636, 1636, 1636, 1636, 1636, 1636, 1636, 1636, 1636]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [153, 91, 147, 171, 115, 148, 111, 56, 170, 139]
image sizes: [(442, 370), (442, 370), (442, 370), (442, 370), (442, 370), (442, 370), (442, 370), (442, 370), (442, 370), (442, 370)]
     -    -              

input_ids shape: torch.Size([10, 308])
attention_mask shape: torch.Size([10, 308])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 308


===== Batch 98 =====
QIDs: [987, 987, 987, 987, 987, 987, 987, 987, 987, 987]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [630, 271, 683, 654, 528, 698, 555, 209, 720, 613]
image sizes: [(580, 185), (580, 185), (580, 185), (580, 185), (580, 185), (580, 185), (580, 185), (580, 185), (580, 185), (580, 185)]
input_ids shape: torch.Size([10, 538])
attention_mask shape: torch.Size([10, 538])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 538


===== Batch 99 =====
QIDs: [1592, 1592, 1592, 1592, 1592, 1592, 1592, 1592, 1592, 1592]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [401, 260, 454, 439, 416, 426, 418, 208, 505, 376]
image sizes: [(278, 171), (278, 171), (278, 171), (278, 171), (278, 171), (278, 171), (278, 171), (278, 171), (278, 171), (278, 171)]
----------------------------------------
input_ids shape: torch.Size([10, 379])
attention_mask shape: torch.Size([10, 379])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 379
TIME: Thu Nov 13 09:17:18 PM CST 2025
Thu Nov 13 21:17:19 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   38C    P0            203W /  700W |   86608MiB / 143771MiB |     31%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+


===== Batch 100 =====
QIDs: [803, 803, 803, 803, 803, 803, 803, 803, 803, 803]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [285, 153, 273, 254, 241, 236, 240, 99, 301, 291]
image sizes: [(1219, 676), (1219, 676), (1219, 676), (1219, 676), (1219, 676), (1219, 676), (1219, 676), (1219, 676), (1219, 676), (1219, 676)]
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+
input_ids shape: torch.Size([10, 1204])
attention_mask shape: torch.Size([10, 1204])

Processes:
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1204
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C      0      0      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              



===== Batch 101 =====
QIDs: [1083, 1083, 1083, 1083, 1083, 1083, 1083, 1083, 1083, 1083]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [193, 131, 175, 213, 163, 172, 179, 114, 192, 197]
image sizes: [(1186, 186), (1186, 186), (1186, 186), (1186, 186), (1186, 186), (1186, 186), (1186, 186), (1186, 186), (1186, 186), (1186, 186)]
----------------------------------------
input_ids shape: torch.Size([10, 450])
attention_mask shape: torch.Size([10, 450])
vision_start count = 10, vision_end count = 10
TIME: Thu Nov 13 09:17:23 PM CST 2025
NaN in input_ids: False
Sequence length = 450
Thu Nov 13 21:17:24 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   43C    P0            255W /  700W |   86608MiB / 143771MiB |     67%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+


===== Batch 102 =====
QIDs: [243, 243, 243, 243, 243, 243, 243, 243, 243, 243]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [446, 297, 452, 439, 411, 456, 403, 260, 459, 413]
image sizes: [(516, 545), (516, 545), (516, 545), (516, 545), (516, 545), (516, 545), (516, 545), (516, 545), (516, 545), (516, 545)]
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+
input_ids shape: torch.Size([10, 590])

Processes:
attention_mask shape: torch.Size([10, 590])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 590
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C     15      3      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              



===== Batch 103 =====
QIDs: [969, 969, 969, 969, 969, 969, 969, 969, 969, 969]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [532, 263, 497, 494, 433, 462, 377, 198, 534, 533]
image sizes: [(174, 131), (174, 131), (174, 131), (174, 131), (174, 131), (174, 131), (174, 131), (174, 131), (174, 131), (174, 131)]
input_ids shape: torch.Size([10, 316])
attention_mask shape: torch.Size([10, 316])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 316


===== Batch 104 =====
QIDs: [436, 436, 436, 436, 436, 436, 436, 436, 436, 436]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [217, 112, 231, 216, 231, 229, 226, 106, 242, 216]
image sizes: [(200, 157), (200, 157), (200, 157), (200, 157), (200, 157), (200, 157), (200, 157), (200, 157), (200, 157), (200, 157)]
input_ids shape: torch.Size([10, 206])
attention_mask shape: torch.Size([10, 206])
----------------------------------------
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 206
TIME: Thu Nov 13 09:17:28 PM CST 2025


===== Batch 105 =====
QIDs: [988, 988, 988, 988, 988, 988, 988, 988, 988, 988]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [145, 80, 136, 144, 134, 144, 128, 52, 162, 134]
image sizes: [(506, 141), (506, 141), (506, 141), (506, 141), (506, 141), (506, 141), (506, 141), (506, 141), (506, 141), (506, 141)]
Thu Nov 13 21:17:28 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
input_ids shape: torch.Size([10, 186])
attention_mask shape: torch.Size([10, 186])
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   38C    P0            171W /  700W |   86608MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 186
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+


===== Batch 106 =====
QIDs: [959, 959, 959, 959, 959, 959, 959, 959, 959, 959]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [87, 71, 93, 90, 78, 88, 85, 63, 93, 94]
image sizes: [(346, 112), (346, 112), (346, 112), (346, 112), (346, 112), (346, 112), (346, 112), (346, 112), (346, 112), (346, 112)]
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+
input_ids shape: torch.Size([10, 118])
attention_mask shape: torch.Size([10, 118])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 118


===== Batch 107 =====
QIDs: [1148, 1148, 1148, 1148, 1148, 1148, 1148, 1148, 1148, 1148]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']

text lens: [605, 317, 579, 711, 587, 551, 574, 191, 639, 691]
Processes:
image sizes: [(2000, 1341), (2000, 1341), (2000, 1341), (2000, 1341), (2000, 1341), (2000, 1341), (2000, 1341), (2000, 1341), (2000, 1341), (2000, 1341)]
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C     10      1      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              

input_ids shape: torch.Size([10, 3757])
attention_mask shape: torch.Size([10, 3757])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 3757
----------------------------------------
TIME: Thu Nov 13 09:17:33 PM CST 2025
Thu Nov 13 21:17:34 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   54C    P0            684W /  700W |   86608MiB / 143771MiB |     95%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C     86     29      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:17:38 PM CST 2025
Thu Nov 13 21:17:38 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   37C    P0            121W /  700W |   86608MiB / 143771MiB |     10%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+


===== Batch 108 =====
QIDs: [1431, 1431, 1431, 1431, 1431, 1431, 1431, 1431, 1431, 1431]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1227, 1078, 1215, 1244, 1178, 1235, 1197, 1058, 1205, 1249]
image sizes: [(271, 165), (271, 165), (271, 165), (271, 165), (271, 165), (271, 165), (271, 165), (271, 165), (271, 165), (271, 165)]

Processes:
input_ids shape: torch.Size([10, 666])
attention_mask shape: torch.Size([10, 666])
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command vision_start count = 10, vision_end count = 10

# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C      7      0      -      -      -      -    python         
NaN in input_ids: False
Sequence length = 666
    1          -     -      -      -      -      -      -      -    -              



===== Batch 109 =====
QIDs: [367, 367, 367, 367, 367, 367, 367, 367, 367, 367]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [262, 124, 264, 259, 237, 246, 279, 100, 281, 239]
image sizes: [(771, 333), (771, 333), (771, 333), (771, 333), (771, 333), (771, 333), (771, 333), (771, 333), (771, 333), (771, 333)]
----------------------------------------
TIME: Thu Nov 13 09:17:43 PM CST 2025
input_ids shape: torch.Size([10, 512])
Thu Nov 13 21:17:43 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
attention_mask shape: torch.Size([10, 512])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 512
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   37C    P0            136W /  700W |   86608MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+


===== Batch 110 =====
QIDs: [66, 66, 66, 66, 66, 66, 66, 66, 66, 66]

Processes:
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [317, 155, 352, 304, 274, 325, 283, 123, 366, 347]
image sizes: [(1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560)]
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C     17      4      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:17:49 PM CST 2025
Thu Nov 13 21:17:49 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
input_ids shape: torch.Size([10, 6487])
attention_mask shape: torch.Size([10, 6487])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 6487
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19637 |
| N/A   36C    P0            134W /  700W |   86608MiB / 143771MiB |     23%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          562495      C   python                                86598MiB |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0     562495     C     97     30      -      -      -      -    python         
    1          -     -      -      -      -      -      -      -    -              
[ERROR] Batch 110 crash!
Exception: RuntimeError('CUDA error: unrecognized error code\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n')

Failed to print input_ids preview.
Failed to locate vision tokens.
Sample text preview: <image 1> What could be the reason for the abnormal yellow color pattern observed on these leaves?\nA. Viral infection\nB. Lack of sunlight\nC. Sun damage\nD. Mineral deficiency\nE. High temperature and dry weather\nF. Waterlogging\nG. Herbicide damage\nH. Insect damage\nI. None of the answers are c
----------------------------------------
TIME: Thu Nov 13 09:17:54 PM CST 2025
Thu Nov 13 21:17:54 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19711 |
| N/A   38C    P0            122W /  700W |       1MiB / 143771MiB |      6%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0          -     -      -      -      -      -      -      -    -              
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:17:58 PM CST 2025
Thu Nov 13 21:17:58 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19711 |
| N/A   35C    P0             77W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0          -     -      -      -      -      -      -      -    -              
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:18:02 PM CST 2025
Thu Nov 13 21:18:02 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19711 |
| N/A   35C    P0             76W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0          -     -      -      -      -      -      -      -    -              
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:18:05 PM CST 2025
Thu Nov 13 21:18:05 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19711 |
| N/A   34C    P0             76W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0          -     -      -      -      -      -      -      -    -              
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:18:08 PM CST 2025
Thu Nov 13 21:18:08 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19711 |
| N/A   34C    P0             76W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0          -     -      -      -      -      -      -      -    -              
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:18:13 PM CST 2025
Thu Nov 13 21:18:13 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19711 |
| N/A   34C    P0             76W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0          -     -      -      -      -      -      -      -    -              
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:18:17 PM CST 2025
Thu Nov 13 21:18:17 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19711 |
| N/A   34C    P0             76W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0          -     -      -      -      -      -      -      -    -              
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:18:20 PM CST 2025
Thu Nov 13 21:18:21 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19711 |
| N/A   33C    P0             76W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0          -     -      -      -      -      -      -      -    -              
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:18:25 PM CST 2025
Thu Nov 13 21:18:25 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19711 |
| N/A   33C    P0             76W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0          -     -      -      -      -      -      -      -    -              
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:18:29 PM CST 2025
Thu Nov 13 21:18:29 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19711 |
| N/A   33C    P0             76W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0          -     -      -      -      -      -      -      -    -              
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:18:32 PM CST 2025
Thu Nov 13 21:18:32 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19711 |
| N/A   33C    P0             76W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0          -     -      -      -      -      -      -      -    -              
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:18:36 PM CST 2025
Thu Nov 13 21:18:36 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19711 |
| N/A   33C    P0             76W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

Processes:
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0          -     -      -      -      -      -      -      -    -              
    1          -     -      -      -      -      -      -      -    -              

----------------------------------------
TIME: Thu Nov 13 09:18:40 PM CST 2025
Thu Nov 13 21:18:40 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.10              Driver Version: 570.86.10      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H200                    On  |   00000000:2A:00.0 Off |                19711 |
| N/A   33C    P0             76W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H200                    On  |   00000000:3A:00.0 Off |                    0 |
| N/A   31C    P0             74W /  700W |       1MiB / 143771MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+


===== Batch 0 =====
QIDs: [669, 669, 669, 669, 669, 669, 669, 669, 669, 669]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [531, 245, 491, 536, 450, 520, 529, 166, 561, 567]
image sizes: [(276, 269), (276, 269), (276, 269), (276, 269), (276, 269), (276, 269), (276, 269), (276, 269), (276, 269), (276, 269)]
input_ids shape: torch.Size([10, 394])
attention_mask shape: torch.Size([10, 394])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 394


===== Batch 1 =====
QIDs: [1356, 1356, 1356, 1356, 1356, 1356, 1356, 1356, 1356, 1356]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [186, 105, 157, 191, 129, 162, 167, 101, 177, 233]
image sizes: [(490, 600), (490, 600), (490, 600), (490, 600), (490, 600), (490, 600), (490, 600), (490, 600), (490, 600), (490, 600)]
input_ids shape: torch.Size([10, 500])
attention_mask shape: torch.Size([10, 500])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 500


===== Batch 2 =====
QIDs: [677, 677, 677, 677, 677, 677, 677, 677, 677, 677]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [486, 196, 440, 468, 371, 417, 438, 141, 438, 449]
image sizes: [(488, 532), (488, 532), (488, 532), (488, 532), (488, 532), (488, 532), (488, 532), (488, 532), (488, 532), (488, 532)]
input_ids shape: torch.Size([10, 594])
attention_mask shape: torch.Size([10, 594])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 594


===== Batch 3 =====
QIDs: [736, 736, 736, 736, 736, 736, 736, 736, 736, 736]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [330, 168, 338, 354, 305, 328, 266, 120, 387, 368]
image sizes: [(470, 366), (470, 366), (470, 366), (470, 366), (470, 366), (470, 366), (470, 366), (470, 366), (470, 366), (470, 366)]
input_ids shape: torch.Size([10, 417])
attention_mask shape: torch.Size([10, 417])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 417


===== Batch 4 =====
QIDs: [52, 52, 52, 52, 52, 52, 52, 52, 52, 52]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [240, 130, 236, 223, 228, 213, 207, 94, 232, 216]
image sizes: [(1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560)]
input_ids shape: torch.Size([10, 6432])
attention_mask shape: torch.Size([10, 6432])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 6432


===== Batch 5 =====
QIDs: [610, 610, 610, 610, 610, 610, 610, 610, 610, 610]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [707, 621, 724, 716, 680, 779, 691, 581, 736, 719]
image sizes: [(750, 271), (750, 271), (750, 271), (750, 271), (750, 271), (750, 271), (750, 271), (750, 271), (750, 271), (750, 271)]
input_ids shape: torch.Size([10, 849])
attention_mask shape: torch.Size([10, 849])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 849


===== Batch 6 =====
QIDs: [1129, 1129, 1129, 1129, 1129, 1129, 1129, 1129, 1129, 1129]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [225, 130, 249, 264, 240, 253, 259, 104, 277, 253]
image sizes: [(1166, 622), (1166, 622), (1166, 622), (1166, 622), (1166, 622), (1166, 622), (1166, 622), (1166, 622), (1166, 622), (1166, 622)]
input_ids shape: torch.Size([10, 1100])
attention_mask shape: torch.Size([10, 1100])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1100


===== Batch 7 =====
QIDs: [27, 27, 27, 27, 27, 27, 27, 27, 27, 27]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [397, 238, 416, 404, 389, 378, 351, 180, 414, 408]
image sizes: [(375, 184), (375, 184), (375, 184), (375, 184), (375, 184), (375, 184), (375, 184), (375, 184), (375, 184), (375, 184)]
input_ids shape: torch.Size([10, 347])
attention_mask shape: torch.Size([10, 347])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 347


===== Batch 8 =====
QIDs: [557, 557, 557, 557, 557, 557, 557, 557, 557, 557]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [244, 192, 241, 243, 266, 238, 241, 162, 270, 245]
image sizes: [(243, 229), (243, 229), (243, 229), (243, 229), (243, 229), (243, 229), (243, 229), (243, 229), (243, 229), (243, 229)]
input_ids shape: torch.Size([10, 263])
attention_mask shape: torch.Size([10, 263])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 263


===== Batch 9 =====
QIDs: [981, 981, 981, 981, 981, 981, 981, 981, 981, 981]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [123, 71, 121, 126, 108, 123, 97, 48, 123, 108]
image sizes: [(340, 292), (340, 292), (340, 292), (340, 292), (340, 292), (340, 292), (340, 292), (340, 292), (340, 292), (340, 292)]
input_ids shape: torch.Size([10, 203])
attention_mask shape: torch.Size([10, 203])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 203


===== Batch 10 =====
QIDs: [1202, 1202, 1202, 1202, 1202, 1202, 1202, 1202, 1202, 1202]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [592, 424, 582, 600, 474, 554, 502, 347, 625, 597]
image sizes: [(889, 101), (889, 101), (889, 101), (889, 101), (889, 101), (889, 101), (889, 101), (889, 101), (889, 101), (889, 101)]
input_ids shape: torch.Size([10, 510])
attention_mask shape: torch.Size([10, 510])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 510


===== Batch 11 =====
QIDs: [335, 335, 335, 335, 335, 335, 335, 335, 335, 335]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [264, 149, 264, 270, 270, 267, 262, 115, 291, 276]
image sizes: [(308, 480), (308, 480), (308, 480), (308, 480), (308, 480), (308, 480), (308, 480), (308, 480), (308, 480), (308, 480)]
input_ids shape: torch.Size([10, 361])
attention_mask shape: torch.Size([10, 361])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 361


===== Batch 12 =====
QIDs: [1725, 1725, 1725, 1725, 1725, 1725, 1725, 1725, 1725, 1725]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [830, 460, 853, 977, 742, 869, 680, 263, 922, 922]
image sizes: [(1282, 1180), (1282, 1180), (1282, 1180), (1282, 1180), (1282, 1180), (1282, 1180), (1282, 1180), (1282, 1180), (1282, 1180), (1282, 1180)]
input_ids shape: torch.Size([10, 2374])
attention_mask shape: torch.Size([10, 2374])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2374


===== Batch 13 =====
QIDs: [78, 78, 78, 78, 78, 78, 78, 78, 78, 78]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [696, 479, 676, 704, 620, 704, 655, 406, 664, 712]
image sizes: [(652, 177), (652, 177), (652, 177), (652, 177), (652, 177), (652, 177), (652, 177), (652, 177), (652, 177), (652, 177)]
input_ids shape: torch.Size([10, 623])
attention_mask shape: torch.Size([10, 623])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 623


===== Batch 14 =====
QIDs: [1108, 1108, 1108, 1108, 1108, 1108, 1108, 1108, 1108, 1108]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [706, 401, 684, 713, 613, 690, 694, 275, 705, 761]
image sizes: [(766, 205), (766, 205), (766, 205), (766, 205), (766, 205), (766, 205), (766, 205), (766, 205), (766, 205), (766, 205)]
input_ids shape: torch.Size([10, 648])
attention_mask shape: torch.Size([10, 648])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 648


===== Batch 15 =====
QIDs: [709, 709, 709, 709, 709, 709, 709, 709, 709, 709]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [571, 219, 510, 546, 443, 503, 508, 169, 575, 540]
image sizes: [(1218, 760), (1218, 760), (1218, 760), (1218, 760), (1218, 760), (1218, 760), (1218, 760), (1218, 760), (1218, 760), (1218, 760)]
input_ids shape: torch.Size([10, 1488])
attention_mask shape: torch.Size([10, 1488])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1488


===== Batch 16 =====
QIDs: [1672, 1672, 1672, 1672, 1672, 1672, 1672, 1672, 1672, 1672]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [513, 260, 437, 442, 405, 430, 434, 160, 499, 458]
image sizes: [(302, 208), (302, 208), (302, 208), (302, 208), (302, 208), (302, 208), (302, 208), (302, 208), (302, 208), (302, 208)]
input_ids shape: torch.Size([10, 355])
attention_mask shape: torch.Size([10, 355])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 355


===== Batch 17 =====
QIDs: [175, 175, 175, 175, 175, 175, 175, 175, 175, 175]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [309, 145, 316, 285, 283, 295, 281, 116, 316, 329]
image sizes: [(1024, 324), (1024, 324), (1024, 324), (1024, 324), (1024, 324), (1024, 324), (1024, 324), (1024, 324), (1024, 324), (1024, 324)]
input_ids shape: torch.Size([10, 629])
attention_mask shape: torch.Size([10, 629])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 629


===== Batch 18 =====
QIDs: [19, 19, 19, 19, 19, 19, 19, 19, 19, 19]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1314, 871, 1406, 1382, 1242, 1398, 1239, 685, 1526, 1477]
image sizes: [(736, 145), (736, 145), (736, 145), (736, 145), (736, 145), (736, 145), (736, 145), (736, 145), (736, 145), (736, 145)]
input_ids shape: torch.Size([10, 1022])
attention_mask shape: torch.Size([10, 1022])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1022


===== Batch 19 =====
QIDs: [1406, 1406, 1406, 1406, 1406, 1406, 1406, 1406, 1406, 1406]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [313, 260, 302, 304, 276, 309, 290, 236, 308, 315]
image sizes: [(407, 207), (407, 207), (407, 207), (407, 207), (407, 207), (407, 207), (407, 207), (407, 207), (407, 207), (407, 207)]
input_ids shape: torch.Size([10, 350])
attention_mask shape: torch.Size([10, 350])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 350


===== Batch 20 =====
QIDs: [1066, 1066, 1066, 1066, 1066, 1066, 1066, 1066, 1066, 1066]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [2947, 1392, 2840, 3177, 2528, 2802, 2385, 884, 3017, 3119]
image sizes: [(866, 532), (866, 532), (866, 532), (866, 532), (866, 532), (866, 532), (866, 532), (866, 532), (866, 532), (866, 532)]
input_ids shape: torch.Size([10, 1972])
attention_mask shape: torch.Size([10, 1972])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1972


===== Batch 21 =====
QIDs: [1326, 1326, 1326, 1326, 1326, 1326, 1326, 1326, 1326, 1326]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [188, 102, 181, 189, 199, 196, 163, 87, 228, 169]
image sizes: [(1192, 1276), (1192, 1276), (1192, 1276), (1192, 1276), (1192, 1276), (1192, 1276), (1192, 1276), (1192, 1276), (1192, 1276), (1192, 1276)]
input_ids shape: torch.Size([10, 2113])
attention_mask shape: torch.Size([10, 2113])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2113


===== Batch 22 =====
QIDs: [719, 719, 719, 719, 719, 719, 719, 719, 719, 719]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [306, 173, 336, 388, 293, 357, 323, 129, 361, 355]
image sizes: [(500, 441), (500, 441), (500, 441), (500, 441), (500, 441), (500, 441), (500, 441), (500, 441), (500, 441), (500, 441)]
input_ids shape: torch.Size([10, 502])
attention_mask shape: torch.Size([10, 502])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 502


===== Batch 23 =====
QIDs: [242, 242, 242, 242, 242, 242, 242, 242, 242, 242]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [246, 142, 201, 245, 203, 220, 216, 112, 265, 263]
image sizes: [(636, 368), (636, 368), (636, 368), (636, 368), (636, 368), (636, 368), (636, 368), (636, 368), (636, 368), (636, 368)]
input_ids shape: torch.Size([10, 449])
attention_mask shape: torch.Size([10, 449])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 449


===== Batch 24 =====
QIDs: [1507, 1507, 1507, 1507, 1507, 1507, 1507, 1507, 1507, 1507]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [2042, 1075, 1913, 2202, 1972, 1986, 1831, 659, 2281, 2127]
image sizes: [(1022, 649), (1022, 649), (1022, 649), (1022, 649), (1022, 649), (1022, 649), (1022, 649), (1022, 649), (1022, 649), (1022, 649)]
input_ids shape: torch.Size([10, 1798])
attention_mask shape: torch.Size([10, 1798])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1798


===== Batch 25 =====
QIDs: [500, 500, 500, 500, 500, 500, 500, 500, 500, 500]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1025, 486, 954, 924, 794, 929, 923, 331, 1028, 903]
image sizes: [(370, 306), (370, 306), (370, 306), (370, 306), (370, 306), (370, 306), (370, 306), (370, 306), (370, 306), (370, 306)]
input_ids shape: torch.Size([10, 684])
attention_mask shape: torch.Size([10, 684])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 684


===== Batch 26 =====
QIDs: [509, 509, 509, 509, 509, 509, 509, 509, 509, 509]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [259, 206, 250, 283, 230, 249, 230, 179, 281, 278]
image sizes: [(562, 226), (562, 226), (562, 226), (562, 226), (562, 226), (562, 226), (562, 226), (562, 226), (562, 226), (562, 226)]
input_ids shape: torch.Size([10, 370])
attention_mask shape: torch.Size([10, 370])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 370


===== Batch 27 =====
QIDs: [1588, 1588, 1588, 1588, 1588, 1588, 1588, 1588, 1588, 1588]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [941, 673, 890, 927, 817, 887, 821, 604, 936, 885]
image sizes: [(906, 798), (906, 798), (906, 798), (906, 798), (906, 798), (906, 798), (906, 798), (906, 798), (906, 798), (906, 798)]
input_ids shape: torch.Size([10, 1418])
attention_mask shape: torch.Size([10, 1418])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1418


===== Batch 28 =====
QIDs: [43, 43, 43, 43, 43, 43, 43, 43, 43, 43]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [307, 156, 368, 318, 305, 320, 309, 128, 362, 325]
image sizes: [(1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560)]
input_ids shape: torch.Size([10, 6492])
attention_mask shape: torch.Size([10, 6492])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 6492


===== Batch 29 =====
QIDs: [445, 445, 445, 445, 445, 445, 445, 445, 445, 445]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [327, 121, 309, 314, 248, 311, 372, 108, 381, 320]
image sizes: [(506, 578), (506, 578), (506, 578), (506, 578), (506, 578), (506, 578), (506, 578), (506, 578), (506, 578), (506, 578)]
input_ids shape: torch.Size([10, 619])
attention_mask shape: torch.Size([10, 619])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 619


===== Batch 30 =====
QIDs: [1388, 1388, 1388, 1388, 1388, 1388, 1388, 1388, 1388, 1388]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [178, 110, 173, 184, 186, 168, 173, 98, 193, 186]
image sizes: [(625, 396), (625, 396), (625, 396), (625, 396), (625, 396), (625, 396), (625, 396), (625, 396), (625, 396), (625, 396)]
input_ids shape: torch.Size([10, 443])
attention_mask shape: torch.Size([10, 443])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 443


===== Batch 31 =====
QIDs: [1593, 1593, 1593, 1593, 1593, 1593, 1593, 1593, 1593, 1593]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [465, 324, 425, 471, 404, 428, 436, 276, 449, 421]
image sizes: [(870, 273), (870, 273), (870, 273), (870, 273), (870, 273), (870, 273), (870, 273), (870, 273), (870, 273), (870, 273)]
input_ids shape: torch.Size([10, 587])
attention_mask shape: torch.Size([10, 587])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 587


===== Batch 32 =====
QIDs: [482, 482, 482, 482, 482, 482, 482, 482, 482, 482]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [185, 119, 171, 177, 156, 171, 142, 97, 209, 168]
image sizes: [(555, 293), (555, 293), (555, 293), (555, 293), (555, 293), (555, 293), (555, 293), (555, 293), (555, 293), (555, 293)]
input_ids shape: torch.Size([10, 345])
attention_mask shape: torch.Size([10, 345])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 345


===== Batch 33 =====
QIDs: [1266, 1266, 1266, 1266, 1266, 1266, 1266, 1266, 1266, 1266]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [124, 66, 102, 120, 103, 106, 122, 50, 115, 108]
image sizes: [(330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260)]
input_ids shape: torch.Size([10, 205])
attention_mask shape: torch.Size([10, 205])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 205


===== Batch 34 =====
QIDs: [1392, 1392, 1392, 1392, 1392, 1392, 1392, 1392, 1392, 1392]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [280, 177, 276, 293, 248, 270, 243, 143, 302, 287]
image sizes: [(1009, 472), (1009, 472), (1009, 472), (1009, 472), (1009, 472), (1009, 472), (1009, 472), (1009, 472), (1009, 472), (1009, 472)]
input_ids shape: torch.Size([10, 803])
attention_mask shape: torch.Size([10, 803])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 803


===== Batch 35 =====
QIDs: [676, 676, 676, 676, 676, 676, 676, 676, 676, 676]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [944, 553, 943, 1023, 939, 977, 814, 336, 1038, 1027]
image sizes: [(1346, 716), (1346, 716), (1346, 716), (1346, 716), (1346, 716), (1346, 716), (1346, 716), (1346, 716), (1346, 716), (1346, 716)]
input_ids shape: torch.Size([10, 1791])
attention_mask shape: torch.Size([10, 1791])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1791


===== Batch 36 =====
QIDs: [616, 616, 616, 616, 616, 616, 616, 616, 616, 616]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1023, 560, 934, 1252, 790, 984, 1028, 330, 1126, 911]
image sizes: [(482, 235), (482, 235), (482, 235), (482, 235), (482, 235), (482, 235), (482, 235), (482, 235), (482, 235), (482, 235)]
input_ids shape: torch.Size([10, 716])
attention_mask shape: torch.Size([10, 716])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 716


===== Batch 37 =====
QIDs: [345, 345, 345, 345, 345, 345, 345, 345, 345, 345]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [130, 100, 120, 121, 114, 116, 127, 88, 169, 123]
image sizes: [(321, 184), (321, 184), (321, 184), (321, 184), (321, 184), (321, 184), (321, 184), (321, 184), (321, 184), (321, 184)]
input_ids shape: torch.Size([10, 214])
attention_mask shape: torch.Size([10, 214])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 214


===== Batch 38 =====
QIDs: [535, 535, 535, 535, 535, 535, 535, 535, 535, 535]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [891, 848, 892, 891, 878, 894, 877, 834, 889, 894]
image sizes: [(613, 177), (613, 177), (613, 177), (613, 177), (613, 177), (613, 177), (613, 177), (613, 177), (613, 177), (613, 177)]
input_ids shape: torch.Size([10, 622])
attention_mask shape: torch.Size([10, 622])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 622


===== Batch 39 =====
QIDs: [346, 346, 346, 346, 346, 346, 346, 346, 346, 346]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [139, 101, 145, 149, 139, 131, 135, 73, 140, 154]
image sizes: [(1548, 1118), (1548, 1118), (1548, 1118), (1548, 1118), (1548, 1118), (1548, 1118), (1548, 1118), (1548, 1118), (1548, 1118), (1548, 1118)]
input_ids shape: torch.Size([10, 2294])
attention_mask shape: torch.Size([10, 2294])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2294


===== Batch 40 =====
QIDs: [1395, 1395, 1395, 1395, 1395, 1395, 1395, 1395, 1395, 1395]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [335, 222, 365, 301, 318, 312, 300, 159, 444, 391]
image sizes: [(439, 652), (439, 652), (439, 652), (439, 652), (439, 652), (439, 652), (439, 652), (439, 652), (439, 652), (439, 652)]
input_ids shape: torch.Size([10, 602])
attention_mask shape: torch.Size([10, 602])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 602


===== Batch 41 =====
QIDs: [1316, 1316, 1316, 1316, 1316, 1316, 1316, 1316, 1316, 1316]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [233, 140, 241, 287, 247, 240, 276, 108, 245, 249]
image sizes: [(136, 240), (136, 240), (136, 240), (136, 240), (136, 240), (136, 240), (136, 240), (136, 240), (136, 240), (136, 240)]
input_ids shape: torch.Size([10, 235])
attention_mask shape: torch.Size([10, 235])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 235


===== Batch 42 =====
QIDs: [455, 455, 455, 455, 455, 455, 455, 455, 455, 455]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [547, 234, 498, 500, 475, 476, 529, 192, 538, 445]
image sizes: [(1776, 388), (1776, 388), (1776, 388), (1776, 388), (1776, 388), (1776, 388), (1776, 388), (1776, 388), (1776, 388), (1776, 388)]
input_ids shape: torch.Size([10, 1210])
attention_mask shape: torch.Size([10, 1210])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1210


===== Batch 43 =====
QIDs: [545, 545, 545, 545, 545, 545, 545, 545, 545, 545]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [569, 525, 566, 584, 544, 575, 542, 512, 567, 587]
image sizes: [(601, 186), (601, 186), (601, 186), (601, 186), (601, 186), (601, 186), (601, 186), (601, 186), (601, 186), (601, 186)]
input_ids shape: torch.Size([10, 530])
attention_mask shape: torch.Size([10, 530])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 530


===== Batch 44 =====
QIDs: [1655, 1655, 1655, 1655, 1655, 1655, 1655, 1655, 1655, 1655]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [347, 284, 355, 373, 327, 369, 338, 246, 344, 416]
image sizes: [(434, 206), (434, 206), (434, 206), (434, 206), (434, 206), (434, 206), (434, 206), (434, 206), (434, 206), (434, 206)]
input_ids shape: torch.Size([10, 355])
attention_mask shape: torch.Size([10, 355])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 355


===== Batch 45 =====
QIDs: [475, 475, 475, 475, 475, 475, 475, 475, 475, 475]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [430, 201, 404, 428, 424, 402, 341, 144, 429, 426]
image sizes: [(534, 319), (534, 319), (534, 319), (534, 319), (534, 319), (534, 319), (534, 319), (534, 319), (534, 319), (534, 319)]
input_ids shape: torch.Size([10, 426])
attention_mask shape: torch.Size([10, 426])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 426


===== Batch 46 =====
QIDs: [1573, 1573, 1573, 1573, 1573, 1573, 1573, 1573, 1573, 1573]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [115, 94, 128, 132, 120, 128, 143, 72, 150, 156]
image sizes: [(329, 391), (329, 391), (329, 391), (329, 391), (329, 391), (329, 391), (329, 391), (329, 391), (329, 391), (329, 391)]
input_ids shape: torch.Size([10, 274])
attention_mask shape: torch.Size([10, 274])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 274


===== Batch 47 =====
QIDs: [1092, 1092, 1092, 1092, 1092, 1092, 1092, 1092, 1092, 1092]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [580, 255, 561, 581, 452, 546, 528, 189, 616, 570]
image sizes: [(753, 91), (753, 91), (753, 91), (753, 91), (753, 91), (753, 91), (753, 91), (753, 91), (753, 91), (753, 91)]
input_ids shape: torch.Size([10, 399])
attention_mask shape: torch.Size([10, 399])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 399


===== Batch 48 =====
QIDs: [385, 385, 385, 385, 385, 385, 385, 385, 385, 385]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [169, 97, 167, 165, 173, 168, 175, 84, 191, 169]
image sizes: [(1842, 812), (1842, 812), (1842, 812), (1842, 812), (1842, 812), (1842, 812), (1842, 812), (1842, 812), (1842, 812), (1842, 812)]
input_ids shape: torch.Size([10, 2047])
attention_mask shape: torch.Size([10, 2047])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2047


===== Batch 49 =====
QIDs: [1269, 1269, 1269, 1269, 1269, 1269, 1269, 1269, 1269, 1269]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [522, 256, 454, 588, 418, 443, 413, 161, 468, 495]
image sizes: [(330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260)]
input_ids shape: torch.Size([10, 384])
attention_mask shape: torch.Size([10, 384])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 384


===== Batch 50 =====
QIDs: [83, 83, 83, 83, 83, 83, 83, 83, 83, 83]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [218, 167, 216, 245, 205, 221, 202, 151, 217, 209]
image sizes: [(423, 244), (423, 244), (423, 244), (423, 244), (423, 244), (423, 244), (423, 244), (423, 244), (423, 244), (423, 244)]
input_ids shape: torch.Size([10, 318])
attention_mask shape: torch.Size([10, 318])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 318


===== Batch 51 =====
QIDs: [960, 960, 960, 960, 960, 960, 960, 960, 960, 960]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [933, 339, 982, 884, 713, 833, 831, 247, 1137, 810]
image sizes: [(541, 312), (541, 312), (541, 312), (541, 312), (541, 312), (541, 312), (541, 312), (541, 312), (541, 312), (541, 312)]
input_ids shape: torch.Size([10, 760])
attention_mask shape: torch.Size([10, 760])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 760


===== Batch 52 =====
QIDs: [1048, 1048, 1048, 1048, 1048, 1048, 1048, 1048, 1048, 1048]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1150, 552, 1109, 1212, 970, 1055, 995, 354, 1204, 1150]
image sizes: [(1394, 702), (1394, 702), (1394, 702), (1394, 702), (1394, 702), (1394, 702), (1394, 702), (1394, 702), (1394, 702), (1394, 702)]
input_ids shape: torch.Size([10, 1833])
attention_mask shape: torch.Size([10, 1833])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1833


===== Batch 53 =====
QIDs: [358, 358, 358, 358, 358, 358, 358, 358, 358, 358]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [73, 60, 86, 79, 73, 84, 94, 47, 96, 102]
image sizes: [(601, 382), (601, 382), (601, 382), (601, 382), (601, 382), (601, 382), (601, 382), (601, 382), (601, 382), (601, 382)]
input_ids shape: torch.Size([10, 368])
attention_mask shape: torch.Size([10, 368])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 368


===== Batch 54 =====
QIDs: [307, 307, 307, 307, 307, 307, 307, 307, 307, 307]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [541, 239, 503, 524, 453, 471, 434, 188, 488, 538]
image sizes: [(316, 233), (316, 233), (316, 233), (316, 233), (316, 233), (316, 233), (316, 233), (316, 233), (316, 233), (316, 233)]
input_ids shape: torch.Size([10, 356])
attention_mask shape: torch.Size([10, 356])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 356


===== Batch 55 =====
QIDs: [190, 190, 190, 190, 190, 190, 190, 190, 190, 190]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [326, 149, 308, 328, 304, 358, 311, 114, 333, 294]
image sizes: [(330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260)]
input_ids shape: torch.Size([10, 334])
attention_mask shape: torch.Size([10, 334])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 334


===== Batch 56 =====
QIDs: [1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [505, 401, 506, 518, 478, 511, 426, 341, 502, 541]
image sizes: [(508, 213), (508, 213), (508, 213), (508, 213), (508, 213), (508, 213), (508, 213), (508, 213), (508, 213), (508, 213)]
input_ids shape: torch.Size([10, 469])
attention_mask shape: torch.Size([10, 469])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 469


===== Batch 57 =====
QIDs: [96, 96, 96, 96, 96, 96, 96, 96, 96, 96]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [238, 172, 230, 229, 226, 211, 248, 138, 214, 261]
image sizes: [(374, 152), (374, 152), (374, 152), (374, 152), (374, 152), (374, 152), (374, 152), (374, 152), (374, 152), (374, 152)]
input_ids shape: torch.Size([10, 259])
attention_mask shape: torch.Size([10, 259])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 259


===== Batch 58 =====
QIDs: [1644, 1644, 1644, 1644, 1644, 1644, 1644, 1644, 1644, 1644]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [781, 517, 767, 792, 687, 821, 734, 461, 791, 803]
image sizes: [(464, 429), (464, 429), (464, 429), (464, 429), (464, 429), (464, 429), (464, 429), (464, 429), (464, 429), (464, 429)]
input_ids shape: torch.Size([10, 778])
attention_mask shape: torch.Size([10, 778])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 778


===== Batch 59 =====
QIDs: [1242, 1242, 1242, 1242, 1242, 1242, 1242, 1242, 1242, 1242]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [577, 259, 542, 610, 442, 515, 455, 168, 589, 543]
image sizes: [(1000, 562), (1000, 562), (1000, 562), (1000, 562), (1000, 562), (1000, 562), (1000, 562), (1000, 562), (1000, 562), (1000, 562)]
input_ids shape: torch.Size([10, 995])
attention_mask shape: torch.Size([10, 995])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 995


===== Batch 60 =====
QIDs: [1634, 1634, 1634, 1634, 1634, 1634, 1634, 1634, 1634, 1634]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [687, 508, 587, 597, 657, 575, 680, 494, 633, 609]
image sizes: [(308, 39), (308, 39), (308, 39), (308, 39), (308, 39), (308, 39), (308, 39), (308, 39), (308, 39), (308, 39)]
input_ids shape: torch.Size([10, 455])
attention_mask shape: torch.Size([10, 455])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 455


===== Batch 61 =====
QIDs: [250, 250, 250, 250, 250, 250, 250, 250, 250, 250]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [181, 110, 201, 208, 182, 183, 182, 84, 197, 172]
image sizes: [(356, 222), (356, 222), (356, 222), (356, 222), (356, 222), (356, 222), (356, 222), (356, 222), (356, 222), (356, 222)]
input_ids shape: torch.Size([10, 250])
attention_mask shape: torch.Size([10, 250])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 250


===== Batch 62 =====
QIDs: [366, 366, 366, 366, 366, 366, 366, 366, 366, 366]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [302, 182, 104, 306, 269, 118, 107, 132, 323, 259]
image sizes: [(283, 143), (283, 143), (283, 143), (283, 143), (283, 143), (283, 143), (283, 143), (283, 143), (283, 143), (283, 143)]
input_ids shape: torch.Size([10, 225])
attention_mask shape: torch.Size([10, 225])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 225


===== Batch 63 =====
QIDs: [1432, 1432, 1432, 1432, 1432, 1432, 1432, 1432, 1432, 1432]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [447, 269, 429, 474, 454, 409, 473, 215, 476, 449]
image sizes: [(415, 252), (415, 252), (415, 252), (415, 252), (415, 252), (415, 252), (415, 252), (415, 252), (415, 252), (415, 252)]
input_ids shape: torch.Size([10, 460])
attention_mask shape: torch.Size([10, 460])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 460


===== Batch 64 =====
QIDs: [209, 209, 209, 209, 209, 209, 209, 209, 209, 209]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [282, 126, 301, 296, 275, 301, 238, 106, 290, 241]
image sizes: [(330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260)]
input_ids shape: torch.Size([10, 285])
attention_mask shape: torch.Size([10, 285])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 285


===== Batch 65 =====
QIDs: [579, 579, 579, 579, 579, 579, 579, 579, 579, 579]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [313, 203, 324, 339, 291, 317, 308, 156, 339, 316]
image sizes: [(225, 202), (225, 202), (225, 202), (225, 202), (225, 202), (225, 202), (225, 202), (225, 202), (225, 202), (225, 202)]
input_ids shape: torch.Size([10, 289])
attention_mask shape: torch.Size([10, 289])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 289


===== Batch 66 =====
QIDs: [221, 221, 221, 221, 221, 221, 221, 221, 221, 221]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [65, 54, 65, 69, 60, 63, 60, 42, 72, 66]
image sizes: [(956, 694), (956, 694), (956, 694), (956, 694), (956, 694), (956, 694), (956, 694), (956, 694), (956, 694), (956, 694)]
input_ids shape: torch.Size([10, 912])
attention_mask shape: torch.Size([10, 912])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 912


===== Batch 67 =====
QIDs: [956, 956, 956, 956, 956, 956, 956, 956, 956, 956]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [163, 105, 161, 173, 150, 167, 148, 69, 169, 147]
image sizes: [(644, 232), (644, 232), (644, 232), (644, 232), (644, 232), (644, 232), (644, 232), (644, 232), (644, 232), (644, 232)]
input_ids shape: torch.Size([10, 292])
attention_mask shape: torch.Size([10, 292])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 292


===== Batch 68 =====
QIDs: [826, 826, 826, 826, 826, 826, 826, 826, 826, 826]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [332, 228, 318, 322, 383, 306, 280, 191, 348, 286]
image sizes: [(429, 583), (429, 583), (429, 583), (429, 583), (429, 583), (429, 583), (429, 583), (429, 583), (429, 583), (429, 583)]
input_ids shape: torch.Size([10, 557])
attention_mask shape: torch.Size([10, 557])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 557


===== Batch 69 =====
QIDs: [923, 923, 923, 923, 923, 923, 923, 923, 923, 923]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [696, 612, 680, 683, 653, 685, 658, 583, 722, 686]
image sizes: [(924, 765), (924, 765), (924, 765), (924, 765), (924, 765), (924, 765), (924, 765), (924, 765), (924, 765), (924, 765)]
input_ids shape: torch.Size([10, 1313])
attention_mask shape: torch.Size([10, 1313])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1313


===== Batch 70 =====
QIDs: [1260, 1260, 1260, 1260, 1260, 1260, 1260, 1260, 1260, 1260]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [272, 140, 282, 276, 278, 281, 282, 124, 289, 243]
image sizes: [(842, 502), (842, 502), (842, 502), (842, 502), (842, 502), (842, 502), (842, 502), (842, 502), (842, 502), (842, 502)]
input_ids shape: torch.Size([10, 740])
attention_mask shape: torch.Size([10, 740])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 740


===== Batch 71 =====
QIDs: [1278, 1278, 1278, 1278, 1278, 1278, 1278, 1278, 1278, 1278]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [169, 101, 171, 182, 166, 162, 154, 87, 179, 198]
image sizes: [(846, 364), (846, 364), (846, 364), (846, 364), (846, 364), (846, 364), (846, 364), (846, 364), (846, 364), (846, 364)]
input_ids shape: torch.Size([10, 521])
attention_mask shape: torch.Size([10, 521])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 521


===== Batch 72 =====
QIDs: [359, 359, 359, 359, 359, 359, 359, 359, 359, 359]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [328, 188, 308, 321, 288, 320, 262, 138, 339, 303]
image sizes: [(286, 110), (286, 110), (286, 110), (286, 110), (286, 110), (286, 110), (286, 110), (286, 110), (286, 110), (286, 110)]
input_ids shape: torch.Size([10, 223])
attention_mask shape: torch.Size([10, 223])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 223


===== Batch 73 =====
QIDs: [818, 818, 818, 818, 818, 818, 818, 818, 818, 818]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [392, 224, 388, 389, 335, 370, 318, 157, 414, 396]
image sizes: [(1221, 373), (1221, 373), (1221, 373), (1221, 373), (1221, 373), (1221, 373), (1221, 373), (1221, 373), (1221, 373), (1221, 373)]
input_ids shape: torch.Size([10, 804])
attention_mask shape: torch.Size([10, 804])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 804


===== Batch 74 =====
QIDs: [1493, 1493, 1493, 1493, 1493, 1493, 1493, 1493, 1493, 1493]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [297, 157, 298, 344, 254, 288, 245, 113, 323, 282]
image sizes: [(472, 480), (472, 480), (472, 480), (472, 480), (472, 480), (472, 480), (472, 480), (472, 480), (472, 480), (472, 480)]
input_ids shape: torch.Size([10, 470])
attention_mask shape: torch.Size([10, 470])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 470


===== Batch 75 =====
QIDs: [1169, 1169, 1169, 1169, 1169, 1169, 1169, 1169, 1169, 1169]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [305, 164, 305, 308, 251, 302, 276, 92, 320, 288]
image sizes: [(445, 103), (445, 103), (445, 103), (445, 103), (445, 103), (445, 103), (445, 103), (445, 103), (445, 103), (445, 103)]
input_ids shape: torch.Size([10, 251])
attention_mask shape: torch.Size([10, 251])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 251


===== Batch 76 =====
QIDs: [1436, 1436, 1436, 1436, 1436, 1436, 1436, 1436, 1436, 1436]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [281, 136, 312, 260, 254, 306, 300, 109, 313, 268]
image sizes: [(351, 384), (351, 384), (351, 384), (351, 384), (351, 384), (351, 384), (351, 384), (351, 384), (351, 384), (351, 384)]
input_ids shape: torch.Size([10, 372])
attention_mask shape: torch.Size([10, 372])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 372


===== Batch 77 =====
QIDs: [89, 89, 89, 89, 89, 89, 89, 89, 89, 89]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [553, 306, 538, 536, 449, 518, 445, 243, 530, 470]
image sizes: [(433, 369), (433, 369), (433, 369), (433, 369), (433, 369), (433, 369), (433, 369), (433, 369), (433, 369), (433, 369)]
input_ids shape: torch.Size([10, 517])
attention_mask shape: torch.Size([10, 517])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 517


===== Batch 78 =====
QIDs: [1098, 1098, 1098, 1098, 1098, 1098, 1098, 1098, 1098, 1098]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [936, 434, 937, 960, 776, 903, 836, 312, 1017, 902]
image sizes: [(567, 142), (567, 142), (567, 142), (567, 142), (567, 142), (567, 142), (567, 142), (567, 142), (567, 142), (567, 142)]
input_ids shape: torch.Size([10, 632])
attention_mask shape: torch.Size([10, 632])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 632


===== Batch 79 =====
QIDs: [1370, 1370, 1370, 1370, 1370, 1370, 1370, 1370, 1370, 1370]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [570, 320, 609, 581, 591, 561, 549, 222, 586, 594]
image sizes: [(382, 470), (382, 470), (382, 470), (382, 470), (382, 470), (382, 470), (382, 470), (382, 470), (382, 470), (382, 470)]
input_ids shape: torch.Size([10, 552])
attention_mask shape: torch.Size([10, 552])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 552


===== Batch 80 =====
QIDs: [922, 922, 922, 922, 922, 922, 922, 922, 922, 922]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [397, 288, 463, 400, 372, 386, 404, 190, 469, 401]
image sizes: [(327, 152), (327, 152), (327, 152), (327, 152), (327, 152), (327, 152), (327, 152), (327, 152), (327, 152), (327, 152)]
input_ids shape: torch.Size([10, 363])
attention_mask shape: torch.Size([10, 363])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 363


===== Batch 81 =====
QIDs: [24, 24, 24, 24, 24, 24, 24, 24, 24, 24]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [210, 154, 218, 207, 187, 212, 197, 146, 205, 216]
image sizes: [(339, 150), (339, 150), (339, 150), (339, 150), (339, 150), (339, 150), (339, 150), (339, 150), (339, 150), (339, 150)]
input_ids shape: torch.Size([10, 234])
attention_mask shape: torch.Size([10, 234])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 234


===== Batch 82 =====
QIDs: [290, 290, 290, 290, 290, 290, 290, 290, 290, 290]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [954, 513, 893, 888, 784, 886, 851, 374, 940, 894]
image sizes: [(580, 572), (580, 572), (580, 572), (580, 572), (580, 572), (580, 572), (580, 572), (580, 572), (580, 572), (580, 572)]
input_ids shape: torch.Size([10, 955])
attention_mask shape: torch.Size([10, 955])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 955


===== Batch 83 =====
QIDs: [1598, 1598, 1598, 1598, 1598, 1598, 1598, 1598, 1598, 1598]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [836, 766, 835, 825, 807, 838, 830, 729, 850, 826]
image sizes: [(803, 1077), (803, 1077), (803, 1077), (803, 1077), (803, 1077), (803, 1077), (803, 1077), (803, 1077), (803, 1077), (803, 1077)]
input_ids shape: torch.Size([10, 1696])
attention_mask shape: torch.Size([10, 1696])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1696


===== Batch 84 =====
QIDs: [1631, 1631, 1631, 1631, 1631, 1631, 1631, 1631, 1631, 1631]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [144, 123, 153, 144, 136, 138, 122, 99, 151, 147]
image sizes: [(1521, 352), (1521, 352), (1521, 352), (1521, 352), (1521, 352), (1521, 352), (1521, 352), (1521, 352), (1521, 352), (1521, 352)]
input_ids shape: torch.Size([10, 813])
attention_mask shape: torch.Size([10, 813])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 813


===== Batch 85 =====
QIDs: [999, 999, 999, 999, 999, 999, 999, 999, 999, 999]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [671, 425, 629, 612, 641, 632, 617, 324, 601, 663]
image sizes: [(443, 137), (443, 137), (443, 137), (443, 137), (443, 137), (443, 137), (443, 137), (443, 137), (443, 137), (443, 137)]
input_ids shape: torch.Size([10, 593])
attention_mask shape: torch.Size([10, 593])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 593


===== Batch 86 =====
QIDs: [589, 589, 589, 589, 589, 589, 589, 589, 589, 589]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [182, 146, 170, 232, 215, 184, 170, 158, 212, 174]
image sizes: [(370, 103), (370, 103), (370, 103), (370, 103), (370, 103), (370, 103), (370, 103), (370, 103), (370, 103), (370, 103)]
input_ids shape: torch.Size([10, 214])
attention_mask shape: torch.Size([10, 214])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 214


===== Batch 87 =====
QIDs: [1465, 1465, 1465, 1465, 1465, 1465, 1465, 1465, 1465, 1465]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [243, 172, 202, 208, 529, 203, 214, 161, 195, 222]
image sizes: [(200, 119), (200, 119), (200, 119), (200, 119), (200, 119), (200, 119), (200, 119), (200, 119), (200, 119), (200, 119)]
input_ids shape: torch.Size([10, 278])
attention_mask shape: torch.Size([10, 278])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 278


===== Batch 88 =====
QIDs: [182, 182, 182, 182, 182, 182, 182, 182, 182, 182]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [246, 118, 250, 233, 171, 273, 307, 93, 302, 194]
image sizes: [(590, 290), (590, 290), (590, 290), (590, 290), (590, 290), (590, 290), (590, 290), (590, 290), (590, 290), (590, 290)]
input_ids shape: torch.Size([10, 436])
attention_mask shape: torch.Size([10, 436])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 436


===== Batch 89 =====
QIDs: [313, 313, 313, 313, 313, 313, 313, 313, 313, 313]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [282, 144, 280, 262, 277, 308, 305, 113, 306, 293]
image sizes: [(317, 240), (317, 240), (317, 240), (317, 240), (317, 240), (317, 240), (317, 240), (317, 240), (317, 240), (317, 240)]
input_ids shape: torch.Size([10, 320])
attention_mask shape: torch.Size([10, 320])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 320


===== Batch 90 =====
QIDs: [1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663, 1663]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [553, 319, 617, 750, 575, 664, 572, 222, 608, 599]
image sizes: [(428, 366), (428, 366), (428, 366), (428, 366), (428, 366), (428, 366), (428, 366), (428, 366), (428, 366), (428, 366)]
input_ids shape: torch.Size([10, 593])
attention_mask shape: torch.Size([10, 593])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 593


===== Batch 91 =====
QIDs: [1285, 1285, 1285, 1285, 1285, 1285, 1285, 1285, 1285, 1285]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [524, 247, 515, 539, 461, 467, 569, 179, 463, 451]
image sizes: [(1134, 618), (1134, 618), (1134, 618), (1134, 618), (1134, 618), (1134, 618), (1134, 618), (1134, 618), (1134, 618), (1134, 618)]
input_ids shape: torch.Size([10, 1271])
attention_mask shape: torch.Size([10, 1271])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1271


===== Batch 92 =====
QIDs: [1122, 1122, 1122, 1122, 1122, 1122, 1122, 1122, 1122, 1122]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [143, 78, 135, 150, 123, 130, 119, 58, 147, 128]
image sizes: [(508, 442), (508, 442), (508, 442), (508, 442), (508, 442), (508, 442), (508, 442), (508, 442), (508, 442), (508, 442)]
input_ids shape: torch.Size([10, 382])
attention_mask shape: torch.Size([10, 382])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 382


===== Batch 93 =====
QIDs: [855, 855, 855, 855, 855, 855, 855, 855, 855, 855]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [451, 268, 437, 440, 445, 420, 410, 203, 459, 477]
image sizes: [(1442, 208), (1442, 208), (1442, 208), (1442, 208), (1442, 208), (1442, 208), (1442, 208), (1442, 208), (1442, 208), (1442, 208)]
input_ids shape: torch.Size([10, 638])
attention_mask shape: torch.Size([10, 638])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 638


===== Batch 94 =====
QIDs: [1418, 1418, 1418, 1418, 1418, 1418, 1418, 1418, 1418, 1418]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [241, 173, 233, 233, 230, 232, 236, 152, 221, 222]
image sizes: [(301, 249), (301, 249), (301, 249), (301, 249), (301, 249), (301, 249), (301, 249), (301, 249), (301, 249), (301, 249)]
input_ids shape: torch.Size([10, 280])
attention_mask shape: torch.Size([10, 280])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 280


===== Batch 95 =====
QIDs: [245, 245, 245, 245, 245, 245, 245, 245, 245, 245]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [258, 116, 274, 265, 297, 252, 356, 100, 317, 227]
image sizes: [(891, 1090), (891, 1090), (891, 1090), (891, 1090), (891, 1090), (891, 1090), (891, 1090), (891, 1090), (891, 1090), (891, 1090)]
input_ids shape: torch.Size([10, 1508])
attention_mask shape: torch.Size([10, 1508])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1508


===== Batch 96 =====
QIDs: [231, 231, 231, 231, 231, 231, 231, 231, 231, 231]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [364, 211, 372, 454, 298, 385, 413, 177, 485, 451]
image sizes: [(462, 142), (462, 142), (462, 142), (462, 142), (462, 142), (462, 142), (462, 142), (462, 142), (462, 142), (462, 142)]
input_ids shape: torch.Size([10, 335])
attention_mask shape: torch.Size([10, 335])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 335


===== Batch 97 =====
QIDs: [1636, 1636, 1636, 1636, 1636, 1636, 1636, 1636, 1636, 1636]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [153, 91, 147, 171, 115, 148, 111, 56, 170, 139]
image sizes: [(442, 370), (442, 370), (442, 370), (442, 370), (442, 370), (442, 370), (442, 370), (442, 370), (442, 370), (442, 370)]
input_ids shape: torch.Size([10, 308])
attention_mask shape: torch.Size([10, 308])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 308


===== Batch 98 =====
QIDs: [987, 987, 987, 987, 987, 987, 987, 987, 987, 987]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [630, 271, 683, 654, 528, 698, 555, 209, 720, 613]
image sizes: [(580, 185), (580, 185), (580, 185), (580, 185), (580, 185), (580, 185), (580, 185), (580, 185), (580, 185), (580, 185)]
input_ids shape: torch.Size([10, 538])
attention_mask shape: torch.Size([10, 538])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 538


===== Batch 99 =====
QIDs: [1592, 1592, 1592, 1592, 1592, 1592, 1592, 1592, 1592, 1592]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [401, 260, 454, 439, 416, 426, 418, 208, 505, 376]
image sizes: [(278, 171), (278, 171), (278, 171), (278, 171), (278, 171), (278, 171), (278, 171), (278, 171), (278, 171), (278, 171)]
input_ids shape: torch.Size([10, 379])
attention_mask shape: torch.Size([10, 379])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 379


===== Batch 100 =====
QIDs: [803, 803, 803, 803, 803, 803, 803, 803, 803, 803]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [285, 153, 273, 254, 241, 236, 240, 99, 301, 291]
image sizes: [(1219, 676), (1219, 676), (1219, 676), (1219, 676), (1219, 676), (1219, 676), (1219, 676), (1219, 676), (1219, 676), (1219, 676)]
input_ids shape: torch.Size([10, 1204])
attention_mask shape: torch.Size([10, 1204])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1204


===== Batch 101 =====
QIDs: [1083, 1083, 1083, 1083, 1083, 1083, 1083, 1083, 1083, 1083]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [193, 131, 175, 213, 163, 172, 179, 114, 192, 197]
image sizes: [(1186, 186), (1186, 186), (1186, 186), (1186, 186), (1186, 186), (1186, 186), (1186, 186), (1186, 186), (1186, 186), (1186, 186)]
input_ids shape: torch.Size([10, 450])
attention_mask shape: torch.Size([10, 450])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 450


===== Batch 102 =====
QIDs: [243, 243, 243, 243, 243, 243, 243, 243, 243, 243]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [446, 297, 452, 439, 411, 456, 403, 260, 459, 413]
image sizes: [(516, 545), (516, 545), (516, 545), (516, 545), (516, 545), (516, 545), (516, 545), (516, 545), (516, 545), (516, 545)]
input_ids shape: torch.Size([10, 590])
attention_mask shape: torch.Size([10, 590])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 590


===== Batch 103 =====
QIDs: [969, 969, 969, 969, 969, 969, 969, 969, 969, 969]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [532, 263, 497, 494, 433, 462, 377, 198, 534, 533]
image sizes: [(174, 131), (174, 131), (174, 131), (174, 131), (174, 131), (174, 131), (174, 131), (174, 131), (174, 131), (174, 131)]
input_ids shape: torch.Size([10, 316])
attention_mask shape: torch.Size([10, 316])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 316


===== Batch 104 =====
QIDs: [436, 436, 436, 436, 436, 436, 436, 436, 436, 436]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [217, 112, 231, 216, 231, 229, 226, 106, 242, 216]
image sizes: [(200, 157), (200, 157), (200, 157), (200, 157), (200, 157), (200, 157), (200, 157), (200, 157), (200, 157), (200, 157)]
input_ids shape: torch.Size([10, 206])
attention_mask shape: torch.Size([10, 206])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 206


===== Batch 105 =====
QIDs: [988, 988, 988, 988, 988, 988, 988, 988, 988, 988]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [145, 80, 136, 144, 134, 144, 128, 52, 162, 134]
image sizes: [(506, 141), (506, 141), (506, 141), (506, 141), (506, 141), (506, 141), (506, 141), (506, 141), (506, 141), (506, 141)]
input_ids shape: torch.Size([10, 186])
attention_mask shape: torch.Size([10, 186])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 186


===== Batch 106 =====
QIDs: [959, 959, 959, 959, 959, 959, 959, 959, 959, 959]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [87, 71, 93, 90, 78, 88, 85, 63, 93, 94]
image sizes: [(346, 112), (346, 112), (346, 112), (346, 112), (346, 112), (346, 112), (346, 112), (346, 112), (346, 112), (346, 112)]
input_ids shape: torch.Size([10, 118])
attention_mask shape: torch.Size([10, 118])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 118


===== Batch 107 =====
QIDs: [1148, 1148, 1148, 1148, 1148, 1148, 1148, 1148, 1148, 1148]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [605, 317, 579, 711, 587, 551, 574, 191, 639, 691]
image sizes: [(2000, 1341), (2000, 1341), (2000, 1341), (2000, 1341), (2000, 1341), (2000, 1341), (2000, 1341), (2000, 1341), (2000, 1341), (2000, 1341)]
input_ids shape: torch.Size([10, 3757])
attention_mask shape: torch.Size([10, 3757])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 3757


===== Batch 108 =====
QIDs: [1431, 1431, 1431, 1431, 1431, 1431, 1431, 1431, 1431, 1431]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1227, 1078, 1215, 1244, 1178, 1235, 1197, 1058, 1205, 1249]
image sizes: [(271, 165), (271, 165), (271, 165), (271, 165), (271, 165), (271, 165), (271, 165), (271, 165), (271, 165), (271, 165)]
input_ids shape: torch.Size([10, 666])
attention_mask shape: torch.Size([10, 666])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 666


===== Batch 109 =====
QIDs: [367, 367, 367, 367, 367, 367, 367, 367, 367, 367]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [262, 124, 264, 259, 237, 246, 279, 100, 281, 239]
image sizes: [(771, 333), (771, 333), (771, 333), (771, 333), (771, 333), (771, 333), (771, 333), (771, 333), (771, 333), (771, 333)]
input_ids shape: torch.Size([10, 512])
attention_mask shape: torch.Size([10, 512])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 512


===== Batch 110 =====
QIDs: [66, 66, 66, 66, 66, 66, 66, 66, 66, 66]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [317, 155, 352, 304, 274, 325, 283, 123, 366, 347]
image sizes: [(1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560)]
input_ids shape: torch.Size([10, 6487])
attention_mask shape: torch.Size([10, 6487])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 6487


===== Batch 111 =====
QIDs: [945, 945, 945, 945, 945, 945, 945, 945, 945, 945]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [747, 385, 703, 723, 637, 758, 715, 263, 883, 720]
image sizes: [(1400, 1772), (1400, 1772), (1400, 1772), (1400, 1772), (1400, 1772), (1400, 1772), (1400, 1772), (1400, 1772), (1400, 1772), (1400, 1772)]
input_ids shape: torch.Size([10, 3607])
attention_mask shape: torch.Size([10, 3607])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 3607


===== Batch 112 =====
QIDs: [1292, 1292, 1292, 1292, 1292, 1292, 1292, 1292, 1292, 1292]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1110, 580, 976, 1276, 939, 1015, 994, 358, 1083, 1000]
image sizes: [(672, 558), (672, 558), (672, 558), (672, 558), (672, 558), (672, 558), (672, 558), (672, 558), (672, 558), (672, 558)]
input_ids shape: torch.Size([10, 1077])
attention_mask shape: torch.Size([10, 1077])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1077


===== Batch 113 =====
QIDs: [1718, 1718, 1718, 1718, 1718, 1718, 1718, 1718, 1718, 1718]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [259, 131, 255, 269, 222, 242, 286, 106, 295, 246]
image sizes: [(1280, 861), (1280, 861), (1280, 861), (1280, 861), (1280, 861), (1280, 861), (1280, 861), (1280, 861), (1280, 861), (1280, 861)]
input_ids shape: torch.Size([10, 1606])
attention_mask shape: torch.Size([10, 1606])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1606


===== Batch 114 =====
QIDs: [429, 429, 429, 429, 429, 429, 429, 429, 429, 429]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [275, 151, 248, 278, 222, 258, 250, 110, 268, 269]
image sizes: [(200, 144), (200, 144), (200, 144), (200, 144), (200, 144), (200, 144), (200, 144), (200, 144), (200, 144), (200, 144)]
input_ids shape: torch.Size([10, 189])
attention_mask shape: torch.Size([10, 189])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 189


===== Batch 115 =====
QIDs: [1020, 1020, 1020, 1020, 1020, 1020, 1020, 1020, 1020, 1020]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [422, 305, 400, 441, 373, 414, 382, 292, 420, 420]
image sizes: [(260, 324), (260, 324), (260, 324), (260, 324), (260, 324), (260, 324), (260, 324), (260, 324), (260, 324), (260, 324)]
input_ids shape: torch.Size([10, 313])
attention_mask shape: torch.Size([10, 313])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 313


===== Batch 116 =====
QIDs: [1508, 1508, 1508, 1508, 1508, 1508, 1508, 1508, 1508, 1508]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [543, 299, 548, 622, 467, 508, 475, 230, 562, 554]
image sizes: [(1065, 748), (1065, 748), (1065, 748), (1065, 748), (1065, 748), (1065, 748), (1065, 748), (1065, 748), (1065, 748), (1065, 748)]
input_ids shape: torch.Size([10, 1359])
attention_mask shape: torch.Size([10, 1359])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1359


===== Batch 117 =====
QIDs: [902, 902, 902, 902, 902, 902, 902, 902, 902, 902]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [87, 59, 84, 84, 76, 87, 72, 44, 87, 74]
image sizes: [(511, 188), (511, 188), (511, 188), (511, 188), (511, 188), (511, 188), (511, 188), (511, 188), (511, 188), (511, 188)]
input_ids shape: torch.Size([10, 192])
attention_mask shape: torch.Size([10, 192])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 192


===== Batch 118 =====
QIDs: [13, 13, 13, 13, 13, 13, 13, 13, 13, 13]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [478, 323, 462, 486, 416, 434, 455, 261, 486, 507]
image sizes: [(700, 160), (700, 160), (700, 160), (700, 160), (700, 160), (700, 160), (700, 160), (700, 160), (700, 160), (700, 160)]
input_ids shape: torch.Size([10, 461])
attention_mask shape: torch.Size([10, 461])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 461


===== Batch 119 =====
QIDs: [862, 862, 862, 862, 862, 862, 862, 862, 862, 862]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [699, 348, 687, 709, 671, 676, 641, 244, 756, 762]
image sizes: [(1434, 250), (1434, 250), (1434, 250), (1434, 250), (1434, 250), (1434, 250), (1434, 250), (1434, 250), (1434, 250), (1434, 250)]
input_ids shape: torch.Size([10, 831])
attention_mask shape: torch.Size([10, 831])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 831


===== Batch 120 =====
QIDs: [910, 910, 910, 910, 910, 910, 910, 910, 910, 910]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [717, 541, 685, 703, 642, 701, 672, 501, 659, 701]
image sizes: [(1211, 726), (1211, 726), (1211, 726), (1211, 726), (1211, 726), (1211, 726), (1211, 726), (1211, 726), (1211, 726), (1211, 726)]
input_ids shape: torch.Size([10, 1567])
attention_mask shape: torch.Size([10, 1567])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1567


===== Batch 121 =====
QIDs: [155, 155, 155, 155, 155, 155, 155, 155, 155, 155]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [279, 150, 308, 271, 262, 327, 240, 108, 320, 281]
image sizes: [(990, 693), (990, 693), (990, 693), (990, 693), (990, 693), (990, 693), (990, 693), (990, 693), (990, 693), (990, 693)]
input_ids shape: torch.Size([10, 1049])
attention_mask shape: torch.Size([10, 1049])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1049


===== Batch 122 =====
QIDs: [600, 600, 600, 600, 600, 600, 600, 600, 600, 600]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1083, 616, 1217, 945, 1127, 1081, 888, 560, 1013, 1247]
image sizes: [(742, 252), (742, 252), (742, 252), (742, 252), (742, 252), (742, 252), (742, 252), (742, 252), (742, 252), (742, 252)]
input_ids shape: torch.Size([10, 931])
attention_mask shape: torch.Size([10, 931])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 931


===== Batch 123 =====
QIDs: [722, 722, 722, 722, 722, 722, 722, 722, 722, 722]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [362, 179, 427, 403, 298, 398, 340, 137, 402, 377]
image sizes: [(474, 673), (474, 673), (474, 673), (474, 673), (474, 673), (474, 673), (474, 673), (474, 673), (474, 673), (474, 673)]
input_ids shape: torch.Size([10, 636])
attention_mask shape: torch.Size([10, 636])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 636


===== Batch 124 =====
QIDs: [1668, 1668, 1668, 1668, 1668, 1668, 1668, 1668, 1668, 1668]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [236, 193, 225, 244, 240, 222, 229, 188, 229, 233]
image sizes: [(360, 206), (360, 206), (360, 206), (360, 206), (360, 206), (360, 206), (360, 206), (360, 206), (360, 206), (360, 206)]
input_ids shape: torch.Size([10, 229])
attention_mask shape: torch.Size([10, 229])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 229


===== Batch 125 =====
QIDs: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [488, 249, 471, 505, 417, 459, 447, 176, 508, 530]
image sizes: [(592, 139), (592, 139), (592, 139), (592, 139), (592, 139), (592, 139), (592, 139), (592, 139), (592, 139), (592, 139)]
input_ids shape: torch.Size([10, 385])
attention_mask shape: torch.Size([10, 385])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 385


===== Batch 126 =====
QIDs: [348, 348, 348, 348, 348, 348, 348, 348, 348, 348]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [378, 230, 377, 378, 347, 372, 334, 190, 421, 373]
image sizes: [(1450, 218), (1450, 218), (1450, 218), (1450, 218), (1450, 218), (1450, 218), (1450, 218), (1450, 218), (1450, 218), (1450, 218)]
input_ids shape: torch.Size([10, 633])
attention_mask shape: torch.Size([10, 633])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 633


===== Batch 127 =====
QIDs: [1035, 1035, 1035, 1035, 1035, 1035, 1035, 1035, 1035, 1035]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [272, 187, 282, 312, 278, 273, 258, 134, 288, 254]
image sizes: [(466, 238), (466, 238), (466, 238), (466, 238), (466, 238), (466, 238), (466, 238), (466, 238), (466, 238), (466, 238)]
input_ids shape: torch.Size([10, 325])
attention_mask shape: torch.Size([10, 325])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 325


===== Batch 128 =====
QIDs: [1212, 1212, 1212, 1212, 1212, 1212, 1212, 1212, 1212, 1212]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [696, 448, 663, 701, 636, 646, 632, 371, 705, 738]
image sizes: [(599, 191), (599, 191), (599, 191), (599, 191), (599, 191), (599, 191), (599, 191), (599, 191), (599, 191), (599, 191)]
input_ids shape: torch.Size([10, 581])
attention_mask shape: torch.Size([10, 581])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 581


===== Batch 129 =====
QIDs: [575, 575, 575, 575, 575, 575, 575, 575, 575, 575]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [398, 257, 369, 424, 375, 395, 381, 215, 431, 396]
image sizes: [(433, 180), (433, 180), (433, 180), (433, 180), (433, 180), (433, 180), (433, 180), (433, 180), (433, 180), (433, 180)]
input_ids shape: torch.Size([10, 360])
attention_mask shape: torch.Size([10, 360])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 360


===== Batch 130 =====
QIDs: [1167, 1167, 1167, 1167, 1167, 1167, 1167, 1167, 1167, 1167]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [686, 442, 673, 671, 572, 692, 628, 327, 691, 664]
image sizes: [(741, 619), (741, 619), (741, 619), (741, 619), (741, 619), (741, 619), (741, 619), (741, 619), (741, 619), (741, 619)]
input_ids shape: torch.Size([10, 1011])
attention_mask shape: torch.Size([10, 1011])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1011


===== Batch 131 =====
QIDs: [1121, 1121, 1121, 1121, 1121, 1121, 1121, 1121, 1121, 1121]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [225, 119, 195, 232, 181, 233, 198, 94, 222, 256]
image sizes: [(750, 468), (750, 468), (750, 468), (750, 468), (750, 468), (750, 468), (750, 468), (750, 468), (750, 468), (750, 468)]
input_ids shape: torch.Size([10, 600])
attention_mask shape: torch.Size([10, 600])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 600


===== Batch 132 =====
QIDs: [1409, 1409, 1409, 1409, 1409, 1409, 1409, 1409, 1409, 1409]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [286, 244, 277, 291, 276, 279, 266, 227, 287, 290]
image sizes: [(601, 263), (601, 263), (601, 263), (601, 263), (601, 263), (601, 263), (601, 263), (601, 263), (601, 263), (601, 263)]
input_ids shape: torch.Size([10, 399])
attention_mask shape: torch.Size([10, 399])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 399


===== Batch 133 =====
QIDs: [1615, 1615, 1615, 1615, 1615, 1615, 1615, 1615, 1615, 1615]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [100, 59, 81, 75, 91, 87, 74, 49, 90, 77]
image sizes: [(1188, 136), (1188, 136), (1188, 136), (1188, 136), (1188, 136), (1188, 136), (1188, 136), (1188, 136), (1188, 136), (1188, 136)]
input_ids shape: torch.Size([10, 281])
attention_mask shape: torch.Size([10, 281])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 281


===== Batch 134 =====
QIDs: [977, 977, 977, 977, 977, 977, 977, 977, 977, 977]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [96, 70, 91, 91, 79, 98, 87, 61, 102, 84]
image sizes: [(183, 118), (183, 118), (183, 118), (183, 118), (183, 118), (183, 118), (183, 118), (183, 118), (183, 118), (183, 118)]
input_ids shape: torch.Size([10, 104])
attention_mask shape: torch.Size([10, 104])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 104


===== Batch 135 =====
QIDs: [361, 361, 361, 361, 361, 361, 361, 361, 361, 361]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [116, 80, 118, 124, 115, 107, 119, 59, 130, 144]
image sizes: [(1064, 564), (1064, 564), (1064, 564), (1064, 564), (1064, 564), (1064, 564), (1064, 564), (1064, 564), (1064, 564), (1064, 564)]
input_ids shape: torch.Size([10, 853])
attention_mask shape: torch.Size([10, 853])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 853


===== Batch 136 =====
QIDs: [726, 726, 726, 726, 726, 726, 726, 726, 726, 726]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [268, 154, 272, 305, 266, 291, 258, 106, 282, 282]
image sizes: [(442, 365), (442, 365), (442, 365), (442, 365), (442, 365), (442, 365), (442, 365), (442, 365), (442, 365), (442, 365)]
input_ids shape: torch.Size([10, 385])
attention_mask shape: torch.Size([10, 385])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 385


===== Batch 137 =====
QIDs: [1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500, 1500]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [291, 164, 322, 309, 560, 300, 257, 122, 326, 309]
image sizes: [(571, 342), (571, 342), (571, 342), (571, 342), (571, 342), (571, 342), (571, 342), (571, 342), (571, 342), (571, 342)]
input_ids shape: torch.Size([10, 453])
attention_mask shape: torch.Size([10, 453])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 453


===== Batch 138 =====
QIDs: [706, 706, 706, 706, 706, 706, 706, 706, 706, 706]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [482, 308, 494, 587, 465, 487, 488, 241, 466, 479]
image sizes: [(574, 620), (574, 620), (574, 620), (574, 620), (574, 620), (574, 620), (574, 620), (574, 620), (574, 620), (574, 620)]
input_ids shape: torch.Size([10, 784])
attention_mask shape: torch.Size([10, 784])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 784


===== Batch 139 =====
QIDs: [519, 519, 519, 519, 519, 519, 519, 519, 519, 519]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [395, 377, 396, 415, 408, 403, 383, 359, 399, 392]
image sizes: [(457, 188), (457, 188), (457, 188), (457, 188), (457, 188), (457, 188), (457, 188), (457, 188), (457, 188), (457, 188)]
input_ids shape: torch.Size([10, 371])
attention_mask shape: torch.Size([10, 371])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 371


===== Batch 140 =====
QIDs: [357, 357, 357, 357, 357, 357, 357, 357, 357, 357]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [123, 85, 120, 111, 116, 123, 96, 64, 130, 121]
image sizes: [(716, 736), (716, 736), (716, 736), (716, 736), (716, 736), (716, 736), (716, 736), (716, 736), (716, 736), (716, 736)]
input_ids shape: torch.Size([10, 757])
attention_mask shape: torch.Size([10, 757])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 757


===== Batch 141 =====
QIDs: [76, 76, 76, 76, 76, 76, 76, 76, 76, 76]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [474, 432, 471, 462, 460, 473, 448, 423, 471, 472]
image sizes: [(314, 159), (314, 159), (314, 159), (314, 159), (314, 159), (314, 159), (314, 159), (314, 159), (314, 159), (314, 159)]
input_ids shape: torch.Size([10, 483])
attention_mask shape: torch.Size([10, 483])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 483


===== Batch 142 =====
QIDs: [739, 739, 739, 739, 739, 739, 739, 739, 739, 739]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [346, 178, 326, 366, 301, 340, 332, 120, 404, 322]
image sizes: [(1080, 1920), (1080, 1920), (1080, 1920), (1080, 1920), (1080, 1920), (1080, 1920), (1080, 1920), (1080, 1920), (1080, 1920), (1080, 1920)]
input_ids shape: torch.Size([10, 2896])
attention_mask shape: torch.Size([10, 2896])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2896


===== Batch 143 =====
QIDs: [1120, 1120, 1120, 1120, 1120, 1120, 1120, 1120, 1120, 1120]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [900, 355, 771, 970, 678, 797, 742, 247, 871, 942]
image sizes: [(1008, 698), (1008, 698), (1008, 698), (1008, 698), (1008, 698), (1008, 698), (1008, 698), (1008, 698), (1008, 698), (1008, 698)]
input_ids shape: torch.Size([10, 1338])
attention_mask shape: torch.Size([10, 1338])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1338


===== Batch 144 =====
QIDs: [1454, 1454, 1454, 1454, 1454, 1454, 1454, 1454, 1454, 1454]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [465, 328, 491, 486, 431, 459, 407, 267, 499, 491]
image sizes: [(567, 376), (567, 376), (567, 376), (567, 376), (567, 376), (567, 376), (567, 376), (567, 376), (567, 376), (567, 376)]
input_ids shape: torch.Size([10, 573])
attention_mask shape: torch.Size([10, 573])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 573


===== Batch 145 =====
QIDs: [518, 518, 518, 518, 518, 518, 518, 518, 518, 518]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [311, 276, 305, 310, 294, 306, 292, 261, 310, 302]
image sizes: [(278, 237), (278, 237), (278, 237), (278, 237), (278, 237), (278, 237), (278, 237), (278, 237), (278, 237), (278, 237)]
input_ids shape: torch.Size([10, 304])
attention_mask shape: torch.Size([10, 304])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 304


===== Batch 146 =====
QIDs: [72, 72, 72, 72, 72, 72, 72, 72, 72, 72]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [363, 187, 373, 377, 336, 324, 318, 135, 394, 365]
image sizes: [(2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536)]
input_ids shape: torch.Size([10, 4235])
attention_mask shape: torch.Size([10, 4235])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 4235


===== Batch 147 =====
QIDs: [1074, 1074, 1074, 1074, 1074, 1074, 1074, 1074, 1074, 1074]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [2416, 1128, 2429, 2654, 2096, 2381, 2028, 697, 2704, 2548]
image sizes: [(968, 218), (968, 218), (968, 218), (968, 218), (968, 218), (968, 218), (968, 218), (968, 218), (968, 218), (968, 218)]
input_ids shape: torch.Size([10, 1505])
attention_mask shape: torch.Size([10, 1505])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1505


===== Batch 148 =====
QIDs: [770, 770, 770, 770, 770, 770, 770, 770, 770, 770]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [477, 363, 448, 452, 411, 456, 418, 326, 490, 482]
image sizes: [(483, 851), (483, 851), (483, 851), (483, 851), (483, 851), (483, 851), (483, 851), (483, 851), (483, 851), (483, 851)]
input_ids shape: torch.Size([10, 759])
attention_mask shape: torch.Size([10, 759])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 759


===== Batch 149 =====
QIDs: [1160, 1160, 1160, 1160, 1160, 1160, 1160, 1160, 1160, 1160]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [389, 244, 383, 421, 430, 383, 325, 202, 417, 384]
image sizes: [(727, 172), (727, 172), (727, 172), (727, 172), (727, 172), (727, 172), (727, 172), (727, 172), (727, 172), (727, 172)]
input_ids shape: torch.Size([10, 417])
attention_mask shape: torch.Size([10, 417])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 417


===== Batch 150 =====
QIDs: [1344, 1344, 1344, 1344, 1344, 1344, 1344, 1344, 1344, 1344]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [317, 217, 311, 321, 290, 323, 277, 177, 343, 323]
image sizes: [(496, 328), (496, 328), (496, 328), (496, 328), (496, 328), (496, 328), (496, 328), (496, 328), (496, 328), (496, 328)]
input_ids shape: torch.Size([10, 425])
attention_mask shape: torch.Size([10, 425])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 425


===== Batch 151 =====
QIDs: [1606, 1606, 1606, 1606, 1606, 1606, 1606, 1606, 1606, 1606]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [111, 91, 108, 119, 103, 112, 102, 75, 124, 108]
image sizes: [(124, 184), (124, 184), (124, 184), (124, 184), (124, 184), (124, 184), (124, 184), (124, 184), (124, 184), (124, 184)]
input_ids shape: torch.Size([10, 108])
attention_mask shape: torch.Size([10, 108])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 108


===== Batch 152 =====
QIDs: [1411, 1411, 1411, 1411, 1411, 1411, 1411, 1411, 1411, 1411]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [304, 270, 293, 316, 285, 294, 289, 257, 321, 294]
image sizes: [(383, 153), (383, 153), (383, 153), (383, 153), (383, 153), (383, 153), (383, 153), (383, 153), (383, 153), (383, 153)]
input_ids shape: torch.Size([10, 279])
attention_mask shape: torch.Size([10, 279])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 279


===== Batch 153 =====
QIDs: [541, 541, 541, 541, 541, 541, 541, 541, 541, 541]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [656, 619, 657, 683, 631, 652, 628, 595, 662, 694]
image sizes: [(548, 164), (548, 164), (548, 164), (548, 164), (548, 164), (548, 164), (548, 164), (548, 164), (548, 164), (548, 164)]
input_ids shape: torch.Size([10, 548])
attention_mask shape: torch.Size([10, 548])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 548


===== Batch 154 =====
QIDs: [1124, 1124, 1124, 1124, 1124, 1124, 1124, 1124, 1124, 1124]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1042, 432, 897, 1080, 747, 947, 886, 294, 1025, 1081]
image sizes: [(1024, 494), (1024, 494), (1024, 494), (1024, 494), (1024, 494), (1024, 494), (1024, 494), (1024, 494), (1024, 494), (1024, 494)]
input_ids shape: torch.Size([10, 1150])
attention_mask shape: torch.Size([10, 1150])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1150


===== Batch 155 =====
QIDs: [529, 529, 529, 529, 529, 529, 529, 529, 529, 529]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [254, 238, 257, 260, 250, 257, 248, 220, 255, 260]
image sizes: [(501, 114), (501, 114), (501, 114), (501, 114), (501, 114), (501, 114), (501, 114), (501, 114), (501, 114), (501, 114)]
input_ids shape: torch.Size([10, 253])
attention_mask shape: torch.Size([10, 253])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 253


===== Batch 156 =====
QIDs: [560, 560, 560, 560, 560, 560, 560, 560, 560, 560]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [319, 230, 294, 332, 380, 298, 352, 184, 311, 311]
image sizes: [(375, 386), (375, 386), (375, 386), (375, 386), (375, 386), (375, 386), (375, 386), (375, 386), (375, 386), (375, 386)]
input_ids shape: torch.Size([10, 456])
attention_mask shape: torch.Size([10, 456])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 456


===== Batch 157 =====
QIDs: [350, 350, 350, 350, 350, 350, 350, 350, 350, 350]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [75, 61, 109, 74, 171, 77, 68, 44, 54, 91]
image sizes: [(401, 309), (401, 309), (401, 309), (401, 309), (401, 309), (401, 309), (401, 309), (401, 309), (401, 309), (401, 309)]
input_ids shape: torch.Size([10, 235])
attention_mask shape: torch.Size([10, 235])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 235


===== Batch 158 =====
QIDs: [1076, 1076, 1076, 1076, 1076, 1076, 1076, 1076, 1076, 1076]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [3345, 1573, 3190, 3421, 2794, 3152, 2690, 1016, 3438, 3427]
image sizes: [(674, 486), (674, 486), (674, 486), (674, 486), (674, 486), (674, 486), (674, 486), (674, 486), (674, 486), (674, 486)]
input_ids shape: torch.Size([10, 1959])
attention_mask shape: torch.Size([10, 1959])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1959


===== Batch 159 =====
QIDs: [378, 378, 378, 378, 378, 378, 378, 378, 378, 378]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [820, 689, 844, 840, 798, 855, 813, 655, 865, 859]
image sizes: [(600, 173), (600, 173), (600, 173), (600, 173), (600, 173), (600, 173), (600, 173), (600, 173), (600, 173), (600, 173)]
input_ids shape: torch.Size([10, 592])
attention_mask shape: torch.Size([10, 592])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 592


===== Batch 160 =====
QIDs: [1481, 1481, 1481, 1481, 1481, 1481, 1481, 1481, 1481, 1481]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [303, 165, 292, 336, 234, 287, 223, 109, 321, 284]
image sizes: [(500, 353), (500, 353), (500, 353), (500, 353), (500, 353), (500, 353), (500, 353), (500, 353), (500, 353), (500, 353)]
input_ids shape: torch.Size([10, 416])
attention_mask shape: torch.Size([10, 416])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 416


===== Batch 161 =====
QIDs: [185, 185, 185, 185, 185, 185, 185, 185, 185, 185]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [128, 71, 118, 120, 107, 123, 100, 58, 108, 113]
image sizes: [(200, 196), (200, 196), (200, 196), (200, 196), (200, 196), (200, 196), (200, 196), (200, 196), (200, 196), (200, 196)]
input_ids shape: torch.Size([10, 130])
attention_mask shape: torch.Size([10, 130])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 130


===== Batch 162 =====
QIDs: [875, 875, 875, 875, 875, 875, 875, 875, 875, 875]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [549, 281, 491, 495, 452, 487, 426, 216, 544, 513]
image sizes: [(472, 262), (472, 262), (472, 262), (472, 262), (472, 262), (472, 262), (472, 262), (472, 262), (472, 262), (472, 262)]
input_ids shape: torch.Size([10, 466])
attention_mask shape: torch.Size([10, 466])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 466


===== Batch 163 =====
QIDs: [878, 878, 878, 878, 878, 878, 878, 878, 878, 878]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [210, 155, 181, 236, 193, 180, 194, 144, 194, 193]
image sizes: [(404, 342), (404, 342), (404, 342), (404, 342), (404, 342), (404, 342), (404, 342), (404, 342), (404, 342), (404, 342)]
input_ids shape: torch.Size([10, 334])
attention_mask shape: torch.Size([10, 334])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 334


===== Batch 164 =====
QIDs: [539, 539, 539, 539, 539, 539, 539, 539, 539, 539]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [267, 231, 261, 296, 251, 260, 256, 205, 268, 285]
image sizes: [(419, 130), (419, 130), (419, 130), (419, 130), (419, 130), (419, 130), (419, 130), (419, 130), (419, 130), (419, 130)]
input_ids shape: torch.Size([10, 285])
attention_mask shape: torch.Size([10, 285])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 285


===== Batch 165 =====
QIDs: [1099, 1099, 1099, 1099, 1099, 1099, 1099, 1099, 1099, 1099]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [379, 206, 362, 400, 316, 372, 330, 170, 415, 376]
image sizes: [(608, 198), (608, 198), (608, 198), (608, 198), (608, 198), (608, 198), (608, 198), (608, 198), (608, 198), (608, 198)]
input_ids shape: torch.Size([10, 398])
attention_mask shape: torch.Size([10, 398])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 398


===== Batch 166 =====
QIDs: [1680, 1680, 1680, 1680, 1680, 1680, 1680, 1680, 1680, 1680]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [160, 127, 170, 168, 140, 155, 138, 86, 180, 149]
image sizes: [(394, 284), (394, 284), (394, 284), (394, 284), (394, 284), (394, 284), (394, 284), (394, 284), (394, 284), (394, 284)]
input_ids shape: torch.Size([10, 244])
attention_mask shape: torch.Size([10, 244])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 244


===== Batch 167 =====
QIDs: [1412, 1412, 1412, 1412, 1412, 1412, 1412, 1412, 1412, 1412]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [340, 313, 344, 368, 343, 331, 332, 288, 349, 356]
image sizes: [(404, 156), (404, 156), (404, 156), (404, 156), (404, 156), (404, 156), (404, 156), (404, 156), (404, 156), (404, 156)]
input_ids shape: torch.Size([10, 307])
attention_mask shape: torch.Size([10, 307])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 307


===== Batch 168 =====
QIDs: [919, 919, 919, 919, 919, 919, 919, 919, 919, 919]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [557, 355, 515, 538, 452, 515, 489, 305, 562, 545]
image sizes: [(794, 304), (794, 304), (794, 304), (794, 304), (794, 304), (794, 304), (794, 304), (794, 304), (794, 304), (794, 304)]
input_ids shape: torch.Size([10, 631])
attention_mask shape: torch.Size([10, 631])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 631


===== Batch 169 =====
QIDs: [1302, 1302, 1302, 1302, 1302, 1302, 1302, 1302, 1302, 1302]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [333, 241, 313, 321, 296, 310, 296, 206, 360, 325]
image sizes: [(708, 124), (708, 124), (708, 124), (708, 124), (708, 124), (708, 124), (708, 124), (708, 124), (708, 124), (708, 124)]
input_ids shape: torch.Size([10, 331])
attention_mask shape: torch.Size([10, 331])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 331


===== Batch 170 =====
QIDs: [1051, 1051, 1051, 1051, 1051, 1051, 1051, 1051, 1051, 1051]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1789, 820, 1764, 1813, 1553, 1639, 1443, 503, 1827, 1897]
image sizes: [(1204, 928), (1204, 928), (1204, 928), (1204, 928), (1204, 928), (1204, 928), (1204, 928), (1204, 928), (1204, 928), (1204, 928)]
input_ids shape: torch.Size([10, 2253])
attention_mask shape: torch.Size([10, 2253])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2253


===== Batch 171 =====
QIDs: [1173, 1173, 1173, 1173, 1173, 1173, 1173, 1173, 1173, 1173]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [536, 276, 497, 549, 435, 502, 549, 215, 547, 567]
image sizes: [(1219, 217), (1219, 217), (1219, 217), (1219, 217), (1219, 217), (1219, 217), (1219, 217), (1219, 217), (1219, 217), (1219, 217)]
input_ids shape: torch.Size([10, 688])
attention_mask shape: torch.Size([10, 688])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 688


===== Batch 172 =====
QIDs: [965, 965, 965, 965, 965, 965, 965, 965, 965, 965]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [122, 82, 118, 121, 108, 114, 106, 68, 127, 131]
image sizes: [(468, 268), (468, 268), (468, 268), (468, 268), (468, 268), (468, 268), (468, 268), (468, 268), (468, 268), (468, 268)]
input_ids shape: torch.Size([10, 249])
attention_mask shape: torch.Size([10, 249])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 249


===== Batch 173 =====
QIDs: [1685, 1685, 1685, 1685, 1685, 1685, 1685, 1685, 1685, 1685]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1439, 725, 1364, 1502, 1244, 1326, 1354, 525, 1456, 1512]
image sizes: [(1042, 586), (1042, 586), (1042, 586), (1042, 586), (1042, 586), (1042, 586), (1042, 586), (1042, 586), (1042, 586), (1042, 586)]
input_ids shape: torch.Size([10, 1543])
attention_mask shape: torch.Size([10, 1543])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1543


===== Batch 174 =====
QIDs: [912, 912, 912, 912, 912, 912, 912, 912, 912, 912]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1253, 675, 1169, 1217, 1046, 1121, 1112, 429, 1240, 1157]
image sizes: [(406, 160), (406, 160), (406, 160), (406, 160), (406, 160), (406, 160), (406, 160), (406, 160), (406, 160), (406, 160)]
input_ids shape: torch.Size([10, 802])
attention_mask shape: torch.Size([10, 802])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 802


===== Batch 175 =====
QIDs: [228, 228, 228, 228, 228, 228, 228, 228, 228, 228]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [172, 97, 176, 180, 171, 189, 160, 86, 187, 174]
image sizes: [(718, 1126), (718, 1126), (718, 1126), (718, 1126), (718, 1126), (718, 1126), (718, 1126), (718, 1126), (718, 1126), (718, 1126)]
input_ids shape: torch.Size([10, 1154])
attention_mask shape: torch.Size([10, 1154])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1154


===== Batch 176 =====
QIDs: [1101, 1101, 1101, 1101, 1101, 1101, 1101, 1101, 1101, 1101]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [658, 326, 625, 594, 588, 645, 530, 246, 686, 685]
image sizes: [(836, 272), (836, 272), (836, 272), (836, 272), (836, 272), (836, 272), (836, 272), (836, 272), (836, 272), (836, 272)]
input_ids shape: torch.Size([10, 687])
attention_mask shape: torch.Size([10, 687])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 687


===== Batch 177 =====
QIDs: [852, 852, 852, 852, 852, 852, 852, 852, 852, 852]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [495, 426, 493, 510, 469, 484, 508, 409, 494, 495]
image sizes: [(498, 433), (498, 433), (498, 433), (498, 433), (498, 433), (498, 433), (498, 433), (498, 433), (498, 433), (498, 433)]
input_ids shape: torch.Size([10, 658])
attention_mask shape: torch.Size([10, 658])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 658


===== Batch 178 =====
QIDs: [994, 994, 994, 994, 994, 994, 994, 994, 994, 994]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [767, 413, 912, 824, 713, 888, 706, 303, 982, 837]
image sizes: [(483, 433), (483, 433), (483, 433), (483, 433), (483, 433), (483, 433), (483, 433), (483, 433), (483, 433), (483, 433)]
input_ids shape: torch.Size([10, 750])
attention_mask shape: torch.Size([10, 750])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 750


===== Batch 179 =====
QIDs: [59, 59, 59, 59, 59, 59, 59, 59, 59, 59]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [450, 242, 475, 454, 376, 439, 398, 167, 490, 474]
image sizes: [(2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920)]
input_ids shape: torch.Size([10, 6548])
attention_mask shape: torch.Size([10, 6548])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 6548


===== Batch 180 =====
QIDs: [1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037, 1037]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [420, 192, 394, 429, 355, 385, 383, 153, 402, 378]
image sizes: [(254, 256), (254, 256), (254, 256), (254, 256), (254, 256), (254, 256), (254, 256), (254, 256), (254, 256), (254, 256)]
input_ids shape: torch.Size([10, 315])
attention_mask shape: torch.Size([10, 315])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 315


===== Batch 181 =====
QIDs: [1632, 1632, 1632, 1632, 1632, 1632, 1632, 1632, 1632, 1632]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [120, 100, 118, 140, 160, 122, 119, 95, 128, 134]
image sizes: [(212, 150), (212, 150), (212, 150), (212, 150), (212, 150), (212, 150), (212, 150), (212, 150), (212, 150), (212, 150)]
input_ids shape: torch.Size([10, 170])
attention_mask shape: torch.Size([10, 170])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 170


===== Batch 182 =====
QIDs: [224, 224, 224, 224, 224, 224, 224, 224, 224, 224]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [255, 112, 265, 250, 240, 286, 224, 92, 281, 211]
image sizes: [(349, 426), (349, 426), (349, 426), (349, 426), (349, 426), (349, 426), (349, 426), (349, 426), (349, 426), (349, 426)]
input_ids shape: torch.Size([10, 342])
attention_mask shape: torch.Size([10, 342])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 342


===== Batch 183 =====
QIDs: [851, 851, 851, 851, 851, 851, 851, 851, 851, 851]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [434, 337, 400, 447, 391, 424, 416, 310, 444, 413]
image sizes: [(478, 303), (478, 303), (478, 303), (478, 303), (478, 303), (478, 303), (478, 303), (478, 303), (478, 303), (478, 303)]
input_ids shape: torch.Size([10, 442])
attention_mask shape: torch.Size([10, 442])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 442


===== Batch 184 =====
QIDs: [1622, 1622, 1622, 1622, 1622, 1622, 1622, 1622, 1622, 1622]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [158, 91, 148, 164, 124, 155, 153, 92, 216, 125]
image sizes: [(397, 164), (397, 164), (397, 164), (397, 164), (397, 164), (397, 164), (397, 164), (397, 164), (397, 164), (397, 164)]
input_ids shape: torch.Size([10, 214])
attention_mask shape: torch.Size([10, 214])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 214


===== Batch 185 =====
QIDs: [207, 207, 207, 207, 207, 207, 207, 207, 207, 207]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [180, 117, 162, 168, 148, 156, 152, 112, 158, 164]
image sizes: [(511, 400), (511, 400), (511, 400), (511, 400), (511, 400), (511, 400), (511, 400), (511, 400), (511, 400), (511, 400)]
input_ids shape: torch.Size([10, 389])
attention_mask shape: torch.Size([10, 389])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 389


===== Batch 186 =====
QIDs: [1601, 1601, 1601, 1601, 1601, 1601, 1601, 1601, 1601, 1601]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [677, 492, 702, 707, 590, 638, 599, 421, 706, 617]
image sizes: [(545, 308), (545, 308), (545, 308), (545, 308), (545, 308), (545, 308), (545, 308), (545, 308), (545, 308), (545, 308)]
input_ids shape: torch.Size([10, 649])
attention_mask shape: torch.Size([10, 649])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 649


===== Batch 187 =====
QIDs: [1492, 1492, 1492, 1492, 1492, 1492, 1492, 1492, 1492, 1492]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [259, 117, 251, 278, 218, 256, 233, 103, 255, 242]
image sizes: [(500, 314), (500, 314), (500, 314), (500, 314), (500, 314), (500, 314), (500, 314), (500, 314), (500, 314), (500, 314)]
input_ids shape: torch.Size([10, 358])
attention_mask shape: torch.Size([10, 358])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 358


===== Batch 188 =====
QIDs: [1296, 1296, 1296, 1296, 1296, 1296, 1296, 1296, 1296, 1296]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [132, 100, 124, 113, 123, 126, 123, 81, 132, 138]
image sizes: [(133, 151), (133, 151), (133, 151), (133, 151), (133, 151), (133, 151), (133, 151), (133, 151), (133, 151), (133, 151)]
input_ids shape: torch.Size([10, 113])
attention_mask shape: torch.Size([10, 113])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 113


===== Batch 189 =====
QIDs: [1115, 1115, 1115, 1115, 1115, 1115, 1115, 1115, 1115, 1115]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [144, 78, 127, 162, 131, 139, 138, 58, 162, 145]
image sizes: [(780, 401), (780, 401), (780, 401), (780, 401), (780, 401), (780, 401), (780, 401), (780, 401), (780, 401), (780, 401)]
input_ids shape: torch.Size([10, 489])
attention_mask shape: torch.Size([10, 489])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 489


===== Batch 190 =====
QIDs: [1213, 1213, 1213, 1213, 1213, 1213, 1213, 1213, 1213, 1213]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [906, 765, 890, 917, 849, 883, 871, 714, 927, 882]
image sizes: [(425, 282), (425, 282), (425, 282), (425, 282), (425, 282), (425, 282), (425, 282), (425, 282), (425, 282), (425, 282)]
input_ids shape: torch.Size([10, 788])
attention_mask shape: torch.Size([10, 788])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 788


===== Batch 191 =====
QIDs: [1345, 1345, 1345, 1345, 1345, 1345, 1345, 1345, 1345, 1345]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [289, 223, 288, 287, 263, 285, 269, 197, 283, 290]
image sizes: [(714, 590), (714, 590), (714, 590), (714, 590), (714, 590), (714, 590), (714, 590), (714, 590), (714, 590), (714, 590)]
input_ids shape: torch.Size([10, 756])
attention_mask shape: torch.Size([10, 756])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 756


===== Batch 192 =====
QIDs: [1693, 1693, 1693, 1693, 1693, 1693, 1693, 1693, 1693, 1693]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [93, 88, 105, 94, 101, 93, 93, 78, 124, 109]
image sizes: [(974, 778), (974, 778), (974, 778), (974, 778), (974, 778), (974, 778), (974, 778), (974, 778), (974, 778), (974, 778)]
input_ids shape: torch.Size([10, 1082])
attention_mask shape: torch.Size([10, 1082])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1082


===== Batch 193 =====
QIDs: [441, 441, 441, 441, 441, 441, 441, 441, 441, 441]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [257, 140, 230, 240, 234, 247, 238, 102, 269, 227]
image sizes: [(200, 108), (200, 108), (200, 108), (200, 108), (200, 108), (200, 108), (200, 108), (200, 108), (200, 108), (200, 108)]
input_ids shape: torch.Size([10, 186])
attention_mask shape: torch.Size([10, 186])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 186


===== Batch 194 =====
QIDs: [1094, 1094, 1094, 1094, 1094, 1094, 1094, 1094, 1094, 1094]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [349, 231, 363, 339, 321, 326, 334, 184, 355, 345]
image sizes: [(548, 365), (548, 365), (548, 365), (548, 365), (548, 365), (548, 365), (548, 365), (548, 365), (548, 365), (548, 365)]
input_ids shape: torch.Size([10, 505])
attention_mask shape: torch.Size([10, 505])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 505


===== Batch 195 =====
QIDs: [1338, 1338, 1338, 1338, 1338, 1338, 1338, 1338, 1338, 1338]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [502, 418, 493, 516, 494, 500, 478, 382, 540, 491]
image sizes: [(608, 277), (608, 277), (608, 277), (608, 277), (608, 277), (608, 277), (608, 277), (608, 277), (608, 277), (608, 277)]
input_ids shape: torch.Size([10, 498])
attention_mask shape: torch.Size([10, 498])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 498


===== Batch 196 =====
QIDs: [442, 442, 442, 442, 442, 442, 442, 442, 442, 442]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [362, 169, 373, 345, 327, 354, 476, 138, 409, 375]
image sizes: [(200, 136), (200, 136), (200, 136), (200, 136), (200, 136), (200, 136), (200, 136), (200, 136), (200, 136), (200, 136)]
input_ids shape: torch.Size([10, 322])
attention_mask shape: torch.Size([10, 322])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 322


===== Batch 197 =====
QIDs: [1371, 1371, 1371, 1371, 1371, 1371, 1371, 1371, 1371, 1371]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [303, 147, 336, 301, 266, 332, 313, 123, 353, 312]
image sizes: [(1040, 372), (1040, 372), (1040, 372), (1040, 372), (1040, 372), (1040, 372), (1040, 372), (1040, 372), (1040, 372), (1040, 372)]
input_ids shape: torch.Size([10, 692])
attention_mask shape: torch.Size([10, 692])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 692


===== Batch 198 =====
QIDs: [113, 113, 113, 113, 113, 113, 113, 113, 113, 113]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [99, 72, 75, 105, 99, 86, 92, 56, 92, 108]
image sizes: [(1540, 762), (1540, 762), (1540, 762), (1540, 762), (1540, 762), (1540, 762), (1540, 762), (1540, 762), (1540, 762), (1540, 762)]
input_ids shape: torch.Size([10, 1556])
attention_mask shape: torch.Size([10, 1556])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1556


===== Batch 199 =====
QIDs: [1385, 1385, 1385, 1385, 1385, 1385, 1385, 1385, 1385, 1385]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [437, 188, 482, 474, 450, 451, 377, 161, 493, 488]
image sizes: [(200, 157), (200, 157), (200, 157), (200, 157), (200, 157), (200, 157), (200, 157), (200, 157), (200, 157), (200, 157)]
input_ids shape: torch.Size([10, 297])
attention_mask shape: torch.Size([10, 297])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 297


===== Batch 200 =====
QIDs: [109, 109, 109, 109, 109, 109, 109, 109, 109, 109]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [228, 197, 230, 242, 224, 229, 216, 187, 233, 224]
image sizes: [(409, 406), (409, 406), (409, 406), (409, 406), (409, 406), (409, 406), (409, 406), (409, 406), (409, 406), (409, 406)]
input_ids shape: torch.Size([10, 397])
attention_mask shape: torch.Size([10, 397])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 397


===== Batch 201 =====
QIDs: [1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1284, 598, 1124, 1193, 924, 1215, 966, 408, 1258, 1331]
image sizes: [(748, 434), (748, 434), (748, 434), (748, 434), (748, 434), (748, 434), (748, 434), (748, 434), (748, 434), (748, 434)]
input_ids shape: torch.Size([10, 1029])
attention_mask shape: torch.Size([10, 1029])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1029


===== Batch 202 =====
QIDs: [1637, 1637, 1637, 1637, 1637, 1637, 1637, 1637, 1637, 1637]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [291, 272, 303, 283, 298, 291, 285, 117, 290, 304]
image sizes: [(378, 288), (378, 288), (378, 288), (378, 288), (378, 288), (378, 288), (378, 288), (378, 288), (378, 288), (378, 288)]
input_ids shape: torch.Size([10, 277])
attention_mask shape: torch.Size([10, 277])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 277


===== Batch 203 =====
QIDs: [1552, 1552, 1552, 1552, 1552, 1552, 1552, 1552, 1552, 1552]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [348, 185, 347, 348, 285, 375, 336, 130, 451, 333]
image sizes: [(1018, 186), (1018, 186), (1018, 186), (1018, 186), (1018, 186), (1018, 186), (1018, 186), (1018, 186), (1018, 186), (1018, 186)]
input_ids shape: torch.Size([10, 469])
attention_mask shape: torch.Size([10, 469])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 469


===== Batch 204 =====
QIDs: [1720, 1720, 1720, 1720, 1720, 1720, 1720, 1720, 1720, 1720]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [237, 135, 266, 256, 197, 254, 234, 112, 263, 270]
image sizes: [(673, 1029), (673, 1029), (673, 1029), (673, 1029), (673, 1029), (673, 1029), (673, 1029), (673, 1029), (673, 1029), (673, 1029)]
input_ids shape: torch.Size([10, 1055])
attention_mask shape: torch.Size([10, 1055])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1055


===== Batch 205 =====
QIDs: [280, 280, 280, 280, 280, 280, 280, 280, 280, 280]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1073, 503, 1139, 1161, 895, 1031, 941, 294, 1195, 1092]
image sizes: [(335, 232), (335, 232), (335, 232), (335, 232), (335, 232), (335, 232), (335, 232), (335, 232), (335, 232), (335, 232)]
input_ids shape: torch.Size([10, 681])
attention_mask shape: torch.Size([10, 681])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 681


===== Batch 206 =====
QIDs: [71, 71, 71, 71, 71, 71, 71, 71, 71, 71]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [736, 378, 676, 809, 643, 678, 703, 235, 690, 719]
image sizes: [(2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920)]
input_ids shape: torch.Size([10, 6712])
attention_mask shape: torch.Size([10, 6712])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 6712


===== Batch 207 =====
QIDs: [215, 215, 215, 215, 215, 215, 215, 215, 215, 215]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [258, 117, 244, 229, 254, 232, 232, 100, 236, 237]
image sizes: [(330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260)]
input_ids shape: torch.Size([10, 284])
attention_mask shape: torch.Size([10, 284])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 284


===== Batch 208 =====
QIDs: [1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524, 1524]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [641, 343, 609, 712, 583, 623, 558, 250, 635, 651]
image sizes: [(1207, 171), (1207, 171), (1207, 171), (1207, 171), (1207, 171), (1207, 171), (1207, 171), (1207, 171), (1207, 171), (1207, 171)]
input_ids shape: torch.Size([10, 629])
attention_mask shape: torch.Size([10, 629])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 629


===== Batch 209 =====
QIDs: [377, 377, 377, 377, 377, 377, 377, 377, 377, 377]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [458, 243, 381, 449, 400, 423, 317, 162, 447, 436]
image sizes: [(120, 100), (120, 100), (120, 100), (120, 100), (120, 100), (120, 100), (120, 100), (120, 100), (120, 100), (120, 100)]
input_ids shape: torch.Size([10, 251])
attention_mask shape: torch.Size([10, 251])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 251


===== Batch 210 =====
QIDs: [811, 811, 811, 811, 811, 811, 811, 811, 811, 811]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [284, 269, 281, 297, 276, 290, 283, 258, 292, 296]
image sizes: [(1303, 433), (1303, 433), (1303, 433), (1303, 433), (1303, 433), (1303, 433), (1303, 433), (1303, 433), (1303, 433), (1303, 433)]
input_ids shape: torch.Size([10, 940])
attention_mask shape: torch.Size([10, 940])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 940


===== Batch 211 =====
QIDs: [1287, 1287, 1287, 1287, 1287, 1287, 1287, 1287, 1287, 1287]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [321, 140, 312, 296, 282, 289, 262, 111, 355, 267]
image sizes: [(1030, 1060), (1030, 1060), (1030, 1060), (1030, 1060), (1030, 1060), (1030, 1060), (1030, 1060), (1030, 1060), (1030, 1060), (1030, 1060)]
input_ids shape: torch.Size([10, 1598])
attention_mask shape: torch.Size([10, 1598])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1598


===== Batch 212 =====
QIDs: [893, 893, 893, 893, 893, 893, 893, 893, 893, 893]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1206, 1094, 1202, 1233, 1161, 1184, 1169, 1045, 1208, 1202]
image sizes: [(548, 456), (548, 456), (548, 456), (548, 456), (548, 456), (548, 456), (548, 456), (548, 456), (548, 456), (548, 456)]
input_ids shape: torch.Size([10, 1089])
attention_mask shape: torch.Size([10, 1089])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1089


===== Batch 213 =====
QIDs: [216, 216, 216, 216, 216, 216, 216, 216, 216, 216]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [249, 117, 269, 231, 187, 260, 245, 92, 280, 228]
image sizes: [(590, 290), (590, 290), (590, 290), (590, 290), (590, 290), (590, 290), (590, 290), (590, 290), (590, 290), (590, 290)]
input_ids shape: torch.Size([10, 390])
attention_mask shape: torch.Size([10, 390])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 390


===== Batch 214 =====
QIDs: [1280, 1280, 1280, 1280, 1280, 1280, 1280, 1280, 1280, 1280]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [752, 379, 715, 767, 933, 693, 562, 304, 793, 651]
image sizes: [(493, 579), (493, 579), (493, 579), (493, 579), (493, 579), (493, 579), (493, 579), (493, 579), (493, 579), (493, 579)]
input_ids shape: torch.Size([10, 789])
attention_mask shape: torch.Size([10, 789])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 789


===== Batch 215 =====
QIDs: [816, 816, 816, 816, 816, 816, 816, 816, 816, 816]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [346, 153, 310, 368, 274, 316, 356, 120, 339, 364]
image sizes: [(1267, 313), (1267, 313), (1267, 313), (1267, 313), (1267, 313), (1267, 313), (1267, 313), (1267, 313), (1267, 313), (1267, 313)]
input_ids shape: torch.Size([10, 720])
attention_mask shape: torch.Size([10, 720])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 720


===== Batch 216 =====
QIDs: [760, 760, 760, 760, 760, 760, 760, 760, 760, 760]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [618, 363, 647, 622, 509, 635, 527, 226, 609, 596]
image sizes: [(827, 272), (827, 272), (827, 272), (827, 272), (827, 272), (827, 272), (827, 272), (827, 272), (827, 272), (827, 272)]
input_ids shape: torch.Size([10, 652])
attention_mask shape: torch.Size([10, 652])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 652


===== Batch 217 =====
QIDs: [888, 888, 888, 888, 888, 888, 888, 888, 888, 888]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [245, 204, 246, 254, 229, 236, 246, 187, 245, 263]
image sizes: [(506, 482), (506, 482), (506, 482), (506, 482), (506, 482), (506, 482), (506, 482), (506, 482), (506, 482), (506, 482)]
input_ids shape: torch.Size([10, 487])
attention_mask shape: torch.Size([10, 487])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 487


===== Batch 218 =====
QIDs: [108, 108, 108, 108, 108, 108, 108, 108, 108, 108]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [685, 378, 604, 631, 579, 583, 532, 331, 623, 634]
image sizes: [(512, 150), (512, 150), (512, 150), (512, 150), (512, 150), (512, 150), (512, 150), (512, 150), (512, 150), (512, 150)]
input_ids shape: torch.Size([10, 451])
attention_mask shape: torch.Size([10, 451])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 451


===== Batch 219 =====
QIDs: [374, 374, 374, 374, 374, 374, 374, 374, 374, 374]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [440, 248, 385, 356, 319, 372, 379, 185, 488, 360]
image sizes: [(217, 197), (217, 197), (217, 197), (217, 197), (217, 197), (217, 197), (217, 197), (217, 197), (217, 197), (217, 197)]
input_ids shape: torch.Size([10, 319])
attention_mask shape: torch.Size([10, 319])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 319


===== Batch 220 =====
QIDs: [1283, 1283, 1283, 1283, 1283, 1283, 1283, 1283, 1283, 1283]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [236, 104, 274, 210, 202, 226, 204, 90, 252, 196]
image sizes: [(1083, 1062), (1083, 1062), (1083, 1062), (1083, 1062), (1083, 1062), (1083, 1062), (1083, 1062), (1083, 1062), (1083, 1062), (1083, 1062)]
input_ids shape: torch.Size([10, 1644])
attention_mask shape: torch.Size([10, 1644])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1644


===== Batch 221 =====
QIDs: [1393, 1393, 1393, 1393, 1393, 1393, 1393, 1393, 1393, 1393]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [824, 411, 749, 855, 670, 798, 652, 283, 853, 776]
image sizes: [(876, 705), (876, 705), (876, 705), (876, 705), (876, 705), (876, 705), (876, 705), (876, 705), (876, 705), (876, 705)]
input_ids shape: torch.Size([10, 1172])
attention_mask shape: torch.Size([10, 1172])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1172


===== Batch 222 =====
QIDs: [759, 759, 759, 759, 759, 759, 759, 759, 759, 759]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [590, 298, 562, 621, 518, 528, 503, 242, 599, 630]
image sizes: [(742, 678), (742, 678), (742, 678), (742, 678), (742, 678), (742, 678), (742, 678), (742, 678), (742, 678), (742, 678)]
input_ids shape: torch.Size([10, 948])
attention_mask shape: torch.Size([10, 948])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 948


===== Batch 223 =====
QIDs: [98, 98, 98, 98, 98, 98, 98, 98, 98, 98]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [263, 218, 253, 254, 254, 247, 241, 189, 271, 270]
image sizes: [(408, 316), (408, 316), (408, 316), (408, 316), (408, 316), (408, 316), (408, 316), (408, 316), (408, 316), (408, 316)]
input_ids shape: torch.Size([10, 365])
attention_mask shape: torch.Size([10, 365])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 365


===== Batch 224 =====
QIDs: [1529, 1529, 1529, 1529, 1529, 1529, 1529, 1529, 1529, 1529]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [132, 113, 137, 137, 122, 128, 125, 104, 136, 130]
image sizes: [(556, 268), (556, 268), (556, 268), (556, 268), (556, 268), (556, 268), (556, 268), (556, 268), (556, 268), (556, 268)]
input_ids shape: torch.Size([10, 318])
attention_mask shape: torch.Size([10, 318])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 318


===== Batch 225 =====
QIDs: [1692, 1692, 1692, 1692, 1692, 1692, 1692, 1692, 1692, 1692]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [177, 104, 153, 185, 163, 169, 128, 73, 181, 152]
image sizes: [(636, 454), (636, 454), (636, 454), (636, 454), (636, 454), (636, 454), (636, 454), (636, 454), (636, 454), (636, 454)]
input_ids shape: torch.Size([10, 473])
attention_mask shape: torch.Size([10, 473])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 473


===== Batch 226 =====
QIDs: [1696, 1696, 1696, 1696, 1696, 1696, 1696, 1696, 1696, 1696]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [148, 123, 145, 155, 157, 144, 133, 91, 156, 163]
image sizes: [(604, 112), (604, 112), (604, 112), (604, 112), (604, 112), (604, 112), (604, 112), (604, 112), (604, 112), (604, 112)]
input_ids shape: torch.Size([10, 212])
attention_mask shape: torch.Size([10, 212])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 212


===== Batch 227 =====
QIDs: [187, 187, 187, 187, 187, 187, 187, 187, 187, 187]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [234, 99, 226, 243, 238, 214, 162, 96, 282, 195]
image sizes: [(330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260)]
input_ids shape: torch.Size([10, 254])
attention_mask shape: torch.Size([10, 254])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 254


===== Batch 228 =====
QIDs: [1175, 1175, 1175, 1175, 1175, 1175, 1175, 1175, 1175, 1175]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [396, 243, 394, 404, 314, 363, 334, 194, 426, 370]
image sizes: [(436, 112), (436, 112), (436, 112), (436, 112), (436, 112), (436, 112), (436, 112), (436, 112), (436, 112), (436, 112)]
input_ids shape: torch.Size([10, 312])
attention_mask shape: torch.Size([10, 312])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 312


===== Batch 229 =====
QIDs: [553, 553, 553, 553, 553, 553, 553, 553, 553, 553]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [962, 921, 968, 981, 951, 998, 953, 908, 967, 982]
image sizes: [(606, 176), (606, 176), (606, 176), (606, 176), (606, 176), (606, 176), (606, 176), (606, 176), (606, 176), (606, 176)]
input_ids shape: torch.Size([10, 730])
attention_mask shape: torch.Size([10, 730])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 730


===== Batch 230 =====
QIDs: [1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304, 1304]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [352, 339, 360, 373, 361, 353, 351, 324, 355, 362]
image sizes: [(271, 172), (271, 172), (271, 172), (271, 172), (271, 172), (271, 172), (271, 172), (271, 172), (271, 172), (271, 172)]
input_ids shape: torch.Size([10, 351])
attention_mask shape: torch.Size([10, 351])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 351


===== Batch 231 =====
QIDs: [200, 200, 200, 200, 200, 200, 200, 200, 200, 200]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [387, 236, 345, 348, 331, 349, 304, 185, 390, 366]
image sizes: [(397, 230), (397, 230), (397, 230), (397, 230), (397, 230), (397, 230), (397, 230), (397, 230), (397, 230), (397, 230)]
input_ids shape: torch.Size([10, 319])
attention_mask shape: torch.Size([10, 319])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 319


===== Batch 232 =====
QIDs: [767, 767, 767, 767, 767, 767, 767, 767, 767, 767]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [511, 285, 526, 500, 780, 518, 534, 212, 573, 508]
image sizes: [(367, 401), (367, 401), (367, 401), (367, 401), (367, 401), (367, 401), (367, 401), (367, 401), (367, 401), (367, 401)]
input_ids shape: torch.Size([10, 501])
attention_mask shape: torch.Size([10, 501])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 501


===== Batch 233 =====
QIDs: [573, 573, 573, 573, 573, 573, 573, 573, 573, 573]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [339, 245, 351, 348, 383, 357, 320, 193, 395, 355]
image sizes: [(355, 274), (355, 274), (355, 274), (355, 274), (355, 274), (355, 274), (355, 274), (355, 274), (355, 274), (355, 274)]
input_ids shape: torch.Size([10, 366])
attention_mask shape: torch.Size([10, 366])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 366


===== Batch 234 =====
QIDs: [111, 111, 111, 111, 111, 111, 111, 111, 111, 111]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [579, 457, 533, 563, 525, 534, 554, 448, 542, 538]
image sizes: [(557, 341), (557, 341), (557, 341), (557, 341), (557, 341), (557, 341), (557, 341), (557, 341), (557, 341), (557, 341)]
input_ids shape: torch.Size([10, 631])
attention_mask shape: torch.Size([10, 631])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 631


===== Batch 235 =====
QIDs: [1541, 1541, 1541, 1541, 1541, 1541, 1541, 1541, 1541, 1541]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [534, 394, 523, 565, 470, 502, 487, 342, 544, 534]
image sizes: [(531, 630), (531, 630), (531, 630), (531, 630), (531, 630), (531, 630), (531, 630), (531, 630), (531, 630), (531, 630)]
input_ids shape: torch.Size([10, 764])
attention_mask shape: torch.Size([10, 764])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 764


===== Batch 236 =====
QIDs: [1068, 1068, 1068, 1068, 1068, 1068, 1068, 1068, 1068, 1068]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [2096, 1228, 2031, 2278, 1672, 2009, 1853, 782, 2197, 2099]
image sizes: [(1046, 408), (1046, 408), (1046, 408), (1046, 408), (1046, 408), (1046, 408), (1046, 408), (1046, 408), (1046, 408), (1046, 408)]
input_ids shape: torch.Size([10, 1626])
attention_mask shape: torch.Size([10, 1626])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1626


===== Batch 237 =====
QIDs: [1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535, 1535]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [248, 174, 216, 230, 349, 210, 200, 152, 239, 219]
image sizes: [(726, 597), (726, 597), (726, 597), (726, 597), (726, 597), (726, 597), (726, 597), (726, 597), (726, 597), (726, 597)]
input_ids shape: torch.Size([10, 793])
attention_mask shape: torch.Size([10, 793])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 793


===== Batch 238 =====
QIDs: [620, 620, 620, 620, 620, 620, 620, 620, 620, 620]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [897, 570, 896, 872, 779, 906, 933, 440, 946, 790]
image sizes: [(604, 234), (604, 234), (604, 234), (604, 234), (604, 234), (604, 234), (604, 234), (604, 234), (604, 234), (604, 234)]
input_ids shape: torch.Size([10, 811])
attention_mask shape: torch.Size([10, 811])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 811


===== Batch 239 =====
QIDs: [12, 12, 12, 12, 12, 12, 12, 12, 12, 12]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [166, 129, 149, 153, 136, 159, 147, 117, 151, 159]
image sizes: [(466, 141), (466, 141), (466, 141), (466, 141), (466, 141), (466, 141), (466, 141), (466, 141), (466, 141), (466, 141)]
input_ids shape: torch.Size([10, 213])
attention_mask shape: torch.Size([10, 213])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 213


===== Batch 240 =====
QIDs: [1172, 1172, 1172, 1172, 1172, 1172, 1172, 1172, 1172, 1172]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [228, 167, 231, 250, 189, 235, 220, 147, 253, 229]
image sizes: [(1222, 237), (1222, 237), (1222, 237), (1222, 237), (1222, 237), (1222, 237), (1222, 237), (1222, 237), (1222, 237), (1222, 237)]
input_ids shape: torch.Size([10, 534])
attention_mask shape: torch.Size([10, 534])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 534


===== Batch 241 =====
QIDs: [996, 996, 996, 996, 996, 996, 996, 996, 996, 996]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [298, 126, 266, 274, 252, 275, 229, 85, 297, 282]
image sizes: [(483, 247), (483, 247), (483, 247), (483, 247), (483, 247), (483, 247), (483, 247), (483, 247), (483, 247), (483, 247)]
input_ids shape: torch.Size([10, 326])
attention_mask shape: torch.Size([10, 326])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 326


===== Batch 242 =====
QIDs: [1484, 1484, 1484, 1484, 1484, 1484, 1484, 1484, 1484, 1484]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [360, 192, 387, 472, 291, 417, 294, 137, 414, 451]
image sizes: [(480, 335), (480, 335), (480, 335), (480, 335), (480, 335), (480, 335), (480, 335), (480, 335), (480, 335), (480, 335)]
input_ids shape: torch.Size([10, 438])
attention_mask shape: torch.Size([10, 438])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 438


===== Batch 243 =====
QIDs: [730, 730, 730, 730, 730, 730, 730, 730, 730, 730]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [417, 178, 427, 427, 360, 406, 333, 140, 463, 393]
image sizes: [(605, 344), (605, 344), (605, 344), (605, 344), (605, 344), (605, 344), (605, 344), (605, 344), (605, 344), (605, 344)]
input_ids shape: torch.Size([10, 495])
attention_mask shape: torch.Size([10, 495])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 495


===== Batch 244 =====
QIDs: [992, 992, 992, 992, 992, 992, 992, 992, 992, 992]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [982, 401, 822, 935, 1141, 714, 1011, 303, 830, 777]
image sizes: [(416, 264), (416, 264), (416, 264), (416, 264), (416, 264), (416, 264), (416, 264), (416, 264), (416, 264), (416, 264)]
input_ids shape: torch.Size([10, 825])
attention_mask shape: torch.Size([10, 825])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 825


===== Batch 245 =====
QIDs: [786, 786, 786, 786, 786, 786, 786, 786, 786, 786]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [511, 280, 466, 556, 423, 471, 436, 205, 505, 434]
image sizes: [(744, 406), (744, 406), (744, 406), (744, 406), (744, 406), (744, 406), (744, 406), (744, 406), (744, 406), (744, 406)]
input_ids shape: torch.Size([10, 689])
attention_mask shape: torch.Size([10, 689])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 689


===== Batch 246 =====
QIDs: [1499, 1499, 1499, 1499, 1499, 1499, 1499, 1499, 1499, 1499]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [225, 125, 227, 214, 206, 229, 197, 100, 236, 242]
image sizes: [(974, 454), (974, 454), (974, 454), (974, 454), (974, 454), (974, 454), (974, 454), (974, 454), (974, 454), (974, 454)]
input_ids shape: torch.Size([10, 704])
attention_mask shape: torch.Size([10, 704])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 704


===== Batch 247 =====
QIDs: [1189, 1189, 1189, 1189, 1189, 1189, 1189, 1189, 1189, 1189]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [851, 417, 831, 890, 696, 819, 721, 293, 909, 940]
image sizes: [(1000, 750), (1000, 750), (1000, 750), (1000, 750), (1000, 750), (1000, 750), (1000, 750), (1000, 750), (1000, 750), (1000, 750)]
input_ids shape: torch.Size([10, 1418])
attention_mask shape: torch.Size([10, 1418])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1418


===== Batch 248 =====
QIDs: [1274, 1274, 1274, 1274, 1274, 1274, 1274, 1274, 1274, 1274]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [262, 129, 253, 293, 241, 253, 297, 101, 309, 241]
image sizes: [(1150, 928), (1150, 928), (1150, 928), (1150, 928), (1150, 928), (1150, 928), (1150, 928), (1150, 928), (1150, 928), (1150, 928)]
input_ids shape: torch.Size([10, 1565])
attention_mask shape: torch.Size([10, 1565])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1565


===== Batch 249 =====
QIDs: [1713, 1713, 1713, 1713, 1713, 1713, 1713, 1713, 1713, 1713]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [576, 296, 549, 585, 521, 552, 506, 229, 579, 583]
image sizes: [(755, 195), (755, 195), (755, 195), (755, 195), (755, 195), (755, 195), (755, 195), (755, 195), (755, 195), (755, 195)]
input_ids shape: torch.Size([10, 518])
attention_mask shape: torch.Size([10, 518])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 518


===== Batch 250 =====
QIDs: [1138, 1138, 1138, 1138, 1138, 1138, 1138, 1138, 1138, 1138]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [288, 153, 292, 319, 234, 296, 269, 111, 313, 316]
image sizes: [(990, 1254), (990, 1254), (990, 1254), (990, 1254), (990, 1254), (990, 1254), (990, 1254), (990, 1254), (990, 1254), (990, 1254)]
input_ids shape: torch.Size([10, 1737])
attention_mask shape: torch.Size([10, 1737])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1737


===== Batch 251 =====
QIDs: [586, 586, 586, 586, 586, 586, 586, 586, 586, 586]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [238, 173, 237, 231, 264, 246, 212, 147, 255, 236]
image sizes: [(824, 342), (824, 342), (824, 342), (824, 342), (824, 342), (824, 342), (824, 342), (824, 342), (824, 342), (824, 342)]
input_ids shape: torch.Size([10, 539])
attention_mask shape: torch.Size([10, 539])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 539


===== Batch 252 =====
QIDs: [1179, 1179, 1179, 1179, 1179, 1179, 1179, 1179, 1179, 1179]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [217, 118, 212, 219, 191, 194, 181, 94, 197, 198]
image sizes: [(320, 240), (320, 240), (320, 240), (320, 240), (320, 240), (320, 240), (320, 240), (320, 240), (320, 240), (320, 240)]
input_ids shape: torch.Size([10, 245])
attention_mask shape: torch.Size([10, 245])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 245


===== Batch 253 =====
QIDs: [326, 326, 326, 326, 326, 326, 326, 326, 326, 326]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [182, 150, 174, 177, 162, 181, 175, 136, 174, 188]
image sizes: [(300, 330), (300, 330), (300, 330), (300, 330), (300, 330), (300, 330), (300, 330), (300, 330), (300, 330), (300, 330)]
input_ids shape: torch.Size([10, 237])
attention_mask shape: torch.Size([10, 237])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 237


===== Batch 254 =====
QIDs: [1511, 1511, 1511, 1511, 1511, 1511, 1511, 1511, 1511, 1511]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [474, 234, 460, 516, 400, 459, 407, 159, 536, 478]
image sizes: [(447, 324), (447, 324), (447, 324), (447, 324), (447, 324), (447, 324), (447, 324), (447, 324), (447, 324), (447, 324)]
input_ids shape: torch.Size([10, 466])
attention_mask shape: torch.Size([10, 466])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 466


===== Batch 255 =====
QIDs: [1131, 1131, 1131, 1131, 1131, 1131, 1131, 1131, 1131, 1131]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [234, 129, 292, 298, 225, 284, 276, 112, 358, 289]
image sizes: [(1684, 1058), (1684, 1058), (1684, 1058), (1684, 1058), (1684, 1058), (1684, 1058), (1684, 1058), (1684, 1058), (1684, 1058), (1684, 1058)]
input_ids shape: torch.Size([10, 2460])
attention_mask shape: torch.Size([10, 2460])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2460


===== Batch 256 =====
QIDs: [31, 31, 31, 31, 31, 31, 31, 31, 31, 31]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [335, 242, 353, 325, 311, 299, 314, 215, 364, 367]
image sizes: [(351, 76), (351, 76), (351, 76), (351, 76), (351, 76), (351, 76), (351, 76), (351, 76), (351, 76), (351, 76)]
input_ids shape: torch.Size([10, 277])
attention_mask shape: torch.Size([10, 277])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 277


===== Batch 257 =====
QIDs: [649, 649, 649, 649, 649, 649, 649, 649, 649, 649]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [332, 197, 280, 294, 251, 282, 274, 146, 305, 276]
image sizes: [(238, 166), (238, 166), (238, 166), (238, 166), (238, 166), (238, 166), (238, 166), (238, 166), (238, 166), (238, 166)]
input_ids shape: torch.Size([10, 252])
attention_mask shape: torch.Size([10, 252])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 252


===== Batch 258 =====
QIDs: [273, 273, 273, 273, 273, 273, 273, 273, 273, 273]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [90, 71, 83, 86, 87, 89, 90, 64, 100, 83]
image sizes: [(216, 76), (216, 76), (216, 76), (216, 76), (216, 76), (216, 76), (216, 76), (216, 76), (216, 76), (216, 76)]
input_ids shape: torch.Size([10, 94])
attention_mask shape: torch.Size([10, 94])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 94


===== Batch 259 =====
QIDs: [1061, 1061, 1061, 1061, 1061, 1061, 1061, 1061, 1061, 1061]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [307, 180, 306, 276, 282, 283, 286, 131, 299, 300]
image sizes: [(1184, 576), (1184, 576), (1184, 576), (1184, 576), (1184, 576), (1184, 576), (1184, 576), (1184, 576), (1184, 576), (1184, 576)]
input_ids shape: torch.Size([10, 1076])
attention_mask shape: torch.Size([10, 1076])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1076


===== Batch 260 =====
QIDs: [1522, 1522, 1522, 1522, 1522, 1522, 1522, 1522, 1522, 1522]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [395, 270, 386, 436, 378, 395, 352, 229, 408, 432]
image sizes: [(1216, 616), (1216, 616), (1216, 616), (1216, 616), (1216, 616), (1216, 616), (1216, 616), (1216, 616), (1216, 616), (1216, 616)]
input_ids shape: torch.Size([10, 1200])
attention_mask shape: torch.Size([10, 1200])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1200


===== Batch 261 =====
QIDs: [858, 858, 858, 858, 858, 858, 858, 858, 858, 858]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [385, 312, 377, 358, 351, 344, 381, 286, 414, 373]
image sizes: [(601, 558), (601, 558), (601, 558), (601, 558), (601, 558), (601, 558), (601, 558), (601, 558), (601, 558), (601, 558)]
input_ids shape: torch.Size([10, 637])
attention_mask shape: torch.Size([10, 637])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 637


===== Batch 262 =====
QIDs: [565, 565, 565, 565, 565, 565, 565, 565, 565, 565]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [486, 316, 455, 463, 428, 449, 420, 263, 498, 441]
image sizes: [(135, 124), (135, 124), (135, 124), (135, 124), (135, 124), (135, 124), (135, 124), (135, 124), (135, 124), (135, 124)]
input_ids shape: torch.Size([10, 326])
attention_mask shape: torch.Size([10, 326])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 326


===== Batch 263 =====
QIDs: [1556, 1556, 1556, 1556, 1556, 1556, 1556, 1556, 1556, 1556]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [307, 185, 272, 263, 255, 280, 248, 152, 294, 275]
image sizes: [(285, 273), (285, 273), (285, 273), (285, 273), (285, 273), (285, 273), (285, 273), (285, 273), (285, 273), (285, 273)]
input_ids shape: torch.Size([10, 272])
attention_mask shape: torch.Size([10, 272])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 272


===== Batch 264 =====
QIDs: [328, 328, 328, 328, 328, 328, 328, 328, 328, 328]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [458, 236, 420, 473, 396, 443, 407, 160, 474, 462]
image sizes: [(504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330)]
input_ids shape: torch.Size([10, 475])
attention_mask shape: torch.Size([10, 475])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 475


===== Batch 265 =====
QIDs: [1004, 1004, 1004, 1004, 1004, 1004, 1004, 1004, 1004, 1004]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [666, 377, 679, 742, 588, 648, 566, 215, 689, 574]
image sizes: [(444, 256), (444, 256), (444, 256), (444, 256), (444, 256), (444, 256), (444, 256), (444, 256), (444, 256), (444, 256)]
input_ids shape: torch.Size([10, 529])
attention_mask shape: torch.Size([10, 529])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 529


===== Batch 266 =====
QIDs: [466, 466, 466, 466, 466, 466, 466, 466, 466, 466]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [493, 219, 379, 524, 357, 405, 343, 156, 433, 462]
image sizes: [(646, 382), (646, 382), (646, 382), (646, 382), (646, 382), (646, 382), (646, 382), (646, 382), (646, 382), (646, 382)]
input_ids shape: torch.Size([10, 580])
attention_mask shape: torch.Size([10, 580])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 580


===== Batch 267 =====
QIDs: [75, 75, 75, 75, 75, 75, 75, 75, 75, 75]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [673, 459, 616, 704, 577, 619, 617, 405, 631, 674]
image sizes: [(418, 120), (418, 120), (418, 120), (418, 120), (418, 120), (418, 120), (418, 120), (418, 120), (418, 120), (418, 120)]
input_ids shape: torch.Size([10, 500])
attention_mask shape: torch.Size([10, 500])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 500


===== Batch 268 =====
QIDs: [594, 594, 594, 594, 594, 594, 594, 594, 594, 594]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [259, 172, 249, 236, 216, 235, 215, 145, 257, 247]
image sizes: [(648, 295), (648, 295), (648, 295), (648, 295), (648, 295), (648, 295), (648, 295), (648, 295), (648, 295), (648, 295)]
input_ids shape: torch.Size([10, 446])
attention_mask shape: torch.Size([10, 446])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 446


===== Batch 269 =====
QIDs: [495, 495, 495, 495, 495, 495, 495, 495, 495, 495]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [360, 208, 328, 404, 350, 342, 316, 174, 363, 379]
image sizes: [(613, 423), (613, 423), (613, 423), (613, 423), (613, 423), (613, 423), (613, 423), (613, 423), (613, 423), (613, 423)]
input_ids shape: torch.Size([10, 546])
attention_mask shape: torch.Size([10, 546])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 546


===== Batch 270 =====
QIDs: [1707, 1707, 1707, 1707, 1707, 1707, 1707, 1707, 1707, 1707]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [713, 400, 691, 691, 647, 693, 591, 289, 758, 703]
image sizes: [(807, 259), (807, 259), (807, 259), (807, 259), (807, 259), (807, 259), (807, 259), (807, 259), (807, 259), (807, 259)]
input_ids shape: torch.Size([10, 669])
attention_mask shape: torch.Size([10, 669])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 669


===== Batch 271 =====
QIDs: [872, 872, 872, 872, 872, 872, 872, 872, 872, 872]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [67, 38, 65, 60, 54, 63, 70, 34, 73, 83]
image sizes: [(598, 284), (598, 284), (598, 284), (598, 284), (598, 284), (598, 284), (598, 284), (598, 284), (598, 284), (598, 284)]
input_ids shape: torch.Size([10, 272])
attention_mask shape: torch.Size([10, 272])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 272


===== Batch 272 =====
QIDs: [492, 492, 492, 492, 492, 492, 492, 492, 492, 492]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [310, 231, 353, 303, 302, 302, 297, 156, 300, 320]
image sizes: [(478, 156), (478, 156), (478, 156), (478, 156), (478, 156), (478, 156), (478, 156), (478, 156), (478, 156), (478, 156)]
input_ids shape: torch.Size([10, 337])
attention_mask shape: torch.Size([10, 337])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 337


===== Batch 273 =====
QIDs: [1195, 1195, 1195, 1195, 1195, 1195, 1195, 1195, 1195, 1195]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [415, 180, 416, 383, 324, 374, 360, 135, 459, 332]
image sizes: [(1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560)]
input_ids shape: torch.Size([10, 6528])
attention_mask shape: torch.Size([10, 6528])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 6528


===== Batch 274 =====
QIDs: [860, 860, 860, 860, 860, 860, 860, 860, 860, 860]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [848, 628, 837, 928, 783, 830, 796, 526, 881, 867]
image sizes: [(444, 349), (444, 349), (444, 349), (444, 349), (444, 349), (444, 349), (444, 349), (444, 349), (444, 349), (444, 349)]
input_ids shape: torch.Size([10, 739])
attention_mask shape: torch.Size([10, 739])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 739


===== Batch 275 =====
QIDs: [1521, 1521, 1521, 1521, 1521, 1521, 1521, 1521, 1521, 1521]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [315, 221, 322, 273, 290, 315, 309, 203, 337, 311]
image sizes: [(1269, 262), (1269, 262), (1269, 262), (1269, 262), (1269, 262), (1269, 262), (1269, 262), (1269, 262), (1269, 262), (1269, 262)]
input_ids shape: torch.Size([10, 640])
attention_mask shape: torch.Size([10, 640])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 640


===== Batch 276 =====
QIDs: [131, 131, 131, 131, 131, 131, 131, 131, 131, 131]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [197, 125, 181, 211, 196, 234, 196, 100, 250, 198]
image sizes: [(1096, 380), (1096, 380), (1096, 380), (1096, 380), (1096, 380), (1096, 380), (1096, 380), (1096, 380), (1096, 380), (1096, 380)]
input_ids shape: torch.Size([10, 701])
attention_mask shape: torch.Size([10, 701])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 701


===== Batch 277 =====
QIDs: [629, 629, 629, 629, 629, 629, 629, 629, 629, 629]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [109, 73, 102, 120, 98, 111, 96, 50, 118, 120]
image sizes: [(1043, 261), (1043, 261), (1043, 261), (1043, 261), (1043, 261), (1043, 261), (1043, 261), (1043, 261), (1043, 261), (1043, 261)]
input_ids shape: torch.Size([10, 415])
attention_mask shape: torch.Size([10, 415])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 415


===== Batch 278 =====
QIDs: [57, 57, 57, 57, 57, 57, 57, 57, 57, 57]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [289, 147, 299, 281, 257, 267, 262, 108, 309, 293]
image sizes: [(800, 600), (800, 600), (800, 600), (800, 600), (800, 600), (800, 600), (800, 600), (800, 600), (800, 600), (800, 600)]
input_ids shape: torch.Size([10, 792])
attention_mask shape: torch.Size([10, 792])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 792


===== Batch 279 =====
QIDs: [1537, 1537, 1537, 1537, 1537, 1537, 1537, 1537, 1537, 1537]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [366, 293, 376, 394, 368, 361, 329, 256, 366, 368]
image sizes: [(631, 468), (631, 468), (631, 468), (631, 468), (631, 468), (631, 468), (631, 468), (631, 468), (631, 468), (631, 468)]
input_ids shape: torch.Size([10, 654])
attention_mask shape: torch.Size([10, 654])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 654


===== Batch 280 =====
QIDs: [107, 107, 107, 107, 107, 107, 107, 107, 107, 107]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [615, 471, 570, 608, 554, 552, 557, 422, 581, 572]
image sizes: [(518, 117), (518, 117), (518, 117), (518, 117), (518, 117), (518, 117), (518, 117), (518, 117), (518, 117), (518, 117)]
input_ids shape: torch.Size([10, 496])
attention_mask shape: torch.Size([10, 496])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 496


===== Batch 281 =====
QIDs: [47, 47, 47, 47, 47, 47, 47, 47, 47, 47]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [244, 133, 254, 225, 236, 225, 216, 102, 259, 241]
image sizes: [(2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920)]
input_ids shape: torch.Size([10, 6438])
attention_mask shape: torch.Size([10, 6438])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 6438


===== Batch 282 =====
QIDs: [1313, 1313, 1313, 1313, 1313, 1313, 1313, 1313, 1313, 1313]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [165, 132, 144, 150, 136, 143, 153, 116, 134, 156]
image sizes: [(686, 610), (686, 610), (686, 610), (686, 610), (686, 610), (686, 610), (686, 610), (686, 610), (686, 610), (686, 610)]
input_ids shape: torch.Size([10, 658])
attention_mask shape: torch.Size([10, 658])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 658


===== Batch 283 =====
QIDs: [1355, 1355, 1355, 1355, 1355, 1355, 1355, 1355, 1355, 1355]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [328, 160, 317, 276, 349, 371, 258, 128, 382, 278]
image sizes: [(500, 464), (500, 464), (500, 464), (500, 464), (500, 464), (500, 464), (500, 464), (500, 464), (500, 464), (500, 464)]
input_ids shape: torch.Size([10, 496])
attention_mask shape: torch.Size([10, 496])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 496


===== Batch 284 =====
QIDs: [1332, 1332, 1332, 1332, 1332, 1332, 1332, 1332, 1332, 1332]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [163, 104, 153, 157, 151, 184, 147, 83, 161, 174]
image sizes: [(148, 97), (148, 97), (148, 97), (148, 97), (148, 97), (148, 97), (148, 97), (148, 97), (148, 97), (148, 97)]
input_ids shape: torch.Size([10, 132])
attention_mask shape: torch.Size([10, 132])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 132


===== Batch 285 =====
QIDs: [1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009, 1009]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [623, 295, 510, 569, 556, 517, 543, 231, 513, 614]
image sizes: [(532, 328), (532, 328), (532, 328), (532, 328), (532, 328), (532, 328), (532, 328), (532, 328), (532, 328), (532, 328)]
input_ids shape: torch.Size([10, 535])
attention_mask shape: torch.Size([10, 535])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 535


===== Batch 286 =====
QIDs: [617, 617, 617, 617, 617, 617, 617, 617, 617, 617]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [369, 283, 366, 374, 369, 366, 334, 231, 371, 375]
image sizes: [(849, 261), (849, 261), (849, 261), (849, 261), (849, 261), (849, 261), (849, 261), (849, 261), (849, 261), (849, 261)]
input_ids shape: torch.Size([10, 497])
attention_mask shape: torch.Size([10, 497])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 497


===== Batch 287 =====
QIDs: [201, 201, 201, 201, 201, 201, 201, 201, 201, 201]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [183, 111, 182, 178, 162, 182, 185, 89, 203, 157]
image sizes: [(200, 174), (200, 174), (200, 174), (200, 174), (200, 174), (200, 174), (200, 174), (200, 174), (200, 174), (200, 174)]
input_ids shape: torch.Size([10, 191])
attention_mask shape: torch.Size([10, 191])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 191


===== Batch 288 =====
QIDs: [1443, 1443, 1443, 1443, 1443, 1443, 1443, 1443, 1443, 1443]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [442, 271, 452, 498, 418, 470, 389, 204, 490, 444]
image sizes: [(757, 888), (757, 888), (757, 888), (757, 888), (757, 888), (757, 888), (757, 888), (757, 888), (757, 888), (757, 888)]
input_ids shape: torch.Size([10, 1153])
attention_mask shape: torch.Size([10, 1153])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1153


===== Batch 289 =====
QIDs: [668, 668, 668, 668, 668, 668, 668, 668, 668, 668]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [120, 88, 115, 102, 110, 122, 113, 75, 116, 110]
image sizes: [(1226, 531), (1226, 531), (1226, 531), (1226, 531), (1226, 531), (1226, 531), (1226, 531), (1226, 531), (1226, 531), (1226, 531)]
input_ids shape: torch.Size([10, 925])
attention_mask shape: torch.Size([10, 925])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 925


===== Batch 290 =====
QIDs: [735, 735, 735, 735, 735, 735, 735, 735, 735, 735]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [112, 76, 108, 127, 113, 113, 104, 54, 129, 140]
image sizes: [(407, 304), (407, 304), (407, 304), (407, 304), (407, 304), (407, 304), (407, 304), (407, 304), (407, 304), (407, 304)]
input_ids shape: torch.Size([10, 245])
attention_mask shape: torch.Size([10, 245])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 245


===== Batch 291 =====
QIDs: [921, 921, 921, 921, 921, 921, 921, 921, 921, 921]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [627, 342, 601, 594, 504, 598, 522, 239, 667, 570]
image sizes: [(395, 393), (395, 393), (395, 393), (395, 393), (395, 393), (395, 393), (395, 393), (395, 393), (395, 393), (395, 393)]
input_ids shape: torch.Size([10, 552])
attention_mask shape: torch.Size([10, 552])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 552


===== Batch 292 =====
QIDs: [1459, 1459, 1459, 1459, 1459, 1459, 1459, 1459, 1459, 1459]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [342, 280, 344, 333, 207, 202, 299, 241, 361, 331]
image sizes: [(336, 113), (336, 113), (336, 113), (336, 113), (336, 113), (336, 113), (336, 113), (336, 113), (336, 113), (336, 113)]
input_ids shape: torch.Size([10, 307])
attention_mask shape: torch.Size([10, 307])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 307


===== Batch 293 =====
QIDs: [1021, 1021, 1021, 1021, 1021, 1021, 1021, 1021, 1021, 1021]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [154, 104, 157, 171, 139, 153, 114, 68, 160, 150]
image sizes: [(398, 398), (398, 398), (398, 398), (398, 398), (398, 398), (398, 398), (398, 398), (398, 398), (398, 398), (398, 398)]
input_ids shape: torch.Size([10, 293])
attention_mask shape: torch.Size([10, 293])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 293


===== Batch 294 =====
QIDs: [1445, 1445, 1445, 1445, 1445, 1445, 1445, 1445, 1445, 1445]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [600, 312, 579, 631, 531, 645, 563, 223, 662, 608]
image sizes: [(350, 276), (350, 276), (350, 276), (350, 276), (350, 276), (350, 276), (350, 276), (350, 276), (350, 276), (350, 276)]
input_ids shape: torch.Size([10, 484])
attention_mask shape: torch.Size([10, 484])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 484


===== Batch 295 =====
QIDs: [1013, 1013, 1013, 1013, 1013, 1013, 1013, 1013, 1013, 1013]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [288, 167, 278, 306, 254, 269, 257, 128, 297, 304]
image sizes: [(736, 236), (736, 236), (736, 236), (736, 236), (736, 236), (736, 236), (736, 236), (736, 236), (736, 236), (736, 236)]
input_ids shape: torch.Size([10, 399])
attention_mask shape: torch.Size([10, 399])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 399


===== Batch 296 =====
QIDs: [249, 249, 249, 249, 249, 249, 249, 249, 249, 249]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [111, 77, 123, 129, 113, 111, 95, 54, 137, 101]
image sizes: [(1618, 1148), (1618, 1148), (1618, 1148), (1618, 1148), (1618, 1148), (1618, 1148), (1618, 1148), (1618, 1148), (1618, 1148), (1618, 1148)]
input_ids shape: torch.Size([10, 2466])
attention_mask shape: torch.Size([10, 2466])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2466


===== Batch 297 =====
QIDs: [1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [513, 345, 469, 526, 462, 508, 452, 270, 535, 579]
image sizes: [(504, 304), (504, 304), (504, 304), (504, 304), (504, 304), (504, 304), (504, 304), (504, 304), (504, 304), (504, 304)]
input_ids shape: torch.Size([10, 476])
attention_mask shape: torch.Size([10, 476])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 476


===== Batch 298 =====
QIDs: [572, 572, 572, 572, 572, 572, 572, 572, 572, 572]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [444, 274, 414, 487, 433, 405, 461, 209, 484, 434]
image sizes: [(420, 268), (420, 268), (420, 268), (420, 268), (420, 268), (420, 268), (420, 268), (420, 268), (420, 268), (420, 268)]
input_ids shape: torch.Size([10, 468])
attention_mask shape: torch.Size([10, 468])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 468


===== Batch 299 =====
QIDs: [1651, 1651, 1651, 1651, 1651, 1651, 1651, 1651, 1651, 1651]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [540, 251, 442, 440, 444, 459, 396, 160, 436, 452]
image sizes: [(376, 368), (376, 368), (376, 368), (376, 368), (376, 368), (376, 368), (376, 368), (376, 368), (376, 368), (376, 368)]
input_ids shape: torch.Size([10, 409])
attention_mask shape: torch.Size([10, 409])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 409


===== Batch 300 =====
QIDs: [1374, 1374, 1374, 1374, 1374, 1374, 1374, 1374, 1374, 1374]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [309, 125, 247, 272, 258, 262, 272, 108, 283, 285]
image sizes: [(200, 81), (200, 81), (200, 81), (200, 81), (200, 81), (200, 81), (200, 81), (200, 81), (200, 81), (200, 81)]
input_ids shape: torch.Size([10, 220])
attention_mask shape: torch.Size([10, 220])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 220


===== Batch 301 =====
QIDs: [309, 309, 309, 309, 309, 309, 309, 309, 309, 309]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [221, 134, 219, 231, 214, 241, 199, 114, 248, 231]
image sizes: [(303, 213), (303, 213), (303, 213), (303, 213), (303, 213), (303, 213), (303, 213), (303, 213), (303, 213), (303, 213)]
input_ids shape: torch.Size([10, 251])
attention_mask shape: torch.Size([10, 251])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 251


===== Batch 302 =====
QIDs: [568, 568, 568, 568, 568, 568, 568, 568, 568, 568]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [496, 375, 459, 513, 445, 466, 449, 323, 498, 469]
image sizes: [(517, 223), (517, 223), (517, 223), (517, 223), (517, 223), (517, 223), (517, 223), (517, 223), (517, 223), (517, 223)]
input_ids shape: torch.Size([10, 436])
attention_mask shape: torch.Size([10, 436])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 436


===== Batch 303 =====
QIDs: [938, 938, 938, 938, 938, 938, 938, 938, 938, 938]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [145, 80, 137, 164, 128, 156, 131, 56, 158, 129]
image sizes: [(1126, 111), (1126, 111), (1126, 111), (1126, 111), (1126, 111), (1126, 111), (1126, 111), (1126, 111), (1126, 111), (1126, 111)]
input_ids shape: torch.Size([10, 259])
attention_mask shape: torch.Size([10, 259])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 259


===== Batch 304 =====
QIDs: [393, 393, 393, 393, 393, 393, 393, 393, 393, 393]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [538, 262, 482, 632, 430, 493, 423, 182, 527, 639]
image sizes: [(363, 500), (363, 500), (363, 500), (363, 500), (363, 500), (363, 500), (363, 500), (363, 500), (363, 500), (363, 500)]
input_ids shape: torch.Size([10, 525])
attention_mask shape: torch.Size([10, 525])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 525


===== Batch 305 =====
QIDs: [1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [223, 115, 196, 209, 170, 197, 176, 98, 208, 211]
image sizes: [(508, 97), (508, 97), (508, 97), (508, 97), (508, 97), (508, 97), (508, 97), (508, 97), (508, 97), (508, 97)]
input_ids shape: torch.Size([10, 200])
attention_mask shape: torch.Size([10, 200])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 200


===== Batch 306 =====
QIDs: [951, 951, 951, 951, 951, 951, 951, 951, 951, 951]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [84, 56, 85, 87, 66, 89, 76, 33, 87, 78]
image sizes: [(205, 94), (205, 94), (205, 94), (205, 94), (205, 94), (205, 94), (205, 94), (205, 94), (205, 94), (205, 94)]
input_ids shape: torch.Size([10, 90])
attention_mask shape: torch.Size([10, 90])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 90


===== Batch 307 =====
QIDs: [1128, 1128, 1128, 1128, 1128, 1128, 1128, 1128, 1128, 1128]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [554, 251, 538, 528, 413, 508, 474, 162, 514, 501]
image sizes: [(840, 1274), (840, 1274), (840, 1274), (840, 1274), (840, 1274), (840, 1274), (840, 1274), (840, 1274), (840, 1274), (840, 1274)]
input_ids shape: torch.Size([10, 1689])
attention_mask shape: torch.Size([10, 1689])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1689


===== Batch 308 =====
QIDs: [968, 968, 968, 968, 968, 968, 968, 968, 968, 968]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [147, 105, 160, 177, 165, 140, 118, 65, 190, 164]
image sizes: [(512, 205), (512, 205), (512, 205), (512, 205), (512, 205), (512, 205), (512, 205), (512, 205), (512, 205), (512, 205)]
input_ids shape: torch.Size([10, 228])
attention_mask shape: torch.Size([10, 228])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 228


===== Batch 309 =====
QIDs: [954, 954, 954, 954, 954, 954, 954, 954, 954, 954]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [196, 122, 170, 183, 166, 180, 168, 93, 187, 192]
image sizes: [(1117, 143), (1117, 143), (1117, 143), (1117, 143), (1117, 143), (1117, 143), (1117, 143), (1117, 143), (1117, 143), (1117, 143)]
input_ids shape: torch.Size([10, 335])
attention_mask shape: torch.Size([10, 335])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 335


===== Batch 310 =====
QIDs: [1520, 1520, 1520, 1520, 1520, 1520, 1520, 1520, 1520, 1520]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [474, 275, 468, 491, 414, 477, 421, 220, 502, 508]
image sizes: [(1215, 303), (1215, 303), (1215, 303), (1215, 303), (1215, 303), (1215, 303), (1215, 303), (1215, 303), (1215, 303), (1215, 303)]
input_ids shape: torch.Size([10, 758])
attention_mask shape: torch.Size([10, 758])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 758


===== Batch 311 =====
QIDs: [1190, 1190, 1190, 1190, 1190, 1190, 1190, 1190, 1190, 1190]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [215, 124, 196, 214, 147, 199, 191, 91, 203, 208]
image sizes: [(2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920)]
input_ids shape: torch.Size([10, 6420])
attention_mask shape: torch.Size([10, 6420])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 6420


===== Batch 312 =====
QIDs: [1403, 1403, 1403, 1403, 1403, 1403, 1403, 1403, 1403, 1403]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [221, 144, 218, 236, 228, 204, 204, 120, 218, 219]
image sizes: [(511, 430), (511, 430), (511, 430), (511, 430), (511, 430), (511, 430), (511, 430), (511, 430), (511, 430), (511, 430)]
input_ids shape: torch.Size([10, 440])
attention_mask shape: torch.Size([10, 440])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 440


===== Batch 313 =====
QIDs: [836, 836, 836, 836, 836, 836, 836, 836, 836, 836]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [373, 241, 396, 360, 306, 338, 302, 192, 399, 335]
image sizes: [(289, 297), (289, 297), (289, 297), (289, 297), (289, 297), (289, 297), (289, 297), (289, 297), (289, 297), (289, 297)]
input_ids shape: torch.Size([10, 366])
attention_mask shape: torch.Size([10, 366])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 366


===== Batch 314 =====
QIDs: [1314, 1314, 1314, 1314, 1314, 1314, 1314, 1314, 1314, 1314]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [347, 186, 337, 361, 314, 324, 318, 145, 361, 336]
image sizes: [(590, 590), (590, 590), (590, 590), (590, 590), (590, 590), (590, 590), (590, 590), (590, 590), (590, 590), (590, 590)]
input_ids shape: torch.Size([10, 634])
attention_mask shape: torch.Size([10, 634])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 634


===== Batch 315 =====
QIDs: [1194, 1194, 1194, 1194, 1194, 1194, 1194, 1194, 1194, 1194]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [232, 158, 243, 254, 297, 240, 238, 126, 249, 262]
image sizes: [(280, 210), (280, 210), (280, 210), (280, 210), (280, 210), (280, 210), (280, 210), (280, 210), (280, 210), (280, 210)]
input_ids shape: torch.Size([10, 250])
attention_mask shape: torch.Size([10, 250])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 250


===== Batch 316 =====
QIDs: [1438, 1438, 1438, 1438, 1438, 1438, 1438, 1438, 1438, 1438]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [225, 187, 220, 243, 253, 223, 230, 167, 227, 248]
image sizes: [(238, 292), (238, 292), (238, 292), (238, 292), (238, 292), (238, 292), (238, 292), (238, 292), (238, 292), (238, 292)]
input_ids shape: torch.Size([10, 271])
attention_mask shape: torch.Size([10, 271])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 271


===== Batch 317 =====
QIDs: [1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282, 1282]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [148, 108, 134, 128, 118, 140, 131, 96, 171, 134]
image sizes: [(1138, 1094), (1138, 1094), (1138, 1094), (1138, 1094), (1138, 1094), (1138, 1094), (1138, 1094), (1138, 1094), (1138, 1094), (1138, 1094)]
input_ids shape: torch.Size([10, 1744])
attention_mask shape: torch.Size([10, 1744])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1744


===== Batch 318 =====
QIDs: [1206, 1206, 1206, 1206, 1206, 1206, 1206, 1206, 1206, 1206]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [817, 593, 751, 788, 803, 748, 794, 571, 774, 942]
image sizes: [(538, 188), (538, 188), (538, 188), (538, 188), (538, 188), (538, 188), (538, 188), (538, 188), (538, 188), (538, 188)]
input_ids shape: torch.Size([10, 689])
attention_mask shape: torch.Size([10, 689])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 689


===== Batch 319 =====
QIDs: [785, 785, 785, 785, 785, 785, 785, 785, 785, 785]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [346, 210, 333, 387, 325, 325, 339, 175, 343, 400]
image sizes: [(1267, 313), (1267, 313), (1267, 313), (1267, 313), (1267, 313), (1267, 313), (1267, 313), (1267, 313), (1267, 313), (1267, 313)]
input_ids shape: torch.Size([10, 740])
attention_mask shape: torch.Size([10, 740])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 740


===== Batch 320 =====
QIDs: [241, 241, 241, 241, 241, 241, 241, 241, 241, 241]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1293, 709, 1319, 1497, 1063, 1270, 1189, 445, 1479, 1366]
image sizes: [(1182, 896), (1182, 896), (1182, 896), (1182, 896), (1182, 896), (1182, 896), (1182, 896), (1182, 896), (1182, 896), (1182, 896)]
input_ids shape: torch.Size([10, 2044])
attention_mask shape: torch.Size([10, 2044])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2044


===== Batch 321 =====
QIDs: [1467, 1467, 1467, 1467, 1467, 1467, 1467, 1467, 1467, 1467]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [590, 308, 547, 590, 443, 543, 481, 214, 576, 544]
image sizes: [(501, 122), (501, 122), (501, 122), (501, 122), (501, 122), (501, 122), (501, 122), (501, 122), (501, 122), (501, 122)]
input_ids shape: torch.Size([10, 416])
attention_mask shape: torch.Size([10, 416])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 416


===== Batch 322 =====
QIDs: [1312, 1312, 1312, 1312, 1312, 1312, 1312, 1312, 1312, 1312]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [578, 312, 586, 550, 554, 531, 527, 245, 615, 584]
image sizes: [(504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330)]
input_ids shape: torch.Size([10, 573])
attention_mask shape: torch.Size([10, 573])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 573


===== Batch 323 =====
QIDs: [1322, 1322, 1322, 1322, 1322, 1322, 1322, 1322, 1322, 1322]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [205, 101, 209, 187, 212, 229, 174, 95, 230, 179]
image sizes: [(970, 1084), (970, 1084), (970, 1084), (970, 1084), (970, 1084), (970, 1084), (970, 1084), (970, 1084), (970, 1084), (970, 1084)]
input_ids shape: torch.Size([10, 1516])
attention_mask shape: torch.Size([10, 1516])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1516


===== Batch 324 =====
QIDs: [559, 559, 559, 559, 559, 559, 559, 559, 559, 559]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [502, 290, 492, 484, 488, 446, 471, 237, 501, 515]
image sizes: [(496, 519), (496, 519), (496, 519), (496, 519), (496, 519), (496, 519), (496, 519), (496, 519), (496, 519), (496, 519)]
input_ids shape: torch.Size([10, 649])
attention_mask shape: torch.Size([10, 649])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 649


===== Batch 325 =====
QIDs: [1186, 1186, 1186, 1186, 1186, 1186, 1186, 1186, 1186, 1186]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [189, 105, 182, 188, 182, 196, 181, 90, 237, 191]
image sizes: [(668, 446), (668, 446), (668, 446), (668, 446), (668, 446), (668, 446), (668, 446), (668, 446), (668, 446), (668, 446)]
input_ids shape: torch.Size([10, 526])
attention_mask shape: torch.Size([10, 526])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 526


===== Batch 326 =====
QIDs: [1364, 1364, 1364, 1364, 1364, 1364, 1364, 1364, 1364, 1364]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [593, 306, 543, 584, 482, 569, 548, 212, 620, 611]
image sizes: [(1852, 1604), (1852, 1604), (1852, 1604), (1852, 1604), (1852, 1604), (1852, 1604), (1852, 1604), (1852, 1604), (1852, 1604), (1852, 1604)]
input_ids shape: torch.Size([10, 4112])
attention_mask shape: torch.Size([10, 4112])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 4112


===== Batch 327 =====
QIDs: [1002, 1002, 1002, 1002, 1002, 1002, 1002, 1002, 1002, 1002]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [215, 146, 183, 200, 189, 195, 182, 118, 216, 196]
image sizes: [(366, 264), (366, 264), (366, 264), (366, 264), (366, 264), (366, 264), (366, 264), (366, 264), (366, 264), (366, 264)]
input_ids shape: torch.Size([10, 244])
attention_mask shape: torch.Size([10, 244])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 244


===== Batch 328 =====
QIDs: [157, 157, 157, 157, 157, 157, 157, 157, 157, 157]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [511, 252, 508, 563, 412, 503, 432, 173, 552, 546]
image sizes: [(958, 836), (958, 836), (958, 836), (958, 836), (958, 836), (958, 836), (958, 836), (958, 836), (958, 836), (958, 836)]
input_ids shape: torch.Size([10, 1292])
attention_mask shape: torch.Size([10, 1292])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1292


===== Batch 329 =====
QIDs: [613, 613, 613, 613, 613, 613, 613, 613, 613, 613]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [569, 392, 534, 596, 509, 532, 507, 282, 584, 604]
image sizes: [(713, 412), (713, 412), (713, 412), (713, 412), (713, 412), (713, 412), (713, 412), (713, 412), (713, 412), (713, 412)]
input_ids shape: torch.Size([10, 668])
attention_mask shape: torch.Size([10, 668])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 668


===== Batch 330 =====
QIDs: [982, 982, 982, 982, 982, 982, 982, 982, 982, 982]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [238, 136, 197, 232, 209, 237, 215, 102, 225, 237]
image sizes: [(434, 226), (434, 226), (434, 226), (434, 226), (434, 226), (434, 226), (434, 226), (434, 226), (434, 226), (434, 226)]
input_ids shape: torch.Size([10, 260])
attention_mask shape: torch.Size([10, 260])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 260


===== Batch 331 =====
QIDs: [1516, 1516, 1516, 1516, 1516, 1516, 1516, 1516, 1516, 1516]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [290, 281, 296, 296, 287, 291, 285, 276, 291, 287]
image sizes: [(1389, 517), (1389, 517), (1389, 517), (1389, 517), (1389, 517), (1389, 517), (1389, 517), (1389, 517), (1389, 517), (1389, 517)]
input_ids shape: torch.Size([10, 1139])
attention_mask shape: torch.Size([10, 1139])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1139


===== Batch 332 =====
QIDs: [983, 983, 983, 983, 983, 983, 983, 983, 983, 983]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [433, 198, 424, 395, 368, 383, 331, 154, 383, 365]
image sizes: [(392, 368), (392, 368), (392, 368), (392, 368), (392, 368), (392, 368), (392, 368), (392, 368), (392, 368), (392, 368)]
input_ids shape: torch.Size([10, 426])
attention_mask shape: torch.Size([10, 426])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 426


===== Batch 333 =====
QIDs: [1624, 1624, 1624, 1624, 1624, 1624, 1624, 1624, 1624, 1624]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [177, 97, 147, 169, 218, 200, 172, 89, 187, 202]
image sizes: [(270, 98), (270, 98), (270, 98), (270, 98), (270, 98), (270, 98), (270, 98), (270, 98), (270, 98), (270, 98)]
input_ids shape: torch.Size([10, 184])
attention_mask shape: torch.Size([10, 184])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 184


===== Batch 334 =====
QIDs: [737, 737, 737, 737, 737, 737, 737, 737, 737, 737]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [303, 161, 279, 309, 238, 314, 316, 113, 329, 307]
image sizes: [(783, 778), (783, 778), (783, 778), (783, 778), (783, 778), (783, 778), (783, 778), (783, 778), (783, 778), (783, 778)]
input_ids shape: torch.Size([10, 1005])
attention_mask shape: torch.Size([10, 1005])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1005


===== Batch 335 =====
QIDs: [1560, 1560, 1560, 1560, 1560, 1560, 1560, 1560, 1560, 1560]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [192, 161, 207, 224, 210, 200, 195, 132, 217, 196]
image sizes: [(808, 550), (808, 550), (808, 550), (808, 550), (808, 550), (808, 550), (808, 550), (808, 550), (808, 550), (808, 550)]
input_ids shape: torch.Size([10, 727])
attention_mask shape: torch.Size([10, 727])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 727


===== Batch 336 =====
QIDs: [1327, 1327, 1327, 1327, 1327, 1327, 1327, 1327, 1327, 1327]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [291, 134, 317, 295, 283, 306, 267, 102, 343, 258]
image sizes: [(288, 240), (288, 240), (288, 240), (288, 240), (288, 240), (288, 240), (288, 240), (288, 240), (288, 240), (288, 240)]
input_ids shape: torch.Size([10, 290])
attention_mask shape: torch.Size([10, 290])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 290


===== Batch 337 =====
QIDs: [311, 311, 311, 311, 311, 311, 311, 311, 311, 311]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [475, 284, 484, 470, 471, 467, 417, 215, 490, 481]
image sizes: [(504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331)]
input_ids shape: torch.Size([10, 507])
attention_mask shape: torch.Size([10, 507])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 507


===== Batch 338 =====
QIDs: [464, 464, 464, 464, 464, 464, 464, 464, 464, 464]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [362, 182, 343, 367, 290, 314, 302, 113, 380, 340]
image sizes: [(304, 112), (304, 112), (304, 112), (304, 112), (304, 112), (304, 112), (304, 112), (304, 112), (304, 112), (304, 112)]
input_ids shape: torch.Size([10, 229])
attention_mask shape: torch.Size([10, 229])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 229


===== Batch 339 =====
QIDs: [1613, 1613, 1613, 1613, 1613, 1613, 1613, 1613, 1613, 1613]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [191, 177, 201, 233, 222, 206, 261, 114, 235, 268]
image sizes: [(1552, 183), (1552, 183), (1552, 183), (1552, 183), (1552, 183), (1552, 183), (1552, 183), (1552, 183), (1552, 183), (1552, 183)]
input_ids shape: torch.Size([10, 569])
attention_mask shape: torch.Size([10, 569])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 569


===== Batch 340 =====
QIDs: [1616, 1616, 1616, 1616, 1616, 1616, 1616, 1616, 1616, 1616]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [41, 29, 49, 43, 34, 53, 38, 24, 44, 39]
image sizes: [(1512, 232), (1512, 232), (1512, 232), (1512, 232), (1512, 232), (1512, 232), (1512, 232), (1512, 232), (1512, 232), (1512, 232)]
input_ids shape: torch.Size([10, 480])
attention_mask shape: torch.Size([10, 480])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 480


===== Batch 341 =====
QIDs: [1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300, 1300]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [772, 448, 679, 823, 724, 698, 660, 311, 698, 926]
image sizes: [(494, 286), (494, 286), (494, 286), (494, 286), (494, 286), (494, 286), (494, 286), (494, 286), (494, 286), (494, 286)]
input_ids shape: torch.Size([10, 593])
attention_mask shape: torch.Size([10, 593])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 593


===== Batch 342 =====
QIDs: [721, 721, 721, 721, 721, 721, 721, 721, 721, 721]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [447, 193, 408, 497, 407, 399, 420, 147, 484, 429]
image sizes: [(512, 377), (512, 377), (512, 377), (512, 377), (512, 377), (512, 377), (512, 377), (512, 377), (512, 377), (512, 377)]
input_ids shape: torch.Size([10, 515])
attention_mask shape: torch.Size([10, 515])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 515


===== Batch 343 =====
QIDs: [1054, 1054, 1054, 1054, 1054, 1054, 1054, 1054, 1054, 1054]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [3290, 1661, 3167, 3543, 2722, 2496, 2599, 1041, 3339, 3459]
image sizes: [(866, 532), (866, 532), (866, 532), (866, 532), (866, 532), (866, 532), (866, 532), (866, 532), (866, 532), (866, 532)]
input_ids shape: torch.Size([10, 2080])
attention_mask shape: torch.Size([10, 2080])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2080


===== Batch 344 =====
QIDs: [204, 204, 204, 204, 204, 204, 204, 204, 204, 204]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [219, 105, 224, 221, 188, 221, 188, 91, 237, 206]
image sizes: [(472, 377), (472, 377), (472, 377), (472, 377), (472, 377), (472, 377), (472, 377), (472, 377), (472, 377), (472, 377)]
input_ids shape: torch.Size([10, 371])
attention_mask shape: torch.Size([10, 371])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 371


===== Batch 345 =====
QIDs: [485, 485, 485, 485, 485, 485, 485, 485, 485, 485]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1687, 930, 1541, 1668, 1467, 1362, 1418, 541, 1473, 1785]
image sizes: [(903, 364), (903, 364), (903, 364), (903, 364), (903, 364), (903, 364), (903, 364), (903, 364), (903, 364), (903, 364)]
input_ids shape: torch.Size([10, 1284])
attention_mask shape: torch.Size([10, 1284])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1284


===== Batch 346 =====
QIDs: [1334, 1334, 1334, 1334, 1334, 1334, 1334, 1334, 1334, 1334]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [297, 228, 294, 274, 276, 285, 275, 205, 316, 296]
image sizes: [(504, 348), (504, 348), (504, 348), (504, 348), (504, 348), (504, 348), (504, 348), (504, 348), (504, 348), (504, 348)]
input_ids shape: torch.Size([10, 392])
attention_mask shape: torch.Size([10, 392])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 392


===== Batch 347 =====
QIDs: [915, 915, 915, 915, 915, 915, 915, 915, 915, 915]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [436, 351, 413, 434, 415, 413, 411, 331, 429, 418]
image sizes: [(805, 509), (805, 509), (805, 509), (805, 509), (805, 509), (805, 509), (805, 509), (805, 509), (805, 509), (805, 509)]
input_ids shape: torch.Size([10, 780])
attention_mask shape: torch.Size([10, 780])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 780


===== Batch 348 =====
QIDs: [1050, 1050, 1050, 1050, 1050, 1050, 1050, 1050, 1050, 1050]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1775, 862, 1772, 1832, 1523, 1728, 1660, 515, 1850, 1586]
image sizes: [(842, 1122), (842, 1122), (842, 1122), (842, 1122), (842, 1122), (842, 1122), (842, 1122), (842, 1122), (842, 1122), (842, 1122)]
input_ids shape: torch.Size([10, 2178])
attention_mask shape: torch.Size([10, 2178])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2178


===== Batch 349 =====
QIDs: [817, 817, 817, 817, 817, 817, 817, 817, 817, 817]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [368, 246, 376, 398, 341, 376, 348, 211, 424, 372]
image sizes: [(1254, 361), (1254, 361), (1254, 361), (1254, 361), (1254, 361), (1254, 361), (1254, 361), (1254, 361), (1254, 361), (1254, 361)]
input_ids shape: torch.Size([10, 842])
attention_mask shape: torch.Size([10, 842])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 842


===== Batch 350 =====
QIDs: [745, 745, 745, 745, 745, 745, 745, 745, 745, 745]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [308, 175, 376, 397, 276, 348, 307, 134, 400, 363]
image sizes: [(500, 375), (500, 375), (500, 375), (500, 375), (500, 375), (500, 375), (500, 375), (500, 375), (500, 375), (500, 375)]
input_ids shape: torch.Size([10, 482])
attention_mask shape: torch.Size([10, 482])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 482


===== Batch 351 =====
QIDs: [1110, 1110, 1110, 1110, 1110, 1110, 1110, 1110, 1110, 1110]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [344, 189, 299, 335, 309, 300, 292, 138, 299, 359]
image sizes: [(487, 138), (487, 138), (487, 138), (487, 138), (487, 138), (487, 138), (487, 138), (487, 138), (487, 138), (487, 138)]
input_ids shape: torch.Size([10, 292])
attention_mask shape: torch.Size([10, 292])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 292


===== Batch 352 =====
QIDs: [809, 809, 809, 809, 809, 809, 809, 809, 809, 809]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [417, 275, 468, 452, 430, 444, 400, 199, 480, 417]
image sizes: [(1231, 486), (1231, 486), (1231, 486), (1231, 486), (1231, 486), (1231, 486), (1231, 486), (1231, 486), (1231, 486), (1231, 486)]
input_ids shape: torch.Size([10, 1050])
attention_mask shape: torch.Size([10, 1050])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1050


===== Batch 353 =====
QIDs: [104, 104, 104, 104, 104, 104, 104, 104, 104, 104]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [570, 448, 547, 599, 610, 656, 625, 381, 662, 568]
image sizes: [(653, 130), (653, 130), (653, 130), (653, 130), (653, 130), (653, 130), (653, 130), (653, 130), (653, 130), (653, 130)]
input_ids shape: torch.Size([10, 547])
attention_mask shape: torch.Size([10, 547])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 547


===== Batch 354 =====
QIDs: [1192, 1192, 1192, 1192, 1192, 1192, 1192, 1192, 1192, 1192]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [265, 152, 287, 262, 238, 268, 233, 107, 291, 266]
image sizes: [(1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560)]
input_ids shape: torch.Size([10, 6450])
attention_mask shape: torch.Size([10, 6450])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 6450


===== Batch 355 =====
QIDs: [1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053, 1053]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [289, 172, 290, 253, 254, 270, 247, 125, 268, 291]
image sizes: [(816, 724), (816, 724), (816, 724), (816, 724), (816, 724), (816, 724), (816, 724), (816, 724), (816, 724), (816, 724)]
input_ids shape: torch.Size([10, 942])
attention_mask shape: torch.Size([10, 942])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 942


===== Batch 356 =====
QIDs: [866, 866, 866, 866, 866, 866, 866, 866, 866, 866]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [538, 495, 551, 558, 536, 562, 548, 465, 572, 561]
image sizes: [(528, 226), (528, 226), (528, 226), (528, 226), (528, 226), (528, 226), (528, 226), (528, 226), (528, 226), (528, 226)]
input_ids shape: torch.Size([10, 418])
attention_mask shape: torch.Size([10, 418])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 418


===== Batch 357 =====
QIDs: [1698, 1698, 1698, 1698, 1698, 1698, 1698, 1698, 1698, 1698]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [863, 478, 893, 895, 838, 830, 797, 384, 894, 910]
image sizes: [(808, 299), (808, 299), (808, 299), (808, 299), (808, 299), (808, 299), (808, 299), (808, 299), (808, 299), (808, 299)]
input_ids shape: torch.Size([10, 851])
attention_mask shape: torch.Size([10, 851])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 851


===== Batch 358 =====
QIDs: [973, 973, 973, 973, 973, 973, 973, 973, 973, 973]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [237, 128, 218, 243, 185, 206, 241, 94, 226, 201]
image sizes: [(850, 434), (850, 434), (850, 434), (850, 434), (850, 434), (850, 434), (850, 434), (850, 434), (850, 434), (850, 434)]
input_ids shape: torch.Size([10, 639])
attention_mask shape: torch.Size([10, 639])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 639


===== Batch 359 =====
QIDs: [189, 189, 189, 189, 189, 189, 189, 189, 189, 189]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [299, 158, 315, 283, 249, 341, 216, 106, 335, 203]
image sizes: [(590, 290), (590, 290), (590, 290), (590, 290), (590, 290), (590, 290), (590, 290), (590, 290), (590, 290), (590, 290)]
input_ids shape: torch.Size([10, 413])
attention_mask shape: torch.Size([10, 413])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 413


===== Batch 360 =====
QIDs: [1176, 1176, 1176, 1176, 1176, 1176, 1176, 1176, 1176, 1176]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [413, 274, 439, 510, 466, 433, 337, 199, 420, 378]
image sizes: [(756, 244), (756, 244), (756, 244), (756, 244), (756, 244), (756, 244), (756, 244), (756, 244), (756, 244), (756, 244)]
input_ids shape: torch.Size([10, 522])
attention_mask shape: torch.Size([10, 522])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 522


===== Batch 361 =====
QIDs: [262, 262, 262, 262, 262, 262, 262, 262, 262, 262]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [237, 131, 244, 270, 189, 242, 213, 86, 275, 220]
image sizes: [(436, 126), (436, 126), (436, 126), (436, 126), (436, 126), (436, 126), (436, 126), (436, 126), (436, 126), (436, 126)]
input_ids shape: torch.Size([10, 248])
attention_mask shape: torch.Size([10, 248])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 248


===== Batch 362 =====
QIDs: [1016, 1016, 1016, 1016, 1016, 1016, 1016, 1016, 1016, 1016]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [450, 366, 457, 453, 428, 443, 429, 309, 471, 459]
image sizes: [(606, 370), (606, 370), (606, 370), (606, 370), (606, 370), (606, 370), (606, 370), (606, 370), (606, 370), (606, 370)]
input_ids shape: torch.Size([10, 565])
attention_mask shape: torch.Size([10, 565])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 565


===== Batch 363 =====
QIDs: [203, 203, 203, 203, 203, 203, 203, 203, 203, 203]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [247, 116, 258, 250, 184, 271, 254, 109, 280, 186]
image sizes: [(259, 194), (259, 194), (259, 194), (259, 194), (259, 194), (259, 194), (259, 194), (259, 194), (259, 194), (259, 194)]
input_ids shape: torch.Size([10, 264])
attention_mask shape: torch.Size([10, 264])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 264


===== Batch 364 =====
QIDs: [543, 543, 543, 543, 543, 543, 543, 543, 543, 543]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [391, 377, 394, 415, 413, 396, 384, 370, 396, 399]
image sizes: [(314, 179), (314, 179), (314, 179), (314, 179), (314, 179), (314, 179), (314, 179), (314, 179), (314, 179), (314, 179)]
input_ids shape: torch.Size([10, 365])
attention_mask shape: torch.Size([10, 365])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 365


===== Batch 365 =====
QIDs: [65, 65, 65, 65, 65, 65, 65, 65, 65, 65]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [682, 311, 633, 702, 531, 605, 584, 210, 646, 717]
image sizes: [(836, 529), (836, 529), (836, 529), (836, 529), (836, 529), (836, 529), (836, 529), (836, 529), (836, 529), (836, 529)]
input_ids shape: torch.Size([10, 937])
attention_mask shape: torch.Size([10, 937])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 937


===== Batch 366 =====
QIDs: [1563, 1563, 1563, 1563, 1563, 1563, 1563, 1563, 1563, 1563]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [257, 226, 241, 276, 233, 260, 250, 208, 251, 255]
image sizes: [(465, 286), (465, 286), (465, 286), (465, 286), (465, 286), (465, 286), (465, 286), (465, 286), (465, 286), (465, 286)]
input_ids shape: torch.Size([10, 365])
attention_mask shape: torch.Size([10, 365])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 365


===== Batch 367 =====
QIDs: [183, 183, 183, 183, 183, 183, 183, 183, 183, 183]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [781, 375, 778, 907, 748, 785, 605, 259, 816, 724]
image sizes: [(612, 168), (612, 168), (612, 168), (612, 168), (612, 168), (612, 168), (612, 168), (612, 168), (612, 168), (612, 168)]
input_ids shape: torch.Size([10, 535])
attention_mask shape: torch.Size([10, 535])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 535


===== Batch 368 =====
QIDs: [880, 880, 880, 880, 880, 880, 880, 880, 880, 880]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [266, 135, 231, 244, 227, 230, 245, 104, 270, 258]
image sizes: [(211, 204), (211, 204), (211, 204), (211, 204), (211, 204), (211, 204), (211, 204), (211, 204), (211, 204), (211, 204)]
input_ids shape: torch.Size([10, 223])
attention_mask shape: torch.Size([10, 223])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 223


===== Batch 369 =====
QIDs: [269, 269, 269, 269, 269, 269, 269, 269, 269, 269]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [80, 69, 82, 103, 82, 79, 76, 60, 86, 83]
image sizes: [(103, 72), (103, 72), (103, 72), (103, 72), (103, 72), (103, 72), (103, 72), (103, 72), (103, 72), (103, 72)]
input_ids shape: torch.Size([10, 80])
attention_mask shape: torch.Size([10, 80])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 80


===== Batch 370 =====
QIDs: [106, 106, 106, 106, 106, 106, 106, 106, 106, 106]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [304, 191, 320, 321, 294, 299, 261, 147, 330, 303]
image sizes: [(867, 198), (867, 198), (867, 198), (867, 198), (867, 198), (867, 198), (867, 198), (867, 198), (867, 198), (867, 198)]
input_ids shape: torch.Size([10, 429])
attention_mask shape: torch.Size([10, 429])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 429


===== Batch 371 =====
QIDs: [15, 15, 15, 15, 15, 15, 15, 15, 15, 15]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [313, 204, 283, 319, 252, 283, 262, 157, 319, 297]
image sizes: [(712, 115), (712, 115), (712, 115), (712, 115), (712, 115), (712, 115), (712, 115), (712, 115), (712, 115), (712, 115)]
input_ids shape: torch.Size([10, 303])
attention_mask shape: torch.Size([10, 303])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 303


===== Batch 372 =====
QIDs: [881, 881, 881, 881, 881, 881, 881, 881, 881, 881]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [248, 210, 251, 243, 220, 233, 283, 191, 234, 235]
image sizes: [(518, 174), (518, 174), (518, 174), (518, 174), (518, 174), (518, 174), (518, 174), (518, 174), (518, 174), (518, 174)]
input_ids shape: torch.Size([10, 292])
attention_mask shape: torch.Size([10, 292])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 292


===== Batch 373 =====
QIDs: [1542, 1542, 1542, 1542, 1542, 1542, 1542, 1542, 1542, 1542]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [343, 280, 345, 331, 332, 331, 305, 253, 350, 344]
image sizes: [(637, 499), (637, 499), (637, 499), (637, 499), (637, 499), (637, 499), (637, 499), (637, 499), (637, 499), (637, 499)]
input_ids shape: torch.Size([10, 660])
attention_mask shape: torch.Size([10, 660])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 660


===== Batch 374 =====
QIDs: [29, 29, 29, 29, 29, 29, 29, 29, 29, 29]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [339, 215, 331, 338, 307, 312, 293, 171, 339, 343]
image sizes: [(615, 124), (615, 124), (615, 124), (615, 124), (615, 124), (615, 124), (615, 124), (615, 124), (615, 124), (615, 124)]
input_ids shape: torch.Size([10, 310])
attention_mask shape: torch.Size([10, 310])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 310


===== Batch 375 =====
QIDs: [814, 814, 814, 814, 814, 814, 814, 814, 814, 814]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1199, 542, 1079, 1203, 862, 1116, 961, 339, 1149, 1122]
image sizes: [(1207, 171), (1207, 171), (1207, 171), (1207, 171), (1207, 171), (1207, 171), (1207, 171), (1207, 171), (1207, 171), (1207, 171)]
input_ids shape: torch.Size([10, 825])
attention_mask shape: torch.Size([10, 825])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 825


===== Batch 376 =====
QIDs: [854, 854, 854, 854, 854, 854, 854, 854, 854, 854]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [521, 333, 468, 521, 457, 485, 447, 260, 562, 525]
image sizes: [(568, 496), (568, 496), (568, 496), (568, 496), (568, 496), (568, 496), (568, 496), (568, 496), (568, 496), (568, 496)]
input_ids shape: torch.Size([10, 679])
attention_mask shape: torch.Size([10, 679])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 679


===== Batch 377 =====
QIDs: [1399, 1399, 1399, 1399, 1399, 1399, 1399, 1399, 1399, 1399]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [259, 130, 266, 256, 213, 282, 255, 110, 295, 264]
image sizes: [(595, 457), (595, 457), (595, 457), (595, 457), (595, 457), (595, 457), (595, 457), (595, 457), (595, 457), (595, 457)]
input_ids shape: torch.Size([10, 518])
attention_mask shape: torch.Size([10, 518])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 518


===== Batch 378 =====
QIDs: [420, 420, 420, 420, 420, 420, 420, 420, 420, 420]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [252, 120, 211, 258, 196, 229, 224, 87, 256, 234]
image sizes: [(853, 648), (853, 648), (853, 648), (853, 648), (853, 648), (853, 648), (853, 648), (853, 648), (853, 648), (853, 648)]
input_ids shape: torch.Size([10, 844])
attention_mask shape: torch.Size([10, 844])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 844


===== Batch 379 =====
QIDs: [77, 77, 77, 77, 77, 77, 77, 77, 77, 77]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [815, 647, 779, 837, 770, 795, 797, 503, 808, 794]
image sizes: [(653, 316), (653, 316), (653, 316), (653, 316), (653, 316), (653, 316), (653, 316), (653, 316), (653, 316), (653, 316)]
input_ids shape: torch.Size([10, 863])
attention_mask shape: torch.Size([10, 863])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 863


===== Batch 380 =====
QIDs: [731, 731, 731, 731, 731, 731, 731, 731, 731, 731]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [325, 175, 361, 358, 332, 326, 331, 154, 377, 309]
image sizes: [(882, 730), (882, 730), (882, 730), (882, 730), (882, 730), (882, 730), (882, 730), (882, 730), (882, 730), (882, 730)]
input_ids shape: torch.Size([10, 1090])
attention_mask shape: torch.Size([10, 1090])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1090


===== Batch 381 =====
QIDs: [355, 355, 355, 355, 355, 355, 355, 355, 355, 355]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [108, 65, 108, 118, 109, 105, 117, 64, 113, 131]
image sizes: [(1548, 1118), (1548, 1118), (1548, 1118), (1548, 1118), (1548, 1118), (1548, 1118), (1548, 1118), (1548, 1118), (1548, 1118), (1548, 1118)]
input_ids shape: torch.Size([10, 2279])
attention_mask shape: torch.Size([10, 2279])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2279


===== Batch 382 =====
QIDs: [1320, 1320, 1320, 1320, 1320, 1320, 1320, 1320, 1320, 1320]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [297, 156, 329, 269, 284, 299, 316, 119, 324, 263]
image sizes: [(1962, 322), (1962, 322), (1962, 322), (1962, 322), (1962, 322), (1962, 322), (1962, 322), (1962, 322), (1962, 322), (1962, 322)]
input_ids shape: torch.Size([10, 1069])
attention_mask shape: torch.Size([10, 1069])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1069


===== Batch 383 =====
QIDs: [1536, 1536, 1536, 1536, 1536, 1536, 1536, 1536, 1536, 1536]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [438, 264, 423, 427, 437, 428, 451, 200, 457, 412]
image sizes: [(615, 322), (615, 322), (615, 322), (615, 322), (615, 322), (615, 322), (615, 322), (615, 322), (615, 322), (615, 322)]
input_ids shape: torch.Size([10, 571])
attention_mask shape: torch.Size([10, 571])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 571


===== Batch 384 =====
QIDs: [1661, 1661, 1661, 1661, 1661, 1661, 1661, 1661, 1661, 1661]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [814, 344, 620, 656, 730, 663, 665, 221, 717, 754]
image sizes: [(432, 322), (432, 322), (432, 322), (432, 322), (432, 322), (432, 322), (432, 322), (432, 322), (432, 322), (432, 322)]
input_ids shape: torch.Size([10, 589])
attention_mask shape: torch.Size([10, 589])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 589


===== Batch 385 =====
QIDs: [743, 743, 743, 743, 743, 743, 743, 743, 743, 743]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [465, 264, 464, 513, 410, 496, 400, 178, 579, 497]
image sizes: [(914, 396), (914, 396), (914, 396), (914, 396), (914, 396), (914, 396), (914, 396), (914, 396), (914, 396), (914, 396)]
input_ids shape: torch.Size([10, 728])
attention_mask shape: torch.Size([10, 728])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 728


===== Batch 386 =====
QIDs: [353, 353, 353, 353, 353, 353, 353, 353, 353, 353]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [273, 135, 275, 290, 238, 256, 256, 103, 268, 270]
image sizes: [(1500, 1524), (1500, 1524), (1500, 1524), (1500, 1524), (1500, 1524), (1500, 1524), (1500, 1524), (1500, 1524), (1500, 1524), (1500, 1524)]
input_ids shape: torch.Size([10, 3086])
attention_mask shape: torch.Size([10, 3086])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 3086


===== Batch 387 =====
QIDs: [821, 821, 821, 821, 821, 821, 821, 821, 821, 821]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [397, 244, 406, 422, 347, 410, 346, 184, 425, 397]
image sizes: [(862, 562), (862, 562), (862, 562), (862, 562), (862, 562), (862, 562), (862, 562), (862, 562), (862, 562), (862, 562)]
input_ids shape: torch.Size([10, 886])
attention_mask shape: torch.Size([10, 886])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 886


===== Batch 388 =====
QIDs: [18, 18, 18, 18, 18, 18, 18, 18, 18, 18]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [218, 164, 243, 206, 200, 222, 190, 148, 250, 228]
image sizes: [(735, 442), (735, 442), (735, 442), (735, 442), (735, 442), (735, 442), (735, 442), (735, 442), (735, 442), (735, 442)]
input_ids shape: torch.Size([10, 592])
attention_mask shape: torch.Size([10, 592])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 592


===== Batch 389 =====
QIDs: [1027, 1027, 1027, 1027, 1027, 1027, 1027, 1027, 1027, 1027]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [442, 263, 449, 485, 331, 499, 388, 198, 460, 456]
image sizes: [(342, 212), (342, 212), (342, 212), (342, 212), (342, 212), (342, 212), (342, 212), (342, 212), (342, 212), (342, 212)]
input_ids shape: torch.Size([10, 373])
attention_mask shape: torch.Size([10, 373])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 373


===== Batch 390 =====
QIDs: [1010, 1010, 1010, 1010, 1010, 1010, 1010, 1010, 1010, 1010]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [412, 241, 399, 488, 360, 376, 349, 152, 399, 417]
image sizes: [(308, 258), (308, 258), (308, 258), (308, 258), (308, 258), (308, 258), (308, 258), (308, 258), (308, 258), (308, 258)]
input_ids shape: torch.Size([10, 345])
attention_mask shape: torch.Size([10, 345])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 345


===== Batch 391 =====
QIDs: [1307, 1307, 1307, 1307, 1307, 1307, 1307, 1307, 1307, 1307]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [190, 158, 178, 180, 180, 188, 181, 150, 179, 188]
image sizes: [(144, 124), (144, 124), (144, 124), (144, 124), (144, 124), (144, 124), (144, 124), (144, 124), (144, 124), (144, 124)]
input_ids shape: torch.Size([10, 203])
attention_mask shape: torch.Size([10, 203])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 203


===== Batch 392 =====
QIDs: [1565, 1565, 1565, 1565, 1565, 1565, 1565, 1565, 1565, 1565]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [953, 908, 949, 949, 959, 936, 930, 882, 948, 955]
image sizes: [(441, 256), (441, 256), (441, 256), (441, 256), (441, 256), (441, 256), (441, 256), (441, 256), (441, 256), (441, 256)]
input_ids shape: torch.Size([10, 819])
attention_mask shape: torch.Size([10, 819])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 819


===== Batch 393 =====
QIDs: [674, 674, 674, 674, 674, 674, 674, 674, 674, 674]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1274, 600, 1164, 1275, 1091, 1076, 1104, 454, 1127, 1083]
image sizes: [(1418, 958), (1418, 958), (1418, 958), (1418, 958), (1418, 958), (1418, 958), (1418, 958), (1418, 958), (1418, 958), (1418, 958)]
input_ids shape: torch.Size([10, 2522])
attention_mask shape: torch.Size([10, 2522])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2522


===== Batch 394 =====
QIDs: [5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [639, 436, 629, 704, 599, 685, 599, 362, 676, 631]
image sizes: [(1182, 511), (1182, 511), (1182, 511), (1182, 511), (1182, 511), (1182, 511), (1182, 511), (1182, 511), (1182, 511), (1182, 511)]
input_ids shape: torch.Size([10, 1180])
attention_mask shape: torch.Size([10, 1180])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1180


===== Batch 395 =====
QIDs: [191, 191, 191, 191, 191, 191, 191, 191, 191, 191]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [263, 110, 243, 269, 194, 209, 172, 96, 234, 194]
image sizes: [(330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260)]
input_ids shape: torch.Size([10, 257])
attention_mask shape: torch.Size([10, 257])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 257


===== Batch 396 =====
QIDs: [645, 645, 645, 645, 645, 645, 645, 645, 645, 645]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [260, 203, 254, 247, 418, 245, 329, 190, 231, 240]
image sizes: [(465, 470), (465, 470), (465, 470), (465, 470), (465, 470), (465, 470), (465, 470), (465, 470), (465, 470), (465, 470)]
input_ids shape: torch.Size([10, 550])
attention_mask shape: torch.Size([10, 550])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 550


===== Batch 397 =====
QIDs: [255, 255, 255, 255, 255, 255, 255, 255, 255, 255]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [137, 80, 127, 121, 120, 133, 102, 68, 141, 117]
image sizes: [(686, 458), (686, 458), (686, 458), (686, 458), (686, 458), (686, 458), (686, 458), (686, 458), (686, 458), (686, 458)]
input_ids shape: torch.Size([10, 472])
attention_mask shape: torch.Size([10, 472])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 472


===== Batch 398 =====
QIDs: [1265, 1265, 1265, 1265, 1265, 1265, 1265, 1265, 1265, 1265]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [84, 65, 82, 92, 90, 81, 77, 54, 82, 80]
image sizes: [(689, 520), (689, 520), (689, 520), (689, 520), (689, 520), (689, 520), (689, 520), (689, 520), (689, 520), (689, 520)]
input_ids shape: torch.Size([10, 544])
attention_mask shape: torch.Size([10, 544])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 544


===== Batch 399 =====
QIDs: [971, 971, 971, 971, 971, 971, 971, 971, 971, 971]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [424, 214, 391, 432, 329, 392, 384, 147, 386, 364]
image sizes: [(497, 206), (497, 206), (497, 206), (497, 206), (497, 206), (497, 206), (497, 206), (497, 206), (497, 206), (497, 206)]
input_ids shape: torch.Size([10, 351])
attention_mask shape: torch.Size([10, 351])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 351


===== Batch 400 =====
QIDs: [332, 332, 332, 332, 332, 332, 332, 332, 332, 332]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [254, 127, 249, 253, 270, 263, 276, 108, 257, 269]
image sizes: [(296, 360), (296, 360), (296, 360), (296, 360), (296, 360), (296, 360), (296, 360), (296, 360), (296, 360), (296, 360)]
input_ids shape: torch.Size([10, 331])
attention_mask shape: torch.Size([10, 331])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 331


===== Batch 401 =====
QIDs: [1530, 1530, 1530, 1530, 1530, 1530, 1530, 1530, 1530, 1530]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [377, 195, 347, 314, 261, 307, 261, 118, 349, 270]
image sizes: [(598, 342), (598, 342), (598, 342), (598, 342), (598, 342), (598, 342), (598, 342), (598, 342), (598, 342), (598, 342)]
input_ids shape: torch.Size([10, 470])
attention_mask shape: torch.Size([10, 470])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 470


===== Batch 402 =====
QIDs: [1084, 1084, 1084, 1084, 1084, 1084, 1084, 1084, 1084, 1084]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1408, 637, 1350, 1417, 1142, 1342, 1118, 422, 1533, 1365]
image sizes: [(752, 231), (752, 231), (752, 231), (752, 231), (752, 231), (752, 231), (752, 231), (752, 231), (752, 231), (752, 231)]
input_ids shape: torch.Size([10, 868])
attention_mask shape: torch.Size([10, 868])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 868


===== Batch 403 =====
QIDs: [1143, 1143, 1143, 1143, 1143, 1143, 1143, 1143, 1143, 1143]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [218, 101, 227, 230, 205, 211, 236, 86, 255, 202]
image sizes: [(1248, 844), (1248, 844), (1248, 844), (1248, 844), (1248, 844), (1248, 844), (1248, 844), (1248, 844), (1248, 844), (1248, 844)]
input_ids shape: torch.Size([10, 1501])
attention_mask shape: torch.Size([10, 1501])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1501


===== Batch 404 =====
QIDs: [1328, 1328, 1328, 1328, 1328, 1328, 1328, 1328, 1328, 1328]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [836, 403, 874, 856, 760, 839, 815, 265, 950, 848]
image sizes: [(504, 332), (504, 332), (504, 332), (504, 332), (504, 332), (504, 332), (504, 332), (504, 332), (504, 332), (504, 332)]
input_ids shape: torch.Size([10, 720])
attention_mask shape: torch.Size([10, 720])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 720


===== Batch 405 =====
QIDs: [298, 298, 298, 298, 298, 298, 298, 298, 298, 298]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [795, 401, 792, 848, 716, 814, 734, 277, 862, 802]
image sizes: [(504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330)]
input_ids shape: torch.Size([10, 681])
attention_mask shape: torch.Size([10, 681])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 681


===== Batch 406 =====
QIDs: [796, 796, 796, 796, 796, 796, 796, 796, 796, 796]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1325, 588, 1212, 1433, 1106, 1209, 1163, 352, 1543, 1455]
image sizes: [(1227, 244), (1227, 244), (1227, 244), (1227, 244), (1227, 244), (1227, 244), (1227, 244), (1227, 244), (1227, 244), (1227, 244)]
input_ids shape: torch.Size([10, 1036])
attention_mask shape: torch.Size([10, 1036])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1036


===== Batch 407 =====
QIDs: [199, 199, 199, 199, 199, 199, 199, 199, 199, 199]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [246, 122, 242, 243, 200, 269, 300, 92, 280, 193]
image sizes: [(384, 387), (384, 387), (384, 387), (384, 387), (384, 387), (384, 387), (384, 387), (384, 387), (384, 387), (384, 387)]
input_ids shape: torch.Size([10, 409])
attention_mask shape: torch.Size([10, 409])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 409


===== Batch 408 =====
QIDs: [34, 34, 34, 34, 34, 34, 34, 34, 34, 34]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [149, 100, 148, 157, 423, 160, 145, 92, 186, 145]
image sizes: [(2560, 2545), (2560, 2545), (2560, 2545), (2560, 2545), (2560, 2545), (2560, 2545), (2560, 2545), (2560, 2545), (2560, 2545), (2560, 2545)]
input_ids shape: torch.Size([10, 8192])
attention_mask shape: torch.Size([10, 8192])
vision_start count = 10, vision_end count = 0
NaN in input_ids: False
Sequence length = 8192
[ERROR] Batch 408 crash!
Exception: ValueError('Image features and image tokens do not match: tokens: 81770, features 82810')
First 50 input_ids: [151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655]
vision token positions: [14]
Sample text preview: <image 1> Potato plants located at RHS Wisley\nA. Genetic\nB. Aquatic\nC. Confused\nD. Abiotic\nE. Organic\nF. Inorganic\nG. Biotic\nH. Terrestrial\nI. Hybrid


===== Batch 409 =====
QIDs: [895, 895, 895, 895, 895, 895, 895, 895, 895, 895]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1729, 1709, 1725, 1729, 1715, 1723, 1721, 1687, 1762, 1788]
image sizes: [(1037, 887), (1037, 887), (1037, 887), (1037, 887), (1037, 887), (1037, 887), (1037, 887), (1037, 887), (1037, 887), (1037, 887)]
input_ids shape: torch.Size([10, 2323])
attention_mask shape: torch.Size([10, 2323])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2323


===== Batch 410 =====
QIDs: [267, 267, 267, 267, 267, 267, 267, 267, 267, 267]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [98, 63, 85, 105, 81, 88, 70, 44, 101, 86]
image sizes: [(1108, 748), (1108, 748), (1108, 748), (1108, 748), (1108, 748), (1108, 748), (1108, 748), (1108, 748), (1108, 748), (1108, 748)]
input_ids shape: torch.Size([10, 1151])
attention_mask shape: torch.Size([10, 1151])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1151


===== Batch 411 =====
QIDs: [804, 804, 804, 804, 804, 804, 804, 804, 804, 804]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [715, 406, 722, 724, 657, 667, 614, 301, 770, 652]
image sizes: [(1227, 532), (1227, 532), (1227, 532), (1227, 532), (1227, 532), (1227, 532), (1227, 532), (1227, 532), (1227, 532), (1227, 532)]
input_ids shape: torch.Size([10, 1247])
attention_mask shape: torch.Size([10, 1247])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1247


===== Batch 412 =====
QIDs: [1343, 1343, 1343, 1343, 1343, 1343, 1343, 1343, 1343, 1343]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [811, 461, 780, 785, 669, 806, 788, 286, 862, 809]
image sizes: [(432, 780), (432, 780), (432, 780), (432, 780), (432, 780), (432, 780), (432, 780), (432, 780), (432, 780), (432, 780)]
input_ids shape: torch.Size([10, 916])
attention_mask shape: torch.Size([10, 916])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 916


===== Batch 413 =====
QIDs: [566, 566, 566, 566, 566, 566, 566, 566, 566, 566]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [239, 162, 251, 282, 226, 258, 217, 139, 262, 231]
image sizes: [(246, 219), (246, 219), (246, 219), (246, 219), (246, 219), (246, 219), (246, 219), (246, 219), (246, 219), (246, 219)]
input_ids shape: torch.Size([10, 259])
attention_mask shape: torch.Size([10, 259])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 259


===== Batch 414 =====
QIDs: [1103, 1103, 1103, 1103, 1103, 1103, 1103, 1103, 1103, 1103]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [154, 132, 158, 158, 146, 153, 148, 117, 184, 155]
image sizes: [(452, 115), (452, 115), (452, 115), (452, 115), (452, 115), (452, 115), (452, 115), (452, 115), (452, 115), (452, 115)]
input_ids shape: torch.Size([10, 213])
attention_mask shape: torch.Size([10, 213])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 213


===== Batch 415 =====
QIDs: [799, 799, 799, 799, 799, 799, 799, 799, 799, 799]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [312, 140, 305, 270, 216, 276, 266, 115, 328, 269]
image sizes: [(544, 277), (544, 277), (544, 277), (544, 277), (544, 277), (544, 277), (544, 277), (544, 277), (544, 277), (544, 277)]
input_ids shape: torch.Size([10, 380])
attention_mask shape: torch.Size([10, 380])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 380


===== Batch 416 =====
QIDs: [1290, 1290, 1290, 1290, 1290, 1290, 1290, 1290, 1290, 1290]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [336, 182, 342, 367, 339, 352, 450, 151, 387, 324]
image sizes: [(524, 167), (524, 167), (524, 167), (524, 167), (524, 167), (524, 167), (524, 167), (524, 167), (524, 167), (524, 167)]
input_ids shape: torch.Size([10, 392])
attention_mask shape: torch.Size([10, 392])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 392


===== Batch 417 =====
QIDs: [1444, 1444, 1444, 1444, 1444, 1444, 1444, 1444, 1444, 1444]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [475, 361, 479, 460, 454, 489, 459, 303, 450, 415]
image sizes: [(609, 241), (609, 241), (609, 241), (609, 241), (609, 241), (609, 241), (609, 241), (609, 241), (609, 241), (609, 241)]
input_ids shape: torch.Size([10, 506])
attention_mask shape: torch.Size([10, 506])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 506


===== Batch 418 =====
QIDs: [507, 507, 507, 507, 507, 507, 507, 507, 507, 507]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [847, 821, 851, 849, 829, 843, 866, 799, 841, 838]
image sizes: [(592, 253), (592, 253), (592, 253), (592, 253), (592, 253), (592, 253), (592, 253), (592, 253), (592, 253), (592, 253)]
input_ids shape: torch.Size([10, 690])
attention_mask shape: torch.Size([10, 690])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 690


===== Batch 419 =====
QIDs: [527, 527, 527, 527, 527, 527, 527, 527, 527, 527]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [253, 216, 251, 255, 241, 239, 249, 197, 249, 268]
image sizes: [(574, 194), (574, 194), (574, 194), (574, 194), (574, 194), (574, 194), (574, 194), (574, 194), (574, 194), (574, 194)]
input_ids shape: torch.Size([10, 333])
attention_mask shape: torch.Size([10, 333])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 333


===== Batch 420 =====
QIDs: [815, 815, 815, 815, 815, 815, 815, 815, 815, 815]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [479, 297, 479, 517, 468, 476, 443, 222, 527, 480]
image sizes: [(862, 562), (862, 562), (862, 562), (862, 562), (862, 562), (862, 562), (862, 562), (862, 562), (862, 562), (862, 562)]
input_ids shape: torch.Size([10, 940])
attention_mask shape: torch.Size([10, 940])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 940


===== Batch 421 =====
QIDs: [460, 460, 460, 460, 460, 460, 460, 460, 460, 460]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [160, 89, 160, 176, 176, 153, 180, 81, 145, 173]
image sizes: [(540, 772), (540, 772), (540, 772), (540, 772), (540, 772), (540, 772), (540, 772), (540, 772), (540, 772), (540, 772)]
input_ids shape: torch.Size([10, 680])
attention_mask shape: torch.Size([10, 680])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 680


===== Batch 422 =====
QIDs: [718, 718, 718, 718, 718, 718, 718, 718, 718, 718]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [391, 221, 404, 433, 376, 452, 346, 160, 424, 439]
image sizes: [(250, 226), (250, 226), (250, 226), (250, 226), (250, 226), (250, 226), (250, 226), (250, 226), (250, 226), (250, 226)]
input_ids shape: torch.Size([10, 318])
attention_mask shape: torch.Size([10, 318])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 318


===== Batch 423 =====
QIDs: [618, 618, 618, 618, 618, 618, 618, 618, 618, 618]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [690, 411, 687, 652, 600, 666, 578, 352, 679, 618]
image sizes: [(514, 345), (514, 345), (514, 345), (514, 345), (514, 345), (514, 345), (514, 345), (514, 345), (514, 345), (514, 345)]
input_ids shape: torch.Size([10, 621])
attention_mask shape: torch.Size([10, 621])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 621


===== Batch 424 =====
QIDs: [546, 546, 546, 546, 546, 546, 546, 546, 546, 546]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [499, 393, 538, 545, 582, 514, 495, 342, 568, 508]
image sizes: [(367, 315), (367, 315), (367, 315), (367, 315), (367, 315), (367, 315), (367, 315), (367, 315), (367, 315), (367, 315)]
input_ids shape: torch.Size([10, 465])
attention_mask shape: torch.Size([10, 465])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 465


===== Batch 425 =====
QIDs: [574, 574, 574, 574, 574, 574, 574, 574, 574, 574]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [262, 206, 263, 265, 247, 252, 247, 183, 285, 265]
image sizes: [(271, 274), (271, 274), (271, 274), (271, 274), (271, 274), (271, 274), (271, 274), (271, 274), (271, 274), (271, 274)]
input_ids shape: torch.Size([10, 292])
attention_mask shape: torch.Size([10, 292])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 292


===== Batch 426 =====
QIDs: [800, 800, 800, 800, 800, 800, 800, 800, 800, 800]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [956, 463, 882, 918, 688, 907, 771, 296, 1032, 790]
image sizes: [(1258, 366), (1258, 366), (1258, 366), (1258, 366), (1258, 366), (1258, 366), (1258, 366), (1258, 366), (1258, 366), (1258, 366)]
input_ids shape: torch.Size([10, 1059])
attention_mask shape: torch.Size([10, 1059])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1059


===== Batch 427 =====
QIDs: [1324, 1324, 1324, 1324, 1324, 1324, 1324, 1324, 1324, 1324]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [641, 323, 576, 627, 532, 587, 515, 200, 640, 627]
image sizes: [(504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331)]
input_ids shape: torch.Size([10, 533])
attention_mask shape: torch.Size([10, 533])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 533


===== Batch 428 =====
QIDs: [87, 87, 87, 87, 87, 87, 87, 87, 87, 87]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [655, 543, 727, 602, 672, 696, 813, 528, 588, 668]
image sizes: [(669, 170), (669, 170), (669, 170), (669, 170), (669, 170), (669, 170), (669, 170), (669, 170), (669, 170), (669, 170)]
input_ids shape: torch.Size([10, 681])
attention_mask shape: torch.Size([10, 681])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 681


===== Batch 429 =====
QIDs: [1649, 1649, 1649, 1649, 1649, 1649, 1649, 1649, 1649, 1649]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [166, 137, 164, 172, 161, 168, 146, 120, 181, 155]
image sizes: [(236, 189), (236, 189), (236, 189), (236, 189), (236, 189), (236, 189), (236, 189), (236, 189), (236, 189), (236, 189)]
input_ids shape: torch.Size([10, 177])
attention_mask shape: torch.Size([10, 177])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 177


===== Batch 430 =====
QIDs: [28, 28, 28, 28, 28, 28, 28, 28, 28, 28]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [575, 302, 533, 565, 491, 522, 532, 203, 599, 620]
image sizes: [(592, 139), (592, 139), (592, 139), (592, 139), (592, 139), (592, 139), (592, 139), (592, 139), (592, 139), (592, 139)]
input_ids shape: torch.Size([10, 438])
attention_mask shape: torch.Size([10, 438])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 438


===== Batch 431 =====
QIDs: [301, 301, 301, 301, 301, 301, 301, 301, 301, 301]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [435, 210, 426, 406, 386, 426, 387, 168, 469, 479]
image sizes: [(316, 479), (316, 479), (316, 479), (316, 479), (316, 479), (316, 479), (316, 479), (316, 479), (316, 479), (316, 479)]
input_ids shape: torch.Size([10, 433])
attention_mask shape: torch.Size([10, 433])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 433


===== Batch 432 =====
QIDs: [479, 479, 479, 479, 479, 479, 479, 479, 479, 479]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [276, 196, 258, 268, 274, 252, 219, 135, 254, 280]
image sizes: [(418, 103), (418, 103), (418, 103), (418, 103), (418, 103), (418, 103), (418, 103), (418, 103), (418, 103), (418, 103)]
input_ids shape: torch.Size([10, 239])
attention_mask shape: torch.Size([10, 239])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 239


===== Batch 433 =====
QIDs: [591, 591, 591, 591, 591, 591, 591, 591, 591, 591]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [694, 616, 706, 703, 669, 751, 676, 596, 696, 704]
image sizes: [(672, 283), (672, 283), (672, 283), (672, 283), (672, 283), (672, 283), (672, 283), (672, 283), (672, 283), (672, 283)]
input_ids shape: torch.Size([10, 804])
attention_mask shape: torch.Size([10, 804])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 804


===== Batch 434 =====
QIDs: [1455, 1455, 1455, 1455, 1455, 1455, 1455, 1455, 1455, 1455]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1043, 585, 963, 1027, 901, 1094, 1014, 410, 1131, 1070]
image sizes: [(629, 222), (629, 222), (629, 222), (629, 222), (629, 222), (629, 222), (629, 222), (629, 222), (629, 222), (629, 222)]
input_ids shape: torch.Size([10, 800])
attention_mask shape: torch.Size([10, 800])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 800


===== Batch 435 =====
QIDs: [483, 483, 483, 483, 483, 483, 483, 483, 483, 483]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [699, 366, 741, 820, 559, 759, 642, 218, 773, 663]
image sizes: [(497, 480), (497, 480), (497, 480), (497, 480), (497, 480), (497, 480), (497, 480), (497, 480), (497, 480), (497, 480)]
input_ids shape: torch.Size([10, 688])
attention_mask shape: torch.Size([10, 688])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 688


===== Batch 436 =====
QIDs: [1579, 1579, 1579, 1579, 1579, 1579, 1579, 1579, 1579, 1579]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [496, 477, 484, 489, 510, 492, 478, 383, 506, 487]
image sizes: [(1498, 1224), (1498, 1224), (1498, 1224), (1498, 1224), (1498, 1224), (1498, 1224), (1498, 1224), (1498, 1224), (1498, 1224), (1498, 1224)]
input_ids shape: torch.Size([10, 2685])
attention_mask shape: torch.Size([10, 2685])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2685


===== Batch 437 =====
QIDs: [1062, 1062, 1062, 1062, 1062, 1062, 1062, 1062, 1062, 1062]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [713, 355, 635, 677, 594, 652, 591, 242, 698, 683]
image sizes: [(1444, 412), (1444, 412), (1444, 412), (1444, 412), (1444, 412), (1444, 412), (1444, 412), (1444, 412), (1444, 412), (1444, 412)]
input_ids shape: torch.Size([10, 1124])
attention_mask shape: torch.Size([10, 1124])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1124


===== Batch 438 =====
QIDs: [1674, 1674, 1674, 1674, 1674, 1674, 1674, 1674, 1674, 1674]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [359, 210, 327, 362, 307, 324, 315, 160, 369, 389]
image sizes: [(250, 176), (250, 176), (250, 176), (250, 176), (250, 176), (250, 176), (250, 176), (250, 176), (250, 176), (250, 176)]
input_ids shape: torch.Size([10, 271])
attention_mask shape: torch.Size([10, 271])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 271


===== Batch 439 =====
QIDs: [876, 876, 876, 876, 876, 876, 876, 876, 876, 876]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [197, 149, 196, 210, 189, 200, 166, 117, 216, 204]
image sizes: [(325, 275), (325, 275), (325, 275), (325, 275), (325, 275), (325, 275), (325, 275), (325, 275), (325, 275), (325, 275)]
input_ids shape: torch.Size([10, 260])
attention_mask shape: torch.Size([10, 260])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 260


===== Batch 440 =====
QIDs: [496, 496, 496, 496, 496, 496, 496, 496, 496, 496]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [779, 358, 725, 823, 589, 664, 518, 227, 807, 671]
image sizes: [(308, 298), (308, 298), (308, 298), (308, 298), (308, 298), (308, 298), (308, 298), (308, 298), (308, 298), (308, 298)]
input_ids shape: torch.Size([10, 477])
attention_mask shape: torch.Size([10, 477])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 477


===== Batch 441 =====
QIDs: [1723, 1723, 1723, 1723, 1723, 1723, 1723, 1723, 1723, 1723]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1025, 524, 1077, 1255, 877, 958, 891, 318, 1093, 1060]
image sizes: [(1614, 818), (1614, 818), (1614, 818), (1614, 818), (1614, 818), (1614, 818), (1614, 818), (1614, 818), (1614, 818), (1614, 818)]
input_ids shape: torch.Size([10, 2248])
attention_mask shape: torch.Size([10, 2248])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2248


===== Batch 442 =====
QIDs: [1618, 1618, 1618, 1618, 1618, 1618, 1618, 1618, 1618, 1618]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [306, 179, 294, 309, 278, 330, 318, 129, 368, 307]
image sizes: [(1557, 2057), (1557, 2057), (1557, 2057), (1557, 2057), (1557, 2057), (1557, 2057), (1557, 2057), (1557, 2057), (1557, 2057), (1557, 2057)]
input_ids shape: torch.Size([10, 4313])
attention_mask shape: torch.Size([10, 4313])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 4313


===== Batch 443 =====
QIDs: [776, 776, 776, 776, 776, 776, 776, 776, 776, 776]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [527, 255, 504, 578, 442, 504, 411, 156, 554, 566]
image sizes: [(1039, 1138), (1039, 1138), (1039, 1138), (1039, 1138), (1039, 1138), (1039, 1138), (1039, 1138), (1039, 1138), (1039, 1138), (1039, 1138)]
input_ids shape: torch.Size([10, 1787])
attention_mask shape: torch.Size([10, 1787])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1787


===== Batch 444 =====
QIDs: [1428, 1428, 1428, 1428, 1428, 1428, 1428, 1428, 1428, 1428]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [688, 474, 700, 699, 709, 678, 642, 393, 714, 699]
image sizes: [(934, 587), (934, 587), (934, 587), (934, 587), (934, 587), (934, 587), (934, 587), (934, 587), (934, 587), (934, 587)]
input_ids shape: torch.Size([10, 1147])
attention_mask shape: torch.Size([10, 1147])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1147


===== Batch 445 =====
QIDs: [584, 584, 584, 584, 584, 584, 584, 584, 584, 584]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [390, 269, 378, 416, 419, 376, 456, 206, 406, 362]
image sizes: [(415, 451), (415, 451), (415, 451), (415, 451), (415, 451), (415, 451), (415, 451), (415, 451), (415, 451), (415, 451)]
input_ids shape: torch.Size([10, 565])
attention_mask shape: torch.Size([10, 565])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 565


===== Batch 446 =====
QIDs: [846, 846, 846, 846, 846, 846, 846, 846, 846, 846]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [322, 231, 332, 356, 328, 335, 322, 212, 359, 324]
image sizes: [(841, 228), (841, 228), (841, 228), (841, 228), (841, 228), (841, 228), (841, 228), (841, 228), (841, 228), (841, 228)]
input_ids shape: torch.Size([10, 462])
attention_mask shape: torch.Size([10, 462])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 462


===== Batch 447 =====
QIDs: [1291, 1291, 1291, 1291, 1291, 1291, 1291, 1291, 1291, 1291]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [169, 84, 170, 156, 138, 177, 177, 66, 175, 166]
image sizes: [(355, 174), (355, 174), (355, 174), (355, 174), (355, 174), (355, 174), (355, 174), (355, 174), (355, 174), (355, 174)]
input_ids shape: torch.Size([10, 225])
attention_mask shape: torch.Size([10, 225])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 225


===== Batch 448 =====
QIDs: [585, 585, 585, 585, 585, 585, 585, 585, 585, 585]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [485, 405, 482, 486, 490, 466, 484, 381, 486, 492]
image sizes: [(564, 720), (564, 720), (564, 720), (564, 720), (564, 720), (564, 720), (564, 720), (564, 720), (564, 720), (564, 720)]
input_ids shape: torch.Size([10, 838])
attention_mask shape: torch.Size([10, 838])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 838


===== Batch 449 =====
QIDs: [293, 293, 293, 293, 293, 293, 293, 293, 293, 293]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [107, 79, 101, 103, 95, 110, 92, 67, 126, 115]
image sizes: [(154, 139), (154, 139), (154, 139), (154, 139), (154, 139), (154, 139), (154, 139), (154, 139), (154, 139), (154, 139)]
input_ids shape: torch.Size([10, 107])
attention_mask shape: torch.Size([10, 107])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 107


===== Batch 450 =====
QIDs: [1466, 1466, 1466, 1466, 1466, 1466, 1466, 1466, 1466, 1466]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [276, 152, 296, 301, 291, 280, 248, 133, 314, 301]
image sizes: [(480, 262), (480, 262), (480, 262), (480, 262), (480, 262), (480, 262), (480, 262), (480, 262), (480, 262), (480, 262)]
input_ids shape: torch.Size([10, 324])
attention_mask shape: torch.Size([10, 324])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 324


===== Batch 451 =====
QIDs: [281, 281, 281, 281, 281, 281, 281, 281, 281, 281]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [406, 215, 367, 375, 327, 370, 324, 141, 375, 408]
image sizes: [(112, 112), (112, 112), (112, 112), (112, 112), (112, 112), (112, 112), (112, 112), (112, 112), (112, 112), (112, 112)]
input_ids shape: torch.Size([10, 240])
attention_mask shape: torch.Size([10, 240])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 240


===== Batch 452 =====
QIDs: [807, 807, 807, 807, 807, 807, 807, 807, 807, 807]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [469, 284, 436, 476, 397, 436, 433, 202, 472, 437]
image sizes: [(1224, 309), (1224, 309), (1224, 309), (1224, 309), (1224, 309), (1224, 309), (1224, 309), (1224, 309), (1224, 309), (1224, 309)]
input_ids shape: torch.Size([10, 798])
attention_mask shape: torch.Size([10, 798])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 798


===== Batch 453 =====
QIDs: [789, 789, 789, 789, 789, 789, 789, 789, 789, 789]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [499, 296, 465, 511, 477, 555, 427, 196, 575, 587]
image sizes: [(1273, 456), (1273, 456), (1273, 456), (1273, 456), (1273, 456), (1273, 456), (1273, 456), (1273, 456), (1273, 456), (1273, 456)]
input_ids shape: torch.Size([10, 986])
attention_mask shape: torch.Size([10, 986])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 986


===== Batch 454 =====
QIDs: [481, 481, 481, 481, 481, 481, 481, 481, 481, 481]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [794, 477, 779, 814, 655, 758, 692, 399, 798, 758]
image sizes: [(1222, 540), (1222, 540), (1222, 540), (1222, 540), (1222, 540), (1222, 540), (1222, 540), (1222, 540), (1222, 540), (1222, 540)]
input_ids shape: torch.Size([10, 1258])
attention_mask shape: torch.Size([10, 1258])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1258


===== Batch 455 =====
QIDs: [681, 681, 681, 681, 681, 681, 681, 681, 681, 681]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [308, 157, 286, 300, 296, 294, 287, 120, 324, 314]
image sizes: [(908, 484), (908, 484), (908, 484), (908, 484), (908, 484), (908, 484), (908, 484), (908, 484), (908, 484), (908, 484)]
input_ids shape: torch.Size([10, 745])
attention_mask shape: torch.Size([10, 745])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 745


===== Batch 456 =====
QIDs: [232, 232, 232, 232, 232, 232, 232, 232, 232, 232]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [471, 252, 454, 490, 419, 417, 452, 202, 475, 489]
image sizes: [(1150, 120), (1150, 120), (1150, 120), (1150, 120), (1150, 120), (1150, 120), (1150, 120), (1150, 120), (1150, 120), (1150, 120)]
input_ids shape: torch.Size([10, 453])
attention_mask shape: torch.Size([10, 453])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 453


===== Batch 457 =====
QIDs: [747, 747, 747, 747, 747, 747, 747, 747, 747, 747]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [624, 364, 648, 701, 547, 647, 529, 233, 615, 585]
image sizes: [(827, 272), (827, 272), (827, 272), (827, 272), (827, 272), (827, 272), (827, 272), (827, 272), (827, 272), (827, 272)]
input_ids shape: torch.Size([10, 652])
attention_mask shape: torch.Size([10, 652])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 652


===== Batch 458 =====
QIDs: [223, 223, 223, 223, 223, 223, 223, 223, 223, 223]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [320, 148, 325, 316, 298, 310, 326, 117, 312, 308]
image sizes: [(716, 409), (716, 409), (716, 409), (716, 409), (716, 409), (716, 409), (716, 409), (716, 409), (716, 409), (716, 409)]
input_ids shape: torch.Size([10, 590])
attention_mask shape: torch.Size([10, 590])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 590


===== Batch 459 =====
QIDs: [887, 887, 887, 887, 887, 887, 887, 887, 887, 887]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [148, 115, 149, 135, 131, 145, 156, 109, 141, 151]
image sizes: [(501, 369), (501, 369), (501, 369), (501, 369), (501, 369), (501, 369), (501, 369), (501, 369), (501, 369), (501, 369)]
input_ids shape: torch.Size([10, 375])
attention_mask shape: torch.Size([10, 375])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 375


===== Batch 460 =====
QIDs: [1527, 1527, 1527, 1527, 1527, 1527, 1527, 1527, 1527, 1527]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [353, 284, 369, 411, 334, 358, 314, 226, 387, 379]
image sizes: [(1285, 433), (1285, 433), (1285, 433), (1285, 433), (1285, 433), (1285, 433), (1285, 433), (1285, 433), (1285, 433), (1285, 433)]
input_ids shape: torch.Size([10, 961])
attention_mask shape: torch.Size([10, 961])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 961


===== Batch 461 =====
QIDs: [928, 928, 928, 928, 928, 928, 928, 928, 928, 928]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [204, 98, 214, 208, 190, 222, 204, 89, 246, 194]
image sizes: [(261, 83), (261, 83), (261, 83), (261, 83), (261, 83), (261, 83), (261, 83), (261, 83), (261, 83), (261, 83)]
input_ids shape: torch.Size([10, 187])
attention_mask shape: torch.Size([10, 187])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 187


===== Batch 462 =====
QIDs: [1069, 1069, 1069, 1069, 1069, 1069, 1069, 1069, 1069, 1069]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [2493, 1139, 2310, 2605, 1984, 2352, 2172, 747, 2550, 2650]
image sizes: [(1354, 860), (1354, 860), (1354, 860), (1354, 860), (1354, 860), (1354, 860), (1354, 860), (1354, 860), (1354, 860), (1354, 860)]
input_ids shape: torch.Size([10, 2673])
attention_mask shape: torch.Size([10, 2673])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2673


===== Batch 463 =====
QIDs: [772, 772, 772, 772, 772, 772, 772, 772, 772, 772]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [287, 150, 309, 293, 259, 276, 276, 119, 304, 270]
image sizes: [(524, 553), (524, 553), (524, 553), (524, 553), (524, 553), (524, 553), (524, 553), (524, 553), (524, 553), (524, 553)]
input_ids shape: torch.Size([10, 570])
attention_mask shape: torch.Size([10, 570])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 570


===== Batch 464 =====
QIDs: [1491, 1491, 1491, 1491, 1491, 1491, 1491, 1491, 1491, 1491]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [457, 221, 454, 483, 397, 458, 429, 166, 490, 477]
image sizes: [(2511, 1843), (2511, 1843), (2511, 1843), (2511, 1843), (2511, 1843), (2511, 1843), (2511, 1843), (2511, 1843), (2511, 1843), (2511, 1843)]
input_ids shape: torch.Size([10, 6192])
attention_mask shape: torch.Size([10, 6192])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 6192


===== Batch 465 =====
QIDs: [110, 110, 110, 110, 110, 110, 110, 110, 110, 110]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1702, 1244, 1648, 1864, 1727, 1553, 1684, 865, 1639, 1798]
image sizes: [(661, 523), (661, 523), (661, 523), (661, 523), (661, 523), (661, 523), (661, 523), (661, 523), (661, 523), (661, 523)]
input_ids shape: torch.Size([10, 1527])
attention_mask shape: torch.Size([10, 1527])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1527


===== Batch 466 =====
QIDs: [1451, 1451, 1451, 1451, 1451, 1451, 1451, 1451, 1451, 1451]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [599, 309, 583, 610, 530, 631, 563, 214, 668, 622]
image sizes: [(350, 276), (350, 276), (350, 276), (350, 276), (350, 276), (350, 276), (350, 276), (350, 276), (350, 276), (350, 276)]
input_ids shape: torch.Size([10, 481])
attention_mask shape: torch.Size([10, 481])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 481


===== Batch 467 =====
QIDs: [413, 413, 413, 413, 413, 413, 413, 413, 413, 413]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [247, 141, 237, 235, 225, 252, 235, 110, 271, 248]
image sizes: [(500, 381), (500, 381), (500, 381), (500, 381), (500, 381), (500, 381), (500, 381), (500, 381), (500, 381), (500, 381)]
input_ids shape: torch.Size([10, 418])
attention_mask shape: torch.Size([10, 418])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 418


===== Batch 468 =====
QIDs: [50, 50, 50, 50, 50, 50, 50, 50, 50, 50]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [283, 129, 276, 263, 205, 235, 240, 112, 276, 251]
image sizes: [(724, 839), (724, 839), (724, 839), (724, 839), (724, 839), (724, 839), (724, 839), (724, 839), (724, 839), (724, 839)]
input_ids shape: torch.Size([10, 953])
attention_mask shape: torch.Size([10, 953])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 953


===== Batch 469 =====
QIDs: [1585, 1585, 1585, 1585, 1585, 1585, 1585, 1585, 1585, 1585]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [59, 46, 60, 70, 55, 58, 62, 39, 60, 63]
image sizes: [(518, 221), (518, 221), (518, 221), (518, 221), (518, 221), (518, 221), (518, 221), (518, 221), (518, 221), (518, 221)]
input_ids shape: torch.Size([10, 202])
attention_mask shape: torch.Size([10, 202])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 202


===== Batch 470 =====
QIDs: [1390, 1390, 1390, 1390, 1390, 1390, 1390, 1390, 1390, 1390]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [548, 296, 545, 514, 412, 556, 444, 204, 564, 520]
image sizes: [(517, 142), (517, 142), (517, 142), (517, 142), (517, 142), (517, 142), (517, 142), (517, 142), (517, 142), (517, 142)]
input_ids shape: torch.Size([10, 403])
attention_mask shape: torch.Size([10, 403])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 403


===== Batch 471 =====
QIDs: [569, 569, 569, 569, 569, 569, 569, 569, 569, 569]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [302, 212, 311, 311, 316, 323, 270, 156, 356, 273]
image sizes: [(177, 198), (177, 198), (177, 198), (177, 198), (177, 198), (177, 198), (177, 198), (177, 198), (177, 198), (177, 198)]
input_ids shape: torch.Size([10, 265])
attention_mask shape: torch.Size([10, 265])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 265


===== Batch 472 =====
QIDs: [1512, 1512, 1512, 1512, 1512, 1512, 1512, 1512, 1512, 1512]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [399, 174, 375, 439, 363, 378, 370, 148, 391, 491]
image sizes: [(1030, 742), (1030, 742), (1030, 742), (1030, 742), (1030, 742), (1030, 742), (1030, 742), (1030, 742), (1030, 742), (1030, 742)]
input_ids shape: torch.Size([10, 1187])
attention_mask shape: torch.Size([10, 1187])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1187


===== Batch 473 =====
QIDs: [513, 513, 513, 513, 513, 513, 513, 513, 513, 513]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [268, 213, 266, 289, 258, 269, 232, 191, 270, 243]
image sizes: [(338, 262), (338, 262), (338, 262), (338, 262), (338, 262), (338, 262), (338, 262), (338, 262), (338, 262), (338, 262)]
input_ids shape: torch.Size([10, 324])
attention_mask shape: torch.Size([10, 324])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 324


===== Batch 474 =====
QIDs: [856, 856, 856, 856, 856, 856, 856, 856, 856, 856]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [117, 92, 118, 119, 105, 122, 111, 81, 139, 130]
image sizes: [(670, 577), (670, 577), (670, 577), (670, 577), (670, 577), (670, 577), (670, 577), (670, 577), (670, 577), (670, 577)]
input_ids shape: torch.Size([10, 612])
attention_mask shape: torch.Size([10, 612])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 612


===== Batch 475 =====
QIDs: [1109, 1109, 1109, 1109, 1109, 1109, 1109, 1109, 1109, 1109]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [364, 212, 379, 340, 369, 333, 310, 164, 389, 410]
image sizes: [(904, 578), (904, 578), (904, 578), (904, 578), (904, 578), (904, 578), (904, 578), (904, 578), (904, 578), (904, 578)]
input_ids shape: torch.Size([10, 912])
attention_mask shape: torch.Size([10, 912])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 912


===== Batch 476 =====
QIDs: [375, 375, 375, 375, 375, 375, 375, 375, 375, 375]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [105, 77, 106, 120, 107, 104, 104, 51, 113, 128]
image sizes: [(1548, 1118), (1548, 1118), (1548, 1118), (1548, 1118), (1548, 1118), (1548, 1118), (1548, 1118), (1548, 1118), (1548, 1118), (1548, 1118)]
input_ids shape: torch.Size([10, 2277])
attention_mask shape: torch.Size([10, 2277])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2277


===== Batch 477 =====
QIDs: [1423, 1423, 1423, 1423, 1423, 1423, 1423, 1423, 1423, 1423]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [475, 340, 508, 528, 500, 504, 464, 276, 535, 527]
image sizes: [(384, 259), (384, 259), (384, 259), (384, 259), (384, 259), (384, 259), (384, 259), (384, 259), (384, 259), (384, 259)]
input_ids shape: torch.Size([10, 471])
attention_mask shape: torch.Size([10, 471])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 471


===== Batch 478 =====
QIDs: [646, 646, 646, 646, 646, 646, 646, 646, 646, 646]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [154, 126, 183, 211, 199, 150, 177, 102, 190, 209]
image sizes: [(844, 669), (844, 669), (844, 669), (844, 669), (844, 669), (844, 669), (844, 669), (844, 669), (844, 669), (844, 669)]
input_ids shape: torch.Size([10, 870])
attention_mask shape: torch.Size([10, 870])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 870


===== Batch 479 =====
QIDs: [195, 195, 195, 195, 195, 195, 195, 195, 195, 195]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [297, 126, 319, 284, 293, 256, 318, 110, 318, 278]
image sizes: [(330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260)]
input_ids shape: torch.Size([10, 317])
attention_mask shape: torch.Size([10, 317])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 317


===== Batch 480 =====
QIDs: [1578, 1578, 1578, 1578, 1578, 1578, 1578, 1578, 1578, 1578]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [472, 420, 488, 479, 460, 467, 461, 390, 492, 480]
image sizes: [(584, 586), (584, 586), (584, 586), (584, 586), (584, 586), (584, 586), (584, 586), (584, 586), (584, 586), (584, 586)]
input_ids shape: torch.Size([10, 743])
attention_mask shape: torch.Size([10, 743])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 743


===== Batch 481 =====
QIDs: [1063, 1063, 1063, 1063, 1063, 1063, 1063, 1063, 1063, 1063]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [291, 150, 301, 307, 265, 318, 312, 108, 331, 284]
image sizes: [(594, 624), (594, 624), (594, 624), (594, 624), (594, 624), (594, 624), (594, 624), (594, 624), (594, 624), (594, 624)]
input_ids shape: torch.Size([10, 652])
attention_mask shape: torch.Size([10, 652])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 652


===== Batch 482 =====
QIDs: [1396, 1396, 1396, 1396, 1396, 1396, 1396, 1396, 1396, 1396]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [742, 667, 999, 755, 725, 738, 716, 612, 748, 736]
image sizes: [(1228, 499), (1228, 499), (1228, 499), (1228, 499), (1228, 499), (1228, 499), (1228, 499), (1228, 499), (1228, 499), (1228, 499)]
input_ids shape: torch.Size([10, 1523])
attention_mask shape: torch.Size([10, 1523])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1523


===== Batch 483 =====
QIDs: [1425, 1425, 1425, 1425, 1425, 1425, 1425, 1425, 1425, 1425]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [277, 224, 285, 297, 322, 282, 282, 183, 298, 302]
image sizes: [(346, 298), (346, 298), (346, 298), (346, 298), (346, 298), (346, 298), (346, 298), (346, 298), (346, 298), (346, 298)]
input_ids shape: torch.Size([10, 350])
attention_mask shape: torch.Size([10, 350])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 350


===== Batch 484 =====
QIDs: [352, 352, 352, 352, 352, 352, 352, 352, 352, 352]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [272, 141, 260, 286, 279, 257, 268, 117, 273, 271]
image sizes: [(291, 297), (291, 297), (291, 297), (291, 297), (291, 297), (291, 297), (291, 297), (291, 297), (291, 297), (291, 297)]
input_ids shape: torch.Size([10, 290])
attention_mask shape: torch.Size([10, 290])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 290


===== Batch 485 =====
QIDs: [1216, 1216, 1216, 1216, 1216, 1216, 1216, 1216, 1216, 1216]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [325, 192, 340, 347, 286, 311, 265, 146, 334, 337]
image sizes: [(867, 198), (867, 198), (867, 198), (867, 198), (867, 198), (867, 198), (867, 198), (867, 198), (867, 198), (867, 198)]
input_ids shape: torch.Size([10, 446])
attention_mask shape: torch.Size([10, 446])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 446


===== Batch 486 =====
QIDs: [784, 784, 784, 784, 784, 784, 784, 784, 784, 784]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1320, 628, 1222, 1402, 1093, 1216, 1135, 430, 1421, 1405]
image sizes: [(1215, 303), (1215, 303), (1215, 303), (1215, 303), (1215, 303), (1215, 303), (1215, 303), (1215, 303), (1215, 303), (1215, 303)]
input_ids shape: torch.Size([10, 1122])
attention_mask shape: torch.Size([10, 1122])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1122


===== Batch 487 =====
QIDs: [1070, 1070, 1070, 1070, 1070, 1070, 1070, 1070, 1070, 1070]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [187, 121, 192, 199, 164, 198, 165, 91, 216, 188]
image sizes: [(281, 241), (281, 241), (281, 241), (281, 241), (281, 241), (281, 241), (281, 241), (281, 241), (281, 241), (281, 241)]
input_ids shape: torch.Size([10, 218])
attention_mask shape: torch.Size([10, 218])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 218


===== Batch 488 =====
QIDs: [1015, 1015, 1015, 1015, 1015, 1015, 1015, 1015, 1015, 1015]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [615, 270, 485, 508, 527, 485, 472, 208, 472, 569]
image sizes: [(752, 470), (752, 470), (752, 470), (752, 470), (752, 470), (752, 470), (752, 470), (752, 470), (752, 470), (752, 470)]
input_ids shape: torch.Size([10, 751])
attention_mask shape: torch.Size([10, 751])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 751


===== Batch 489 =====
QIDs: [454, 454, 454, 454, 454, 454, 454, 454, 454, 454]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [424, 215, 505, 421, 513, 495, 524, 165, 490, 411]
image sizes: [(1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548)]
input_ids shape: torch.Size([10, 4102])
attention_mask shape: torch.Size([10, 4102])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 4102


===== Batch 490 =====
QIDs: [1277, 1277, 1277, 1277, 1277, 1277, 1277, 1277, 1277, 1277]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [741, 433, 732, 780, 820, 740, 694, 325, 814, 761]
image sizes: [(185, 151), (185, 151), (185, 151), (185, 151), (185, 151), (185, 151), (185, 151), (185, 151), (185, 151), (185, 151)]
input_ids shape: torch.Size([10, 471])
attention_mask shape: torch.Size([10, 471])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 471


===== Batch 491 =====
QIDs: [1299, 1299, 1299, 1299, 1299, 1299, 1299, 1299, 1299, 1299]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1057, 515, 975, 1042, 762, 964, 854, 306, 1037, 984]
image sizes: [(208, 43), (208, 43), (208, 43), (208, 43), (208, 43), (208, 43), (208, 43), (208, 43), (208, 43), (208, 43)]
input_ids shape: torch.Size([10, 535])
attention_mask shape: torch.Size([10, 535])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 535


===== Batch 492 =====
QIDs: [1217, 1217, 1217, 1217, 1217, 1217, 1217, 1217, 1217, 1217]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [627, 577, 625, 623, 620, 622, 591, 545, 642, 607]
image sizes: [(433, 102), (433, 102), (433, 102), (433, 102), (433, 102), (433, 102), (433, 102), (433, 102), (433, 102), (433, 102)]
input_ids shape: torch.Size([10, 491])
attention_mask shape: torch.Size([10, 491])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 491


===== Batch 493 =====
QIDs: [1457, 1457, 1457, 1457, 1457, 1457, 1457, 1457, 1457, 1457]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [724, 451, 704, 750, 805, 703, 869, 441, 698, 759]
image sizes: [(516, 154), (516, 154), (516, 154), (516, 154), (516, 154), (516, 154), (516, 154), (516, 154), (516, 154), (516, 154)]
input_ids shape: torch.Size([10, 732])
attention_mask shape: torch.Size([10, 732])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 732


===== Batch 494 =====
QIDs: [897, 897, 897, 897, 897, 897, 897, 897, 897, 897]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [92, 64, 98, 92, 88, 97, 89, 47, 105, 106]
image sizes: [(429, 163), (429, 163), (429, 163), (429, 163), (429, 163), (429, 163), (429, 163), (429, 163), (429, 163), (429, 163)]
input_ids shape: torch.Size([10, 162])
attention_mask shape: torch.Size([10, 162])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 162


===== Batch 495 =====
QIDs: [338, 338, 338, 338, 338, 338, 338, 338, 338, 338]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1036, 490, 905, 1022, 791, 961, 926, 291, 1113, 1016]
image sizes: [(504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331)]
input_ids shape: torch.Size([10, 767])
attention_mask shape: torch.Size([10, 767])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 767


===== Batch 496 =====
QIDs: [1513, 1513, 1513, 1513, 1513, 1513, 1513, 1513, 1513, 1513]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [310, 207, 318, 326, 279, 285, 256, 195, 305, 318]
image sizes: [(1270, 427), (1270, 427), (1270, 427), (1270, 427), (1270, 427), (1270, 427), (1270, 427), (1270, 427), (1270, 427), (1270, 427)]
input_ids shape: torch.Size([10, 906])
attention_mask shape: torch.Size([10, 906])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 906


===== Batch 497 =====
QIDs: [1166, 1166, 1166, 1166, 1166, 1166, 1166, 1166, 1166, 1166]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [509, 375, 511, 513, 480, 486, 472, 321, 533, 526]
image sizes: [(592, 139), (592, 139), (592, 139), (592, 139), (592, 139), (592, 139), (592, 139), (592, 139), (592, 139), (592, 139)]
input_ids shape: torch.Size([10, 472])
attention_mask shape: torch.Size([10, 472])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 472


===== Batch 498 =====
QIDs: [1562, 1562, 1562, 1562, 1562, 1562, 1562, 1562, 1562, 1562]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [337, 262, 343, 346, 332, 322, 301, 232, 340, 338]
image sizes: [(514, 260), (514, 260), (514, 260), (514, 260), (514, 260), (514, 260), (514, 260), (514, 260), (514, 260), (514, 260)]
input_ids shape: torch.Size([10, 388])
attention_mask shape: torch.Size([10, 388])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 388


===== Batch 499 =====
QIDs: [369, 369, 369, 369, 369, 369, 369, 369, 369, 369]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [321, 164, 306, 334, 239, 313, 309, 128, 325, 331]
image sizes: [(1574, 1382), (1574, 1382), (1574, 1382), (1574, 1382), (1574, 1382), (1574, 1382), (1574, 1382), (1574, 1382), (1574, 1382), (1574, 1382)]
input_ids shape: torch.Size([10, 2950])
attention_mask shape: torch.Size([10, 2950])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2950


===== Batch 500 =====
QIDs: [1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609, 1609]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [114, 78, 109, 122, 100, 116, 91, 66, 110, 110]
image sizes: [(2954, 526), (2954, 526), (2954, 526), (2954, 526), (2954, 526), (2954, 526), (2954, 526), (2954, 526), (2954, 526), (2954, 526)]
input_ids shape: torch.Size([10, 2096])
attention_mask shape: torch.Size([10, 2096])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2096


===== Batch 501 =====
QIDs: [1504, 1504, 1504, 1504, 1504, 1504, 1504, 1504, 1504, 1504]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [304, 153, 297, 298, 287, 288, 274, 120, 336, 281]
image sizes: [(576, 421), (576, 421), (576, 421), (576, 421), (576, 421), (576, 421), (576, 421), (576, 421), (576, 421), (576, 421)]
input_ids shape: torch.Size([10, 494])
attention_mask shape: torch.Size([10, 494])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 494


===== Batch 502 =====
QIDs: [1046, 1046, 1046, 1046, 1046, 1046, 1046, 1046, 1046, 1046]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [3565, 1659, 3490, 3622, 3278, 3407, 3182, 1003, 3778, 3604]
image sizes: [(858, 488), (858, 488), (858, 488), (858, 488), (858, 488), (858, 488), (858, 488), (858, 488), (858, 488), (858, 488)]
input_ids shape: torch.Size([10, 2269])
attention_mask shape: torch.Size([10, 2269])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2269


===== Batch 503 =====
QIDs: [587, 587, 587, 587, 587, 587, 587, 587, 587, 587]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [282, 181, 282, 282, 248, 266, 242, 147, 282, 271]
image sizes: [(650, 269), (650, 269), (650, 269), (650, 269), (650, 269), (650, 269), (650, 269), (650, 269), (650, 269), (650, 269)]
input_ids shape: torch.Size([10, 422])
attention_mask shape: torch.Size([10, 422])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 422


===== Batch 504 =====
QIDs: [1433, 1433, 1433, 1433, 1433, 1433, 1433, 1433, 1433, 1433]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [336, 232, 333, 373, 337, 356, 390, 205, 344, 363]
image sizes: [(414, 210), (414, 210), (414, 210), (414, 210), (414, 210), (414, 210), (414, 210), (414, 210), (414, 210), (414, 210)]
input_ids shape: torch.Size([10, 405])
attention_mask shape: torch.Size([10, 405])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 405


===== Batch 505 =====
QIDs: [101, 101, 101, 101, 101, 101, 101, 101, 101, 101]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [483, 317, 550, 532, 404, 493, 439, 259, 507, 469]
image sizes: [(442, 283), (442, 283), (442, 283), (442, 283), (442, 283), (442, 283), (442, 283), (442, 283), (442, 283), (442, 283)]
input_ids shape: torch.Size([10, 509])
attention_mask shape: torch.Size([10, 509])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 509


===== Batch 506 =====
QIDs: [1617, 1617, 1617, 1617, 1617, 1617, 1617, 1617, 1617, 1617]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [142, 109, 134, 123, 128, 137, 112, 95, 163, 140]
image sizes: [(2448, 362), (2448, 362), (2448, 362), (2448, 362), (2448, 362), (2448, 362), (2448, 362), (2448, 362), (2448, 362), (2448, 362)]
input_ids shape: torch.Size([10, 1233])
attention_mask shape: torch.Size([10, 1233])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1233


===== Batch 507 =====
QIDs: [1005, 1005, 1005, 1005, 1005, 1005, 1005, 1005, 1005, 1005]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [206, 121, 190, 193, 153, 181, 156, 77, 193, 205]
image sizes: [(850, 920), (850, 920), (850, 920), (850, 920), (850, 920), (850, 920), (850, 920), (850, 920), (850, 920), (850, 920)]
input_ids shape: torch.Size([10, 1106])
attention_mask shape: torch.Size([10, 1106])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1106


===== Batch 508 =====
QIDs: [1548, 1548, 1548, 1548, 1548, 1548, 1548, 1548, 1548, 1548]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [222, 177, 216, 219, 204, 226, 197, 155, 225, 209]
image sizes: [(304, 219), (304, 219), (304, 219), (304, 219), (304, 219), (304, 219), (304, 219), (304, 219), (304, 219), (304, 219)]
input_ids shape: torch.Size([10, 238])
attention_mask shape: torch.Size([10, 238])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 238


===== Batch 509 =====
QIDs: [229, 229, 229, 229, 229, 229, 229, 229, 229, 229]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [290, 127, 272, 323, 228, 266, 259, 93, 297, 262]
image sizes: [(1148, 568), (1148, 568), (1148, 568), (1148, 568), (1148, 568), (1148, 568), (1148, 568), (1148, 568), (1148, 568), (1148, 568)]
input_ids shape: torch.Size([10, 989])
attention_mask shape: torch.Size([10, 989])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 989


===== Batch 510 =====
QIDs: [431, 431, 431, 431, 431, 431, 431, 431, 431, 431]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [356, 185, 308, 372, 270, 332, 279, 119, 372, 339]
image sizes: [(200, 110), (200, 110), (200, 110), (200, 110), (200, 110), (200, 110), (200, 110), (200, 110), (200, 110), (200, 110)]
input_ids shape: torch.Size([10, 211])
attention_mask shape: torch.Size([10, 211])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 211


===== Batch 511 =====
QIDs: [1397, 1397, 1397, 1397, 1397, 1397, 1397, 1397, 1397, 1397]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [545, 333, 562, 580, 505, 534, 564, 280, 600, 559]
image sizes: [(1228, 379), (1228, 379), (1228, 379), (1228, 379), (1228, 379), (1228, 379), (1228, 379), (1228, 379), (1228, 379), (1228, 379)]
input_ids shape: torch.Size([10, 1011])
attention_mask shape: torch.Size([10, 1011])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1011


===== Batch 512 =====
QIDs: [1348, 1348, 1348, 1348, 1348, 1348, 1348, 1348, 1348, 1348]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [570, 433, 568, 595, 523, 558, 511, 357, 570, 556]
image sizes: [(129, 222), (129, 222), (129, 222), (129, 222), (129, 222), (129, 222), (129, 222), (129, 222), (129, 222), (129, 222)]
input_ids shape: torch.Size([10, 470])
attention_mask shape: torch.Size([10, 470])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 470


===== Batch 513 =====
QIDs: [1545, 1545, 1545, 1545, 1545, 1545, 1545, 1545, 1545, 1545]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [540, 459, 533, 549, 509, 517, 499, 431, 566, 529]
image sizes: [(763, 330), (763, 330), (763, 330), (763, 330), (763, 330), (763, 330), (763, 330), (763, 330), (763, 330), (763, 330)]
input_ids shape: torch.Size([10, 693])
attention_mask shape: torch.Size([10, 693])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 693


===== Batch 514 =====
QIDs: [1703, 1703, 1703, 1703, 1703, 1703, 1703, 1703, 1703, 1703]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [233, 158, 225, 227, 261, 237, 204, 129, 233, 244]
image sizes: [(533, 158), (533, 158), (533, 158), (533, 158), (533, 158), (533, 158), (533, 158), (533, 158), (533, 158), (533, 158)]
input_ids shape: torch.Size([10, 295])
attention_mask shape: torch.Size([10, 295])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 295


===== Batch 515 =====
QIDs: [564, 564, 564, 564, 564, 564, 564, 564, 564, 564]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [860, 592, 819, 848, 804, 777, 791, 543, 858, 825]
image sizes: [(459, 483), (459, 483), (459, 483), (459, 483), (459, 483), (459, 483), (459, 483), (459, 483), (459, 483), (459, 483)]
input_ids shape: torch.Size([10, 779])
attention_mask shape: torch.Size([10, 779])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 779


===== Batch 516 =====
QIDs: [896, 896, 896, 896, 896, 896, 896, 896, 896, 896]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [105, 59, 85, 91, 91, 86, 87, 44, 104, 74]
image sizes: [(387, 243), (387, 243), (387, 243), (387, 243), (387, 243), (387, 243), (387, 243), (387, 243), (387, 243), (387, 243)]
input_ids shape: torch.Size([10, 197])
attention_mask shape: torch.Size([10, 197])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 197


===== Batch 517 =====
QIDs: [1658, 1658, 1658, 1658, 1658, 1658, 1658, 1658, 1658, 1658]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [128, 94, 138, 176, 111, 121, 142, 61, 136, 152]
image sizes: [(1036, 308), (1036, 308), (1036, 308), (1036, 308), (1036, 308), (1036, 308), (1036, 308), (1036, 308), (1036, 308), (1036, 308)]
input_ids shape: torch.Size([10, 512])
attention_mask shape: torch.Size([10, 512])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 512


===== Batch 518 =====
QIDs: [1095, 1095, 1095, 1095, 1095, 1095, 1095, 1095, 1095, 1095]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [606, 346, 570, 608, 479, 558, 555, 215, 657, 627]
image sizes: [(609, 117), (609, 117), (609, 117), (609, 117), (609, 117), (609, 117), (609, 117), (609, 117), (609, 117), (609, 117)]
input_ids shape: torch.Size([10, 456])
attention_mask shape: torch.Size([10, 456])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 456


===== Batch 519 =====
QIDs: [762, 762, 762, 762, 762, 762, 762, 762, 762, 762]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [405, 219, 392, 461, 332, 402, 352, 162, 464, 396]
image sizes: [(588, 152), (588, 152), (588, 152), (588, 152), (588, 152), (588, 152), (588, 152), (588, 152), (588, 152), (588, 152)]
input_ids shape: torch.Size([10, 343])
attention_mask shape: torch.Size([10, 343])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 343


===== Batch 520 =====
QIDs: [206, 206, 206, 206, 206, 206, 206, 206, 206, 206]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [517, 279, 544, 590, 473, 559, 517, 182, 667, 515]
image sizes: [(359, 424), (359, 424), (359, 424), (359, 424), (359, 424), (359, 424), (359, 424), (359, 424), (359, 424), (359, 424)]
input_ids shape: torch.Size([10, 527])
attention_mask shape: torch.Size([10, 527])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 527


===== Batch 521 =====
QIDs: [74, 74, 74, 74, 74, 74, 74, 74, 74, 74]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [525, 359, 442, 483, 450, 458, 462, 352, 454, 465]
image sizes: [(320, 193), (320, 193), (320, 193), (320, 193), (320, 193), (320, 193), (320, 193), (320, 193), (320, 193), (320, 193)]
input_ids shape: torch.Size([10, 412])
attention_mask shape: torch.Size([10, 412])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 412


===== Batch 522 =====
QIDs: [1136, 1136, 1136, 1136, 1136, 1136, 1136, 1136, 1136, 1136]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [167, 99, 162, 170, 163, 162, 173, 90, 165, 203]
image sizes: [(1600, 1066), (1600, 1066), (1600, 1066), (1600, 1066), (1600, 1066), (1600, 1066), (1600, 1066), (1600, 1066), (1600, 1066), (1600, 1066)]
input_ids shape: torch.Size([10, 2301])
attention_mask shape: torch.Size([10, 2301])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2301


===== Batch 523 =====
QIDs: [1038, 1038, 1038, 1038, 1038, 1038, 1038, 1038, 1038, 1038]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [382, 224, 403, 388, 321, 383, 377, 191, 388, 379]
image sizes: [(1130, 224), (1130, 224), (1130, 224), (1130, 224), (1130, 224), (1130, 224), (1130, 224), (1130, 224), (1130, 224), (1130, 224)]
input_ids shape: torch.Size([10, 539])
attention_mask shape: torch.Size([10, 539])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 539


===== Batch 524 =====
QIDs: [865, 865, 865, 865, 865, 865, 865, 865, 865, 865]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [231, 176, 223, 233, 221, 211, 203, 159, 229, 240]
image sizes: [(458, 332), (458, 332), (458, 332), (458, 332), (458, 332), (458, 332), (458, 332), (458, 332), (458, 332), (458, 332)]
input_ids shape: torch.Size([10, 359])
attention_mask shape: torch.Size([10, 359])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 359


===== Batch 525 =====
QIDs: [1339, 1339, 1339, 1339, 1339, 1339, 1339, 1339, 1339, 1339]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [403, 254, 346, 341, 352, 315, 340, 187, 437, 357]
image sizes: [(262, 222), (262, 222), (262, 222), (262, 222), (262, 222), (262, 222), (262, 222), (262, 222), (262, 222), (262, 222)]
input_ids shape: torch.Size([10, 310])
attention_mask shape: torch.Size([10, 310])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 310


===== Batch 526 =====
QIDs: [1081, 1081, 1081, 1081, 1081, 1081, 1081, 1081, 1081, 1081]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [367, 202, 400, 409, 375, 391, 334, 153, 435, 403]
image sizes: [(1726, 630), (1726, 630), (1726, 630), (1726, 630), (1726, 630), (1726, 630), (1726, 630), (1726, 630), (1726, 630), (1726, 630)]
input_ids shape: torch.Size([10, 1592])
attention_mask shape: torch.Size([10, 1592])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1592


===== Batch 527 =====
QIDs: [435, 435, 435, 435, 435, 435, 435, 435, 435, 435]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [383, 188, 377, 363, 696, 406, 393, 143, 400, 357]
image sizes: [(1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548)]
input_ids shape: torch.Size([10, 4010])
attention_mask shape: torch.Size([10, 4010])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 4010


===== Batch 528 =====
QIDs: [1620, 1620, 1620, 1620, 1620, 1620, 1620, 1620, 1620, 1620]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [62, 58, 78, 84, 75, 71, 75, 48, 89, 77]
image sizes: [(1651, 187), (1651, 187), (1651, 187), (1651, 187), (1651, 187), (1651, 187), (1651, 187), (1651, 187), (1651, 187), (1651, 187)]
input_ids shape: torch.Size([10, 485])
attention_mask shape: torch.Size([10, 485])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 485


===== Batch 529 =====
QIDs: [434, 434, 434, 434, 434, 434, 434, 434, 434, 434]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [103, 49, 101, 85, 137, 105, 90, 39, 119, 97]
image sizes: [(1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604)]
input_ids shape: torch.Size([10, 3892])
attention_mask shape: torch.Size([10, 3892])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 3892


===== Batch 530 =====
QIDs: [1430, 1430, 1430, 1430, 1430, 1430, 1430, 1430, 1430, 1430]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [312, 229, 304, 303, 337, 277, 286, 194, 308, 320]
image sizes: [(262, 295), (262, 295), (262, 295), (262, 295), (262, 295), (262, 295), (262, 295), (262, 295), (262, 295), (262, 295)]
input_ids shape: torch.Size([10, 327])
attention_mask shape: torch.Size([10, 327])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 327


===== Batch 531 =====
QIDs: [939, 939, 939, 939, 939, 939, 939, 939, 939, 939]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [282, 160, 221, 310, 183, 247, 248, 110, 240, 216]
image sizes: [(1205, 118), (1205, 118), (1205, 118), (1205, 118), (1205, 118), (1205, 118), (1205, 118), (1205, 118), (1205, 118), (1205, 118)]
input_ids shape: torch.Size([10, 349])
attention_mask shape: torch.Size([10, 349])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 349


===== Batch 532 =====
QIDs: [599, 599, 599, 599, 599, 599, 599, 599, 599, 599]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [425, 274, 401, 421, 375, 417, 419, 205, 414, 404]
image sizes: [(640, 147), (640, 147), (640, 147), (640, 147), (640, 147), (640, 147), (640, 147), (640, 147), (640, 147), (640, 147)]
input_ids shape: torch.Size([10, 386])
attention_mask shape: torch.Size([10, 386])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 386


===== Batch 533 =====
QIDs: [733, 733, 733, 733, 733, 733, 733, 733, 733, 733]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [245, 139, 249, 301, 217, 237, 232, 99, 269, 263]
image sizes: [(1200, 797), (1200, 797), (1200, 797), (1200, 797), (1200, 797), (1200, 797), (1200, 797), (1200, 797), (1200, 797), (1200, 797)]
input_ids shape: torch.Size([10, 1371])
attention_mask shape: torch.Size([10, 1371])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1371


===== Batch 534 =====
QIDs: [508, 508, 508, 508, 508, 508, 508, 508, 508, 508]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [284, 226, 265, 275, 273, 285, 253, 204, 283, 285]
image sizes: [(407, 207), (407, 207), (407, 207), (407, 207), (407, 207), (407, 207), (407, 207), (407, 207), (407, 207), (407, 207)]
input_ids shape: torch.Size([10, 315])
attention_mask shape: torch.Size([10, 315])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 315


===== Batch 535 =====
QIDs: [986, 986, 986, 986, 986, 986, 986, 986, 986, 986]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [534, 304, 476, 547, 489, 461, 434, 196, 477, 545]
image sizes: [(559, 164), (559, 164), (559, 164), (559, 164), (559, 164), (559, 164), (559, 164), (559, 164), (559, 164), (559, 164)]
input_ids shape: torch.Size([10, 429])
attention_mask shape: torch.Size([10, 429])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 429


===== Batch 536 =====
QIDs: [1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515, 1515]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [318, 214, 294, 397, 308, 288, 275, 163, 327, 312]
image sizes: [(1243, 724), (1243, 724), (1243, 724), (1243, 724), (1243, 724), (1243, 724), (1243, 724), (1243, 724), (1243, 724), (1243, 724)]
input_ids shape: torch.Size([10, 1393])
attention_mask shape: torch.Size([10, 1393])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1393


===== Batch 537 =====
QIDs: [1413, 1413, 1413, 1413, 1413, 1413, 1413, 1413, 1413, 1413]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [797, 771, 792, 823, 780, 793, 792, 749, 781, 789]
image sizes: [(592, 253), (592, 253), (592, 253), (592, 253), (592, 253), (592, 253), (592, 253), (592, 253), (592, 253), (592, 253)]
input_ids shape: torch.Size([10, 645])
attention_mask shape: torch.Size([10, 645])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 645


===== Batch 538 =====
QIDs: [246, 246, 246, 246, 246, 246, 246, 246, 246, 246]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [486, 301, 441, 506, 406, 467, 446, 239, 496, 463]
image sizes: [(1518, 614), (1518, 614), (1518, 614), (1518, 614), (1518, 614), (1518, 614), (1518, 614), (1518, 614), (1518, 614), (1518, 614)]
input_ids shape: torch.Size([10, 1485])
attention_mask shape: torch.Size([10, 1485])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1485


===== Batch 539 =====
QIDs: [16, 16, 16, 16, 16, 16, 16, 16, 16, 16]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [531, 323, 574, 598, 487, 604, 515, 239, 590, 576]
image sizes: [(781, 199), (781, 199), (781, 199), (781, 199), (781, 199), (781, 199), (781, 199), (781, 199), (781, 199), (781, 199)]
input_ids shape: torch.Size([10, 531])
attention_mask shape: torch.Size([10, 531])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 531


===== Batch 540 =====
QIDs: [1706, 1706, 1706, 1706, 1706, 1706, 1706, 1706, 1706, 1706]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [117, 106, 113, 115, 108, 113, 116, 98, 110, 132]
image sizes: [(1180, 636), (1180, 636), (1180, 636), (1180, 636), (1180, 636), (1180, 636), (1180, 636), (1180, 636), (1180, 636), (1180, 636)]
input_ids shape: torch.Size([10, 1088])
attention_mask shape: torch.Size([10, 1088])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1088


===== Batch 541 =====
QIDs: [327, 327, 327, 327, 327, 327, 327, 327, 327, 327]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [297, 187, 291, 307, 274, 300, 269, 149, 309, 277]
image sizes: [(504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331)]
input_ids shape: torch.Size([10, 399])
attention_mask shape: torch.Size([10, 399])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 399


===== Batch 542 =====
QIDs: [61, 61, 61, 61, 61, 61, 61, 61, 61, 61]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [566, 275, 498, 562, 473, 519, 430, 183, 558, 515]
image sizes: [(1600, 1200), (1600, 1200), (1600, 1200), (1600, 1200), (1600, 1200), (1600, 1200), (1600, 1200), (1600, 1200), (1600, 1200), (1600, 1200)]
input_ids shape: torch.Size([10, 2720])
attention_mask shape: torch.Size([10, 2720])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2720


===== Batch 543 =====
QIDs: [82, 82, 82, 82, 82, 82, 82, 82, 82, 82]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [391, 338, 384, 394, 359, 372, 377, 314, 388, 381]
image sizes: [(309, 371), (309, 371), (309, 371), (309, 371), (309, 371), (309, 371), (309, 371), (309, 371), (309, 371), (309, 371)]
input_ids shape: torch.Size([10, 415])
attention_mask shape: torch.Size([10, 415])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 415


===== Batch 544 =====
QIDs: [933, 933, 933, 933, 933, 933, 933, 933, 933, 933]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [191, 175, 210, 234, 222, 211, 262, 104, 217, 221]
image sizes: [(1572, 116), (1572, 116), (1572, 116), (1572, 116), (1572, 116), (1572, 116), (1572, 116), (1572, 116), (1572, 116), (1572, 116)]
input_ids shape: torch.Size([10, 408])
attention_mask shape: torch.Size([10, 408])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 408


===== Batch 545 =====
QIDs: [1294, 1294, 1294, 1294, 1294, 1294, 1294, 1294, 1294, 1294]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [635, 340, 583, 597, 523, 601, 514, 210, 651, 608]
image sizes: [(826, 154), (826, 154), (826, 154), (826, 154), (826, 154), (826, 154), (826, 154), (826, 154), (826, 154), (826, 154)]
input_ids shape: torch.Size([10, 504])
attention_mask shape: torch.Size([10, 504])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 504


===== Batch 546 =====
QIDs: [1305, 1305, 1305, 1305, 1305, 1305, 1305, 1305, 1305, 1305]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [100, 71, 91, 92, 104, 96, 97, 68, 96, 106]
image sizes: [(199, 64), (199, 64), (199, 64), (199, 64), (199, 64), (199, 64), (199, 64), (199, 64), (199, 64), (199, 64)]
input_ids shape: torch.Size([10, 90])
attention_mask shape: torch.Size([10, 90])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 90


===== Batch 547 =====
QIDs: [1441, 1441, 1441, 1441, 1441, 1441, 1441, 1441, 1441, 1441]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [349, 249, 343, 382, 389, 340, 293, 193, 370, 373]
image sizes: [(774, 118), (774, 118), (774, 118), (774, 118), (774, 118), (774, 118), (774, 118), (774, 118), (774, 118), (774, 118)]
input_ids shape: torch.Size([10, 353])
attention_mask shape: torch.Size([10, 353])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 353


===== Batch 548 =====
QIDs: [1553, 1553, 1553, 1553, 1553, 1553, 1553, 1553, 1553, 1553]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [235, 173, 240, 219, 381, 217, 202, 159, 232, 221]
image sizes: [(697, 432), (697, 432), (697, 432), (697, 432), (697, 432), (697, 432), (697, 432), (697, 432), (697, 432), (697, 432)]
input_ids shape: torch.Size([10, 635])
attention_mask shape: torch.Size([10, 635])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 635


===== Batch 549 =====
QIDs: [1381, 1381, 1381, 1381, 1381, 1381, 1381, 1381, 1381, 1381]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [415, 203, 428, 417, 411, 445, 445, 156, 447, 412]
image sizes: [(1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604)]
input_ids shape: torch.Size([10, 4113])
attention_mask shape: torch.Size([10, 4113])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 4113


===== Batch 550 =====
QIDs: [1426, 1426, 1426, 1426, 1426, 1426, 1426, 1426, 1426, 1426]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [299, 201, 298, 327, 294, 293, 310, 178, 308, 291]
image sizes: [(355, 286), (355, 286), (355, 286), (355, 286), (355, 286), (355, 286), (355, 286), (355, 286), (355, 286), (355, 286)]
input_ids shape: torch.Size([10, 370])
attention_mask shape: torch.Size([10, 370])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 370


===== Batch 551 =====
QIDs: [188, 188, 188, 188, 188, 188, 188, 188, 188, 188]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [126, 67, 115, 132, 110, 124, 110, 54, 148, 131]
image sizes: [(540, 648), (540, 648), (540, 648), (540, 648), (540, 648), (540, 648), (540, 648), (540, 648), (540, 648), (540, 648)]
input_ids shape: torch.Size([10, 527])
attention_mask shape: torch.Size([10, 527])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 527


===== Batch 552 =====
QIDs: [948, 948, 948, 948, 948, 948, 948, 948, 948, 948]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [234, 134, 209, 240, 221, 219, 223, 106, 249, 221]
image sizes: [(472, 202), (472, 202), (472, 202), (472, 202), (472, 202), (472, 202), (472, 202), (472, 202), (472, 202), (472, 202)]
input_ids shape: torch.Size([10, 262])
attention_mask shape: torch.Size([10, 262])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 262


===== Batch 553 =====
QIDs: [1604, 1604, 1604, 1604, 1604, 1604, 1604, 1604, 1604, 1604]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [620, 395, 615, 713, 609, 606, 889, 304, 771, 666]
image sizes: [(285, 253), (285, 253), (285, 253), (285, 253), (285, 253), (285, 253), (285, 253), (285, 253), (285, 253), (285, 253)]
input_ids shape: torch.Size([10, 657])
attention_mask shape: torch.Size([10, 657])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 657


===== Batch 554 =====
QIDs: [1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011, 1011]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [372, 283, 358, 366, 344, 376, 357, 267, 375, 388]
image sizes: [(344, 236), (344, 236), (344, 236), (344, 236), (344, 236), (344, 236), (344, 236), (344, 236), (344, 236), (344, 236)]
input_ids shape: torch.Size([10, 351])
attention_mask shape: torch.Size([10, 351])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 351


===== Batch 555 =====
QIDs: [438, 438, 438, 438, 438, 438, 438, 438, 438, 438]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [516, 291, 479, 528, 437, 489, 514, 184, 572, 547]
image sizes: [(1852, 1604), (1852, 1604), (1852, 1604), (1852, 1604), (1852, 1604), (1852, 1604), (1852, 1604), (1852, 1604), (1852, 1604), (1852, 1604)]
input_ids shape: torch.Size([10, 4075])
attention_mask shape: torch.Size([10, 4075])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 4075


===== Batch 556 =====
QIDs: [953, 953, 953, 953, 953, 953, 953, 953, 953, 953]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [167, 96, 156, 172, 129, 166, 158, 81, 187, 140]
image sizes: [(245, 93), (245, 93), (245, 93), (245, 93), (245, 93), (245, 93), (245, 93), (245, 93), (245, 93), (245, 93)]
input_ids shape: torch.Size([10, 151])
attention_mask shape: torch.Size([10, 151])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 151


===== Batch 557 =====
QIDs: [576, 576, 576, 576, 576, 576, 576, 576, 576, 576]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [386, 271, 388, 417, 420, 410, 433, 210, 461, 430]
image sizes: [(318, 240), (318, 240), (318, 240), (318, 240), (318, 240), (318, 240), (318, 240), (318, 240), (318, 240), (318, 240)]
input_ids shape: torch.Size([10, 402])
attention_mask shape: torch.Size([10, 402])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 402


===== Batch 558 =====
QIDs: [1085, 1085, 1085, 1085, 1085, 1085, 1085, 1085, 1085, 1085]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [617, 299, 572, 634, 508, 581, 553, 224, 672, 652]
image sizes: [(609, 117), (609, 117), (609, 117), (609, 117), (609, 117), (609, 117), (609, 117), (609, 117), (609, 117), (609, 117)]
input_ids shape: torch.Size([10, 462])
attention_mask shape: torch.Size([10, 462])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 462


===== Batch 559 =====
QIDs: [1318, 1318, 1318, 1318, 1318, 1318, 1318, 1318, 1318, 1318]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [277, 174, 286, 257, 255, 261, 266, 138, 416, 220]
image sizes: [(2024, 374), (2024, 374), (2024, 374), (2024, 374), (2024, 374), (2024, 374), (2024, 374), (2024, 374), (2024, 374), (2024, 374)]
input_ids shape: torch.Size([10, 1148])
attention_mask shape: torch.Size([10, 1148])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1148


===== Batch 560 =====
QIDs: [504, 504, 504, 504, 504, 504, 504, 504, 504, 504]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [288, 226, 274, 310, 256, 266, 252, 201, 273, 280]
image sizes: [(420, 167), (420, 167), (420, 167), (420, 167), (420, 167), (420, 167), (420, 167), (420, 167), (420, 167), (420, 167)]
input_ids shape: torch.Size([10, 312])
attention_mask shape: torch.Size([10, 312])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 312


===== Batch 561 =====
QIDs: [1208, 1208, 1208, 1208, 1208, 1208, 1208, 1208, 1208, 1208]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [708, 502, 663, 716, 627, 677, 667, 478, 706, 722]
image sizes: [(382, 353), (382, 353), (382, 353), (382, 353), (382, 353), (382, 353), (382, 353), (382, 353), (382, 353), (382, 353)]
input_ids shape: torch.Size([10, 619])
attention_mask shape: torch.Size([10, 619])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 619


===== Batch 562 =====
QIDs: [1434, 1434, 1434, 1434, 1434, 1434, 1434, 1434, 1434, 1434]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [488, 416, 471, 493, 456, 468, 470, 388, 479, 482]
image sizes: [(922, 236), (922, 236), (922, 236), (922, 236), (922, 236), (922, 236), (922, 236), (922, 236), (922, 236), (922, 236)]
input_ids shape: torch.Size([10, 580])
attention_mask shape: torch.Size([10, 580])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 580


===== Batch 563 =====
QIDs: [360, 360, 360, 360, 360, 360, 360, 360, 360, 360]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [102, 75, 107, 114, 106, 111, 102, 54, 136, 123]
image sizes: [(748, 433), (748, 433), (748, 433), (748, 433), (748, 433), (748, 433), (748, 433), (748, 433), (748, 433), (748, 433)]
input_ids shape: torch.Size([10, 497])
attention_mask shape: torch.Size([10, 497])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 497


===== Batch 564 =====
QIDs: [22, 22, 22, 22, 22, 22, 22, 22, 22, 22]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [184, 135, 185, 205, 172, 185, 158, 113, 190, 184]
image sizes: [(637, 303), (637, 303), (637, 303), (637, 303), (637, 303), (637, 303), (637, 303), (637, 303), (637, 303), (637, 303)]
input_ids shape: torch.Size([10, 398])
attention_mask shape: torch.Size([10, 398])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 398


===== Batch 565 =====
QIDs: [1453, 1453, 1453, 1453, 1453, 1453, 1453, 1453, 1453, 1453]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [269, 193, 261, 270, 234, 265, 226, 157, 288, 277]
image sizes: [(701, 256), (701, 256), (701, 256), (701, 256), (701, 256), (701, 256), (701, 256), (701, 256), (701, 256), (701, 256)]
input_ids shape: torch.Size([10, 417])
attention_mask shape: torch.Size([10, 417])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 417


===== Batch 566 =====
QIDs: [40, 40, 40, 40, 40, 40, 40, 40, 40, 40]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [408, 194, 406, 411, 332, 402, 340, 140, 434, 430]
image sizes: [(1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560)]
input_ids shape: torch.Size([10, 6511])
attention_mask shape: torch.Size([10, 6511])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 6511


===== Batch 567 =====
QIDs: [1635, 1635, 1635, 1635, 1635, 1635, 1635, 1635, 1635, 1635]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [330, 171, 346, 376, 303, 329, 300, 122, 401, 343]
image sizes: [(345, 270), (345, 270), (345, 270), (345, 270), (345, 270), (345, 270), (345, 270), (345, 270), (345, 270), (345, 270)]
input_ids shape: torch.Size([10, 343])
attention_mask shape: torch.Size([10, 343])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 343


===== Batch 568 =====
QIDs: [1273, 1273, 1273, 1273, 1273, 1273, 1273, 1273, 1273, 1273]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [206, 93, 219, 196, 190, 204, 182, 94, 249, 192]
image sizes: [(472, 377), (472, 377), (472, 377), (472, 377), (472, 377), (472, 377), (472, 377), (472, 377), (472, 377), (472, 377)]
input_ids shape: torch.Size([10, 372])
attention_mask shape: torch.Size([10, 372])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 372


===== Batch 569 =====
QIDs: [1577, 1577, 1577, 1577, 1577, 1577, 1577, 1577, 1577, 1577]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [240, 220, 233, 232, 241, 232, 224, 207, 240, 238]
image sizes: [(325, 371), (325, 371), (325, 371), (325, 371), (325, 371), (325, 371), (325, 371), (325, 371), (325, 371), (325, 371)]
input_ids shape: torch.Size([10, 318])
attention_mask shape: torch.Size([10, 318])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 318


===== Batch 570 =====
QIDs: [782, 782, 782, 782, 782, 782, 782, 782, 782, 782]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [712, 408, 702, 759, 570, 639, 639, 358, 711, 668]
image sizes: [(1053, 736), (1053, 736), (1053, 736), (1053, 736), (1053, 736), (1053, 736), (1053, 736), (1053, 736), (1053, 736), (1053, 736)]
input_ids shape: torch.Size([10, 1431])
attention_mask shape: torch.Size([10, 1431])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1431


===== Batch 571 =====
QIDs: [958, 958, 958, 958, 958, 958, 958, 958, 958, 958]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [70, 49, 65, 66, 57, 71, 64, 40, 78, 56]
image sizes: [(195, 415), (195, 415), (195, 415), (195, 415), (195, 415), (195, 415), (195, 415), (195, 415), (195, 415), (195, 415)]
input_ids shape: torch.Size([10, 166])
attention_mask shape: torch.Size([10, 166])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 166


===== Batch 572 =====
QIDs: [1427, 1427, 1427, 1427, 1427, 1427, 1427, 1427, 1427, 1427]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [213, 166, 219, 206, 193, 215, 203, 151, 224, 246]
image sizes: [(485, 232), (485, 232), (485, 232), (485, 232), (485, 232), (485, 232), (485, 232), (485, 232), (485, 232), (485, 232)]
input_ids shape: torch.Size([10, 292])
attention_mask shape: torch.Size([10, 292])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 292


===== Batch 573 =====
QIDs: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [869, 447, 811, 883, 740, 771, 797, 271, 832, 811]
image sizes: [(612, 205), (612, 205), (612, 205), (612, 205), (612, 205), (612, 205), (612, 205), (612, 205), (612, 205), (612, 205)]
input_ids shape: torch.Size([10, 584])
attention_mask shape: torch.Size([10, 584])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 584


===== Batch 574 =====
QIDs: [1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [2238, 1068, 2139, 2238, 1819, 2119, 1911, 686, 2277, 2310]
image sizes: [(867, 521), (867, 521), (867, 521), (867, 521), (867, 521), (867, 521), (867, 521), (867, 521), (867, 521), (867, 521)]
input_ids shape: torch.Size([10, 1659])
attention_mask shape: torch.Size([10, 1659])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1659


===== Batch 575 =====
QIDs: [1539, 1539, 1539, 1539, 1539, 1539, 1539, 1539, 1539, 1539]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [317, 209, 280, 288, 300, 285, 288, 163, 309, 288]
image sizes: [(621, 687), (621, 687), (621, 687), (621, 687), (621, 687), (621, 687), (621, 687), (621, 687), (621, 687), (621, 687)]
input_ids shape: torch.Size([10, 767])
attention_mask shape: torch.Size([10, 767])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 767


===== Batch 576 =====
QIDs: [46, 46, 46, 46, 46, 46, 46, 46, 46, 46]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [269, 141, 253, 253, 246, 233, 235, 109, 268, 250]
image sizes: [(2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920)]
input_ids shape: torch.Size([10, 6442])
attention_mask shape: torch.Size([10, 6442])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 6442


===== Batch 577 =====
QIDs: [778, 778, 778, 778, 778, 778, 778, 778, 778, 778]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [793, 403, 778, 845, 701, 771, 768, 277, 903, 816]
image sizes: [(590, 606), (590, 606), (590, 606), (590, 606), (590, 606), (590, 606), (590, 606), (590, 606), (590, 606), (590, 606)]
input_ids shape: torch.Size([10, 881])
attention_mask shape: torch.Size([10, 881])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 881


===== Batch 578 =====
QIDs: [514, 514, 514, 514, 514, 514, 514, 514, 514, 514]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [667, 579, 668, 729, 636, 666, 638, 521, 709, 675]
image sizes: [(509, 207), (509, 207), (509, 207), (509, 207), (509, 207), (509, 207), (509, 207), (509, 207), (509, 207), (509, 207)]
input_ids shape: torch.Size([10, 577])
attention_mask shape: torch.Size([10, 577])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 577


===== Batch 579 =====
QIDs: [1483, 1483, 1483, 1483, 1483, 1483, 1483, 1483, 1483, 1483]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [244, 141, 235, 270, 264, 238, 224, 106, 248, 218]
image sizes: [(400, 615), (400, 615), (400, 615), (400, 615), (400, 615), (400, 615), (400, 615), (400, 615), (400, 615), (400, 615)]
input_ids shape: torch.Size([10, 465])
attention_mask shape: torch.Size([10, 465])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 465


===== Batch 580 =====
QIDs: [1652, 1652, 1652, 1652, 1652, 1652, 1652, 1652, 1652, 1652]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [316, 214, 302, 313, 353, 299, 261, 176, 330, 295]
image sizes: [(524, 282), (524, 282), (524, 282), (524, 282), (524, 282), (524, 282), (524, 282), (524, 282), (524, 282), (524, 282)]
input_ids shape: torch.Size([10, 403])
attention_mask shape: torch.Size([10, 403])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 403


===== Batch 581 =====
QIDs: [1152, 1152, 1152, 1152, 1152, 1152, 1152, 1152, 1152, 1152]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [222, 120, 238, 243, 202, 220, 226, 98, 280, 228]
image sizes: [(878, 812), (878, 812), (878, 812), (878, 812), (878, 812), (878, 812), (878, 812), (878, 812), (878, 812), (878, 812)]
input_ids shape: torch.Size([10, 1057])
attention_mask shape: torch.Size([10, 1057])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1057


===== Batch 582 =====
QIDs: [883, 883, 883, 883, 883, 883, 883, 883, 883, 883]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [366, 234, 389, 381, 325, 346, 339, 176, 374, 388]
image sizes: [(538, 300), (538, 300), (538, 300), (538, 300), (538, 300), (538, 300), (538, 300), (538, 300), (538, 300), (538, 300)]
input_ids shape: torch.Size([10, 454])
attention_mask shape: torch.Size([10, 454])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 454


===== Batch 583 =====
QIDs: [402, 402, 402, 402, 402, 402, 402, 402, 402, 402]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [142, 92, 145, 151, 127, 149, 136, 88, 149, 148]
image sizes: [(640, 522), (640, 522), (640, 522), (640, 522), (640, 522), (640, 522), (640, 522), (640, 522), (640, 522), (640, 522)]
input_ids shape: torch.Size([10, 551])
attention_mask shape: torch.Size([10, 551])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 551


===== Batch 584 =====
QIDs: [619, 619, 619, 619, 619, 619, 619, 619, 619, 619]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [475, 329, 503, 524, 449, 467, 493, 276, 527, 503]
image sizes: [(567, 376), (567, 376), (567, 376), (567, 376), (567, 376), (567, 376), (567, 376), (567, 376), (567, 376), (567, 376)]
input_ids shape: torch.Size([10, 634])
attention_mask shape: torch.Size([10, 634])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 634


===== Batch 585 =====
QIDs: [1386, 1386, 1386, 1386, 1386, 1386, 1386, 1386, 1386, 1386]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [338, 157, 350, 372, 306, 354, 369, 125, 383, 361]
image sizes: [(1914, 1548), (1914, 1548), (1914, 1548), (1914, 1548), (1914, 1548), (1914, 1548), (1914, 1548), (1914, 1548), (1914, 1548), (1914, 1548)]
input_ids shape: torch.Size([10, 3976])
attention_mask shape: torch.Size([10, 3976])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 3976


===== Batch 586 =====
QIDs: [1197, 1197, 1197, 1197, 1197, 1197, 1197, 1197, 1197, 1197]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [313, 232, 286, 301, 265, 289, 277, 208, 285, 324]
image sizes: [(380, 167), (380, 167), (380, 167), (380, 167), (380, 167), (380, 167), (380, 167), (380, 167), (380, 167), (380, 167)]
input_ids shape: torch.Size([10, 300])
attention_mask shape: torch.Size([10, 300])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 300


===== Batch 587 =====
QIDs: [94, 94, 94, 94, 94, 94, 94, 94, 94, 94]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [214, 200, 321, 259, 200, 319, 235, 171, 268, 221]
image sizes: [(251, 199), (251, 199), (251, 199), (251, 199), (251, 199), (251, 199), (251, 199), (251, 199), (251, 199), (251, 199)]
input_ids shape: torch.Size([10, 284])
attention_mask shape: torch.Size([10, 284])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 284


===== Batch 588 =====
QIDs: [1611, 1611, 1611, 1611, 1611, 1611, 1611, 1611, 1611, 1611]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [79, 66, 73, 89, 70, 78, 70, 59, 78, 76]
image sizes: [(384, 154), (384, 154), (384, 154), (384, 154), (384, 154), (384, 154), (384, 154), (384, 154), (384, 154), (384, 154)]
input_ids shape: torch.Size([10, 159])
attention_mask shape: torch.Size([10, 159])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 159


===== Batch 589 =====
QIDs: [1448, 1448, 1448, 1448, 1448, 1448, 1448, 1448, 1448, 1448]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [472, 285, 459, 473, 398, 460, 411, 234, 442, 454]
image sizes: [(754, 349), (754, 349), (754, 349), (754, 349), (754, 349), (754, 349), (754, 349), (754, 349), (754, 349), (754, 349)]
input_ids shape: torch.Size([10, 606])
attention_mask shape: torch.Size([10, 606])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 606


===== Batch 590 =====
QIDs: [651, 651, 651, 651, 651, 651, 651, 651, 651, 651]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [231, 122, 227, 248, 207, 220, 233, 105, 248, 272]
image sizes: [(506, 360), (506, 360), (506, 360), (506, 360), (506, 360), (506, 360), (506, 360), (506, 360), (506, 360), (506, 360)]
input_ids shape: torch.Size([10, 382])
attention_mask shape: torch.Size([10, 382])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 382


===== Batch 591 =====
QIDs: [254, 254, 254, 254, 254, 254, 254, 254, 254, 254]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [660, 336, 691, 374, 601, 362, 341, 229, 392, 672]
image sizes: [(744, 640), (744, 640), (744, 640), (744, 640), (744, 640), (744, 640), (744, 640), (744, 640), (744, 640), (744, 640)]
input_ids shape: torch.Size([10, 993])
attention_mask shape: torch.Size([10, 993])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 993


===== Batch 592 =====
QIDs: [699, 699, 699, 699, 699, 699, 699, 699, 699, 699]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [350, 157, 293, 339, 312, 314, 289, 120, 329, 308]
image sizes: [(1096, 610), (1096, 610), (1096, 610), (1096, 610), (1096, 610), (1096, 610), (1096, 610), (1096, 610), (1096, 610), (1096, 610)]
input_ids shape: torch.Size([10, 1036])
attention_mask shape: torch.Size([10, 1036])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1036


===== Batch 593 =====
QIDs: [1271, 1271, 1271, 1271, 1271, 1271, 1271, 1271, 1271, 1271]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [240, 128, 244, 252, 166, 237, 232, 103, 251, 292]
image sizes: [(379, 478), (379, 478), (379, 478), (379, 478), (379, 478), (379, 478), (379, 478), (379, 478), (379, 478), (379, 478)]
input_ids shape: torch.Size([10, 427])
attention_mask shape: torch.Size([10, 427])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 427


===== Batch 594 =====
QIDs: [544, 544, 544, 544, 544, 544, 544, 544, 544, 544]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [307, 241, 284, 302, 275, 284, 299, 220, 292, 290]
image sizes: [(420, 167), (420, 167), (420, 167), (420, 167), (420, 167), (420, 167), (420, 167), (420, 167), (420, 167), (420, 167)]
input_ids shape: torch.Size([10, 331])
attention_mask shape: torch.Size([10, 331])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 331


===== Batch 595 =====
QIDs: [1086, 1086, 1086, 1086, 1086, 1086, 1086, 1086, 1086, 1086]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1226, 554, 1117, 1313, 1359, 1167, 979, 362, 1337, 1289]
image sizes: [(606, 141), (606, 141), (606, 141), (606, 141), (606, 141), (606, 141), (606, 141), (606, 141), (606, 141), (606, 141)]
input_ids shape: torch.Size([10, 697])
attention_mask shape: torch.Size([10, 697])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 697


===== Batch 596 =====
QIDs: [192, 192, 192, 192, 192, 192, 192, 192, 192, 192]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [318, 123, 325, 289, 289, 281, 329, 117, 348, 288]
image sizes: [(238, 260), (238, 260), (238, 260), (238, 260), (238, 260), (238, 260), (238, 260), (238, 260), (238, 260), (238, 260)]
input_ids shape: torch.Size([10, 290])
attention_mask shape: torch.Size([10, 290])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 290


===== Batch 597 =====
QIDs: [779, 779, 779, 779, 779, 779, 779, 779, 779, 779]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [849, 358, 895, 999, 696, 894, 708, 242, 893, 827]
image sizes: [(1039, 735), (1039, 735), (1039, 735), (1039, 735), (1039, 735), (1039, 735), (1039, 735), (1039, 735), (1039, 735), (1039, 735)]
input_ids shape: torch.Size([10, 1442])
attention_mask shape: torch.Size([10, 1442])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1442


===== Batch 598 =====
QIDs: [848, 848, 848, 848, 848, 848, 848, 848, 848, 848]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [259, 143, 242, 272, 207, 256, 195, 114, 281, 222]
image sizes: [(147, 292), (147, 292), (147, 292), (147, 292), (147, 292), (147, 292), (147, 292), (147, 292), (147, 292), (147, 292)]
input_ids shape: torch.Size([10, 202])
attention_mask shape: torch.Size([10, 202])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 202


===== Batch 599 =====
QIDs: [1452, 1452, 1452, 1452, 1452, 1452, 1452, 1452, 1452, 1452]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [777, 479, 810, 711, 744, 736, 679, 333, 746, 680]
image sizes: [(599, 213), (599, 213), (599, 213), (599, 213), (599, 213), (599, 213), (599, 213), (599, 213), (599, 213), (599, 213)]
input_ids shape: torch.Size([10, 610])
attention_mask shape: torch.Size([10, 610])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 610


===== Batch 600 =====
QIDs: [202, 202, 202, 202, 202, 202, 202, 202, 202, 202]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [162, 94, 153, 136, 179, 164, 178, 78, 184, 140]
image sizes: [(275, 183), (275, 183), (275, 183), (275, 183), (275, 183), (275, 183), (275, 183), (275, 183), (275, 183), (275, 183)]
input_ids shape: torch.Size([10, 211])
attention_mask shape: torch.Size([10, 211])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 211


===== Batch 601 =====
QIDs: [755, 755, 755, 755, 755, 755, 755, 755, 755, 755]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [816, 556, 912, 805, 734, 872, 684, 361, 1024, 854]
image sizes: [(688, 295), (688, 295), (688, 295), (688, 295), (688, 295), (688, 295), (688, 295), (688, 295), (688, 295), (688, 295)]
input_ids shape: torch.Size([10, 819])
attention_mask shape: torch.Size([10, 819])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 819


===== Batch 602 =====
QIDs: [774, 774, 774, 774, 774, 774, 774, 774, 774, 774]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [401, 179, 377, 457, 320, 379, 330, 125, 429, 393]
image sizes: [(915, 1356), (915, 1356), (915, 1356), (915, 1356), (915, 1356), (915, 1356), (915, 1356), (915, 1356), (915, 1356), (915, 1356)]
input_ids shape: torch.Size([10, 1810])
attention_mask shape: torch.Size([10, 1810])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1810


===== Batch 603 =====
QIDs: [531, 531, 531, 531, 531, 531, 531, 531, 531, 531]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [526, 455, 496, 544, 501, 500, 518, 424, 546, 518]
image sizes: [(392, 172), (392, 172), (392, 172), (392, 172), (392, 172), (392, 172), (392, 172), (392, 172), (392, 172), (392, 172)]
input_ids shape: torch.Size([10, 460])
attention_mask shape: torch.Size([10, 460])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 460


===== Batch 604 =====
QIDs: [1203, 1203, 1203, 1203, 1203, 1203, 1203, 1203, 1203, 1203]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1100, 826, 1214, 1239, 1130, 1200, 1271, 744, 1399, 1232]
image sizes: [(461, 234), (461, 234), (461, 234), (461, 234), (461, 234), (461, 234), (461, 234), (461, 234), (461, 234), (461, 234)]
input_ids shape: torch.Size([10, 931])
attention_mask shape: torch.Size([10, 931])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 931


===== Batch 605 =====
QIDs: [349, 349, 349, 349, 349, 349, 349, 349, 349, 349]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [342, 195, 301, 339, 310, 319, 310, 162, 343, 323]
image sizes: [(922, 529), (922, 529), (922, 529), (922, 529), (922, 529), (922, 529), (922, 529), (922, 529), (922, 529), (922, 529)]
input_ids shape: torch.Size([10, 839])
attention_mask shape: torch.Size([10, 839])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 839


===== Batch 606 =====
QIDs: [1641, 1641, 1641, 1641, 1641, 1641, 1641, 1641, 1641, 1641]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [139, 94, 137, 145, 123, 128, 129, 79, 130, 126]
image sizes: [(337, 160), (337, 160), (337, 160), (337, 160), (337, 160), (337, 160), (337, 160), (337, 160), (337, 160), (337, 160)]
input_ids shape: torch.Size([10, 178])
attention_mask shape: torch.Size([10, 178])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 178


===== Batch 607 =====
QIDs: [284, 284, 284, 284, 284, 284, 284, 284, 284, 284]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [376, 167, 357, 382, 320, 336, 346, 114, 369, 368]
image sizes: [(1084, 600), (1084, 600), (1084, 600), (1084, 600), (1084, 600), (1084, 600), (1084, 600), (1084, 600), (1084, 600), (1084, 600)]
input_ids shape: torch.Size([10, 1037])
attention_mask shape: torch.Size([10, 1037])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1037


===== Batch 608 =====
QIDs: [1044, 1044, 1044, 1044, 1044, 1044, 1044, 1044, 1044, 1044]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [2343, 1088, 2219, 2473, 1875, 2110, 1891, 622, 2408, 2247]
image sizes: [(660, 230), (660, 230), (660, 230), (660, 230), (660, 230), (660, 230), (660, 230), (660, 230), (660, 230), (660, 230)]
input_ids shape: torch.Size([10, 1242])
attention_mask shape: torch.Size([10, 1242])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1242


===== Batch 609 =====
QIDs: [1688, 1688, 1688, 1688, 1688, 1688, 1688, 1688, 1688, 1688]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [211, 125, 180, 225, 167, 180, 174, 108, 208, 203]
image sizes: [(636, 496), (636, 496), (636, 496), (636, 496), (636, 496), (636, 496), (636, 496), (636, 496), (636, 496), (636, 496)]
input_ids shape: torch.Size([10, 534])
attention_mask shape: torch.Size([10, 534])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 534


===== Batch 610 =====
QIDs: [1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [299, 181, 307, 313, 285, 289, 283, 136, 332, 304]
image sizes: [(1052, 754), (1052, 754), (1052, 754), (1052, 754), (1052, 754), (1052, 754), (1052, 754), (1052, 754), (1052, 754), (1052, 754)]
input_ids shape: torch.Size([10, 1231])
attention_mask shape: torch.Size([10, 1231])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1231


===== Batch 611 =====
QIDs: [1691, 1691, 1691, 1691, 1691, 1691, 1691, 1691, 1691, 1691]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [187, 99, 184, 192, 129, 195, 163, 74, 225, 190]
image sizes: [(772, 512), (772, 512), (772, 512), (772, 512), (772, 512), (772, 512), (772, 512), (772, 512), (772, 512), (772, 512)]
input_ids shape: torch.Size([10, 623])
attention_mask shape: torch.Size([10, 623])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 623


===== Batch 612 =====
QIDs: [97, 97, 97, 97, 97, 97, 97, 97, 97, 97]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [681, 340, 704, 708, 540, 637, 542, 246, 770, 687]
image sizes: [(442, 350), (442, 350), (442, 350), (442, 350), (442, 350), (442, 350), (442, 350), (442, 350), (442, 350), (442, 350)]
input_ids shape: torch.Size([10, 566])
attention_mask shape: torch.Size([10, 566])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 566


===== Batch 613 =====
QIDs: [593, 593, 593, 593, 593, 593, 593, 593, 593, 593]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [326, 196, 318, 333, 290, 313, 272, 158, 329, 311]
image sizes: [(635, 251), (635, 251), (635, 251), (635, 251), (635, 251), (635, 251), (635, 251), (635, 251), (635, 251), (635, 251)]
input_ids shape: torch.Size([10, 416])
attention_mask shape: torch.Size([10, 416])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 416


===== Batch 614 =====
QIDs: [437, 437, 437, 437, 437, 437, 437, 437, 437, 437]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [311, 190, 303, 336, 276, 282, 300, 128, 304, 332]
image sizes: [(694, 544), (694, 544), (694, 544), (694, 544), (694, 544), (694, 544), (694, 544), (694, 544), (694, 544), (694, 544)]
input_ids shape: torch.Size([10, 696])
attention_mask shape: torch.Size([10, 696])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 696


===== Batch 615 =====
QIDs: [1630, 1630, 1630, 1630, 1630, 1630, 1630, 1630, 1630, 1630]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [207, 123, 195, 201, 176, 204, 188, 98, 213, 201]
image sizes: [(585, 153), (585, 153), (585, 153), (585, 153), (585, 153), (585, 153), (585, 153), (585, 153), (585, 153), (585, 153)]
input_ids shape: torch.Size([10, 250])
attention_mask shape: torch.Size([10, 250])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 250


===== Batch 616 =====
QIDs: [806, 806, 806, 806, 806, 806, 806, 806, 806, 806]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1727, 911, 1651, 1824, 1505, 1587, 1382, 569, 1834, 1730]
image sizes: [(1240, 549), (1240, 549), (1240, 549), (1240, 549), (1240, 549), (1240, 549), (1240, 549), (1240, 549), (1240, 549), (1240, 549)]
input_ids shape: torch.Size([10, 1712])
attention_mask shape: torch.Size([10, 1712])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1712


===== Batch 617 =====
QIDs: [690, 690, 690, 690, 690, 690, 690, 690, 690, 690]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [891, 390, 882, 973, 708, 836, 794, 248, 928, 914]
image sizes: [(1244, 765), (1244, 765), (1244, 765), (1244, 765), (1244, 765), (1244, 765), (1244, 765), (1244, 765), (1244, 765), (1244, 765)]
input_ids shape: torch.Size([10, 1672])
attention_mask shape: torch.Size([10, 1672])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1672


===== Batch 618 =====
QIDs: [555, 555, 555, 555, 555, 555, 555, 555, 555, 555]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [269, 226, 270, 293, 300, 264, 335, 189, 271, 286]
image sizes: [(220, 169), (220, 169), (220, 169), (220, 169), (220, 169), (220, 169), (220, 169), (220, 169), (220, 169), (220, 169)]
input_ids shape: torch.Size([10, 318])
attention_mask shape: torch.Size([10, 318])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 318


===== Batch 619 =====
QIDs: [914, 914, 914, 914, 914, 914, 914, 914, 914, 914]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [801, 551, 784, 841, 694, 772, 760, 459, 814, 775]
image sizes: [(497, 180), (497, 180), (497, 180), (497, 180), (497, 180), (497, 180), (497, 180), (497, 180), (497, 180), (497, 180)]
input_ids shape: torch.Size([10, 579])
attention_mask shape: torch.Size([10, 579])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 579


===== Batch 620 =====
QIDs: [1105, 1105, 1105, 1105, 1105, 1105, 1105, 1105, 1105, 1105]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [893, 480, 858, 894, 804, 836, 736, 341, 965, 944]
image sizes: [(751, 160), (751, 160), (751, 160), (751, 160), (751, 160), (751, 160), (751, 160), (751, 160), (751, 160), (751, 160)]
input_ids shape: torch.Size([10, 646])
attention_mask shape: torch.Size([10, 646])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 646


===== Batch 621 =====
QIDs: [607, 607, 607, 607, 607, 607, 607, 607, 607, 607]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [741, 515, 698, 742, 668, 745, 679, 426, 805, 699]
image sizes: [(579, 267), (579, 267), (579, 267), (579, 267), (579, 267), (579, 267), (579, 267), (579, 267), (579, 267), (579, 267)]
input_ids shape: torch.Size([10, 659])
attention_mask shape: torch.Size([10, 659])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 659


===== Batch 622 =====
QIDs: [1623, 1623, 1623, 1623, 1623, 1623, 1623, 1623, 1623, 1623]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [133, 97, 131, 126, 112, 123, 118, 78, 129, 131]
image sizes: [(2130, 320), (2130, 320), (2130, 320), (2130, 320), (2130, 320), (2130, 320), (2130, 320), (2130, 320), (2130, 320), (2130, 320)]
input_ids shape: torch.Size([10, 951])
attention_mask shape: torch.Size([10, 951])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 951


===== Batch 623 =====
QIDs: [754, 754, 754, 754, 754, 754, 754, 754, 754, 754]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [618, 319, 591, 697, 554, 570, 564, 225, 639, 626]
image sizes: [(942, 674), (942, 674), (942, 674), (942, 674), (942, 674), (942, 674), (942, 674), (942, 674), (942, 674), (942, 674)]
input_ids shape: torch.Size([10, 1127])
attention_mask shape: torch.Size([10, 1127])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1127


===== Batch 624 =====
QIDs: [911, 911, 911, 911, 911, 911, 911, 911, 911, 911]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [74, 53, 73, 76, 72, 70, 68, 40, 74, 61]
image sizes: [(476, 188), (476, 188), (476, 188), (476, 188), (476, 188), (476, 188), (476, 188), (476, 188), (476, 188), (476, 188)]
input_ids shape: torch.Size([10, 180])
attention_mask shape: torch.Size([10, 180])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 180


===== Batch 625 =====
QIDs: [1042, 1042, 1042, 1042, 1042, 1042, 1042, 1042, 1042, 1042]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [142, 67, 115, 138, 139, 124, 106, 53, 127, 168]
image sizes: [(1248, 490), (1248, 490), (1248, 490), (1248, 490), (1248, 490), (1248, 490), (1248, 490), (1248, 490), (1248, 490), (1248, 490)]
input_ids shape: torch.Size([10, 899])
attention_mask shape: torch.Size([10, 899])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 899


===== Batch 626 =====
QIDs: [1581, 1581, 1581, 1581, 1581, 1581, 1581, 1581, 1581, 1581]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [105, 62, 91, 99, 83, 88, 81, 50, 94, 87]
image sizes: [(337, 214), (337, 214), (337, 214), (337, 214), (337, 214), (337, 214), (337, 214), (337, 214), (337, 214), (337, 214)]
input_ids shape: torch.Size([10, 166])
attention_mask shape: torch.Size([10, 166])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 166


===== Batch 627 =====
QIDs: [1272, 1272, 1272, 1272, 1272, 1272, 1272, 1272, 1272, 1272]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [959, 480, 916, 1116, 928, 961, 784, 321, 1044, 914]
image sizes: [(953, 1351), (953, 1351), (953, 1351), (953, 1351), (953, 1351), (953, 1351), (953, 1351), (953, 1351), (953, 1351), (953, 1351)]
input_ids shape: torch.Size([10, 2181])
attention_mask shape: torch.Size([10, 2181])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2181


===== Batch 628 =====
QIDs: [351, 351, 351, 351, 351, 351, 351, 351, 351, 351]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [185, 130, 185, 174, 199, 168, 200, 91, 189, 263]
image sizes: [(414, 416), (414, 416), (414, 416), (414, 416), (414, 416), (414, 416), (414, 416), (414, 416), (414, 416), (414, 416)]
input_ids shape: torch.Size([10, 355])
attention_mask shape: torch.Size([10, 355])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 355


===== Batch 629 =====
QIDs: [1154, 1154, 1154, 1154, 1154, 1154, 1154, 1154, 1154, 1154]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [744, 418, 715, 851, 678, 719, 685, 301, 812, 827]
image sizes: [(777, 184), (777, 184), (777, 184), (777, 184), (777, 184), (777, 184), (777, 184), (777, 184), (777, 184), (777, 184)]
input_ids shape: torch.Size([10, 612])
attention_mask shape: torch.Size([10, 612])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 612


===== Batch 630 =====
QIDs: [944, 944, 944, 944, 944, 944, 944, 944, 944, 944]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [162, 90, 147, 164, 135, 155, 157, 83, 172, 146]
image sizes: [(387, 121), (387, 121), (387, 121), (387, 121), (387, 121), (387, 121), (387, 121), (387, 121), (387, 121), (387, 121)]
input_ids shape: torch.Size([10, 181])
attention_mask shape: torch.Size([10, 181])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 181


===== Batch 631 =====
QIDs: [515, 515, 515, 515, 515, 515, 515, 515, 515, 515]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [203, 188, 197, 206, 191, 204, 197, 182, 201, 212]
image sizes: [(383, 108), (383, 108), (383, 108), (383, 108), (383, 108), (383, 108), (383, 108), (383, 108), (383, 108), (383, 108)]
input_ids shape: torch.Size([10, 218])
attention_mask shape: torch.Size([10, 218])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 218


===== Batch 632 =====
QIDs: [279, 279, 279, 279, 279, 279, 279, 279, 279, 279]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [119, 77, 108, 114, 105, 103, 100, 69, 114, 107]
image sizes: [(868, 728), (868, 728), (868, 728), (868, 728), (868, 728), (868, 728), (868, 728), (868, 728), (868, 728), (868, 728)]
input_ids shape: torch.Size([10, 906])
attention_mask shape: torch.Size([10, 906])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 906


===== Batch 633 =====
QIDs: [1595, 1595, 1595, 1595, 1595, 1595, 1595, 1595, 1595, 1595]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [91, 58, 83, 90, 72, 90, 70, 46, 98, 84]
image sizes: [(618, 193), (618, 193), (618, 193), (618, 193), (618, 193), (618, 193), (618, 193), (618, 193), (618, 193), (618, 193)]
input_ids shape: torch.Size([10, 222])
attention_mask shape: torch.Size([10, 222])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 222


===== Batch 634 =====
QIDs: [756, 756, 756, 756, 756, 756, 756, 756, 756, 756]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [325, 168, 320, 360, 320, 327, 289, 113, 351, 336]
image sizes: [(1079, 593), (1079, 593), (1079, 593), (1079, 593), (1079, 593), (1079, 593), (1079, 593), (1079, 593), (1079, 593), (1079, 593)]
input_ids shape: torch.Size([10, 1017])
attention_mask shape: torch.Size([10, 1017])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1017


===== Batch 635 =====
QIDs: [1073, 1073, 1073, 1073, 1073, 1073, 1073, 1073, 1073, 1073]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1157, 528, 1135, 1137, 933, 1109, 995, 347, 1185, 1146]
image sizes: [(1444, 590), (1444, 590), (1444, 590), (1444, 590), (1444, 590), (1444, 590), (1444, 590), (1444, 590), (1444, 590), (1444, 590)]
input_ids shape: torch.Size([10, 1688])
attention_mask shape: torch.Size([10, 1688])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1688


===== Batch 636 =====
QIDs: [1704, 1704, 1704, 1704, 1704, 1704, 1704, 1704, 1704, 1704]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [338, 209, 334, 335, 307, 311, 305, 177, 360, 340]
image sizes: [(548, 365), (548, 365), (548, 365), (548, 365), (548, 365), (548, 365), (548, 365), (548, 365), (548, 365), (548, 365)]
input_ids shape: torch.Size([10, 493])
attention_mask shape: torch.Size([10, 493])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 493


===== Batch 637 =====
QIDs: [1107, 1107, 1107, 1107, 1107, 1107, 1107, 1107, 1107, 1107]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [532, 259, 506, 533, 414, 491, 441, 200, 546, 510]
image sizes: [(675, 144), (675, 144), (675, 144), (675, 144), (675, 144), (675, 144), (675, 144), (675, 144), (675, 144), (675, 144)]
input_ids shape: torch.Size([10, 418])
attention_mask shape: torch.Size([10, 418])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 418


===== Batch 638 =====
QIDs: [1582, 1582, 1582, 1582, 1582, 1582, 1582, 1582, 1582, 1582]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [433, 400, 443, 436, 429, 444, 433, 378, 447, 446]
image sizes: [(1290, 534), (1290, 534), (1290, 534), (1290, 534), (1290, 534), (1290, 534), (1290, 534), (1290, 534), (1290, 534), (1290, 534)]
input_ids shape: torch.Size([10, 1163])
attention_mask shape: torch.Size([10, 1163])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1163


===== Batch 639 =====
QIDs: [339, 339, 339, 339, 339, 339, 339, 339, 339, 339]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [526, 296, 546, 583, 465, 551, 502, 205, 606, 578]
image sizes: [(310, 219), (310, 219), (310, 219), (310, 219), (310, 219), (310, 219), (310, 219), (310, 219), (310, 219), (310, 219)]
input_ids shape: torch.Size([10, 407])
attention_mask shape: torch.Size([10, 407])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 407


===== Batch 640 =====
QIDs: [1544, 1544, 1544, 1544, 1544, 1544, 1544, 1544, 1544, 1544]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [405, 307, 413, 408, 371, 385, 355, 280, 400, 392]
image sizes: [(463, 508), (463, 508), (463, 508), (463, 508), (463, 508), (463, 508), (463, 508), (463, 508), (463, 508), (463, 508)]
input_ids shape: torch.Size([10, 606])
attention_mask shape: torch.Size([10, 606])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 606


===== Batch 641 =====
QIDs: [1669, 1669, 1669, 1669, 1669, 1669, 1669, 1669, 1669, 1669]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [212, 143, 219, 204, 185, 208, 205, 123, 196, 209]
image sizes: [(422, 188), (422, 188), (422, 188), (422, 188), (422, 188), (422, 188), (422, 188), (422, 188), (422, 188), (422, 188)]
input_ids shape: torch.Size([10, 270])
attention_mask shape: torch.Size([10, 270])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 270


===== Batch 642 =====
QIDs: [1331, 1331, 1331, 1331, 1331, 1331, 1331, 1331, 1331, 1331]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [187, 131, 195, 180, 172, 190, 181, 97, 202, 167]
image sizes: [(177, 146), (177, 146), (177, 146), (177, 146), (177, 146), (177, 146), (177, 146), (177, 146), (177, 146), (177, 146)]
input_ids shape: torch.Size([10, 182])
attention_mask shape: torch.Size([10, 182])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 182


===== Batch 643 =====
QIDs: [1614, 1614, 1614, 1614, 1614, 1614, 1614, 1614, 1614, 1614]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [296, 186, 256, 285, 287, 303, 272, 128, 318, 343]
image sizes: [(666, 284), (666, 284), (666, 284), (666, 284), (666, 284), (666, 284), (666, 284), (666, 284), (666, 284), (666, 284)]
input_ids shape: torch.Size([10, 405])
attention_mask shape: torch.Size([10, 405])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 405


===== Batch 644 =====
QIDs: [1678, 1678, 1678, 1678, 1678, 1678, 1678, 1678, 1678, 1678]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [327, 146, 305, 314, 242, 311, 324, 115, 375, 303]
image sizes: [(440, 410), (440, 410), (440, 410), (440, 410), (440, 410), (440, 410), (440, 410), (440, 410), (440, 410), (440, 410)]
input_ids shape: torch.Size([10, 435])
attention_mask shape: torch.Size([10, 435])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 435


===== Batch 645 =====
QIDs: [913, 913, 913, 913, 913, 913, 913, 913, 913, 913]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [417, 375, 414, 429, 400, 413, 394, 360, 429, 417]
image sizes: [(1103, 409), (1103, 409), (1103, 409), (1103, 409), (1103, 409), (1103, 409), (1103, 409), (1103, 409), (1103, 409), (1103, 409)]
input_ids shape: torch.Size([10, 886])
attention_mask shape: torch.Size([10, 886])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 886


===== Batch 646 =====
QIDs: [1337, 1337, 1337, 1337, 1337, 1337, 1337, 1337, 1337, 1337]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [427, 326, 404, 413, 945, 409, 371, 292, 427, 395]
image sizes: [(232, 157), (232, 157), (232, 157), (232, 157), (232, 157), (232, 157), (232, 157), (232, 157), (232, 157), (232, 157)]
input_ids shape: torch.Size([10, 574])
attention_mask shape: torch.Size([10, 574])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 574


===== Batch 647 =====
QIDs: [516, 516, 516, 516, 516, 516, 516, 516, 516, 516]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [428, 396, 420, 445, 418, 427, 421, 378, 428, 424]
image sizes: [(408, 164), (408, 164), (408, 164), (408, 164), (408, 164), (408, 164), (408, 164), (408, 164), (408, 164), (408, 164)]
input_ids shape: torch.Size([10, 386])
attention_mask shape: torch.Size([10, 386])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 386


===== Batch 648 =====
QIDs: [1298, 1298, 1298, 1298, 1298, 1298, 1298, 1298, 1298, 1298]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [164, 119, 148, 160, 146, 169, 151, 121, 145, 170]
image sizes: [(643, 166), (643, 166), (643, 166), (643, 166), (643, 166), (643, 166), (643, 166), (643, 166), (643, 166), (643, 166)]
input_ids shape: torch.Size([10, 259])
attention_mask shape: torch.Size([10, 259])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 259


===== Batch 649 =====
QIDs: [802, 802, 802, 802, 802, 802, 802, 802, 802, 802]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [395, 217, 387, 374, 380, 379, 348, 166, 414, 425]
image sizes: [(811, 442), (811, 442), (811, 442), (811, 442), (811, 442), (811, 442), (811, 442), (811, 442), (811, 442), (811, 442)]
input_ids shape: torch.Size([10, 705])
attention_mask shape: torch.Size([10, 705])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 705


===== Batch 650 =====
QIDs: [1587, 1587, 1587, 1587, 1587, 1587, 1587, 1587, 1587, 1587]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [115, 62, 90, 110, 95, 94, 89, 49, 95, 92]
image sizes: [(292, 212), (292, 212), (292, 212), (292, 212), (292, 212), (292, 212), (292, 212), (292, 212), (292, 212), (292, 212)]
input_ids shape: torch.Size([10, 157])
attention_mask shape: torch.Size([10, 157])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 157


===== Batch 651 =====
QIDs: [797, 797, 797, 797, 797, 797, 797, 797, 797, 797]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [351, 226, 319, 372, 294, 324, 301, 181, 341, 366]
image sizes: [(1264, 321), (1264, 321), (1264, 321), (1264, 321), (1264, 321), (1264, 321), (1264, 321), (1264, 321), (1264, 321), (1264, 321)]
input_ids shape: torch.Size([10, 721])
attention_mask shape: torch.Size([10, 721])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 721


===== Batch 652 =====
QIDs: [1007, 1007, 1007, 1007, 1007, 1007, 1007, 1007, 1007, 1007]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [227, 159, 223, 243, 198, 215, 239, 93, 229, 231]
image sizes: [(576, 284), (576, 284), (576, 284), (576, 284), (576, 284), (576, 284), (576, 284), (576, 284), (576, 284), (576, 284)]
input_ids shape: torch.Size([10, 374])
attention_mask shape: torch.Size([10, 374])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 374


===== Batch 653 =====
QIDs: [37, 37, 37, 37, 37, 37, 37, 37, 37, 37]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [700, 362, 646, 698, 612, 620, 555, 258, 645, 689]
image sizes: [(1108, 495), (1108, 495), (1108, 495), (1108, 495), (1108, 495), (1108, 495), (1108, 495), (1108, 495), (1108, 495), (1108, 495)]
input_ids shape: torch.Size([10, 1060])
attention_mask shape: torch.Size([10, 1060])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1060


===== Batch 654 =====
QIDs: [1065, 1065, 1065, 1065, 1065, 1065, 1065, 1065, 1065, 1065]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [92, 79, 93, 94, 100, 90, 86, 77, 97, 100]
image sizes: [(974, 778), (974, 778), (974, 778), (974, 778), (974, 778), (974, 778), (974, 778), (974, 778), (974, 778), (974, 778)]
input_ids shape: torch.Size([10, 1076])
attention_mask shape: torch.Size([10, 1076])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1076


===== Batch 655 =====
QIDs: [1157, 1157, 1157, 1157, 1157, 1157, 1157, 1157, 1157, 1157]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [241, 196, 251, 230, 229, 246, 229, 168, 271, 237]
image sizes: [(742, 504), (742, 504), (742, 504), (742, 504), (742, 504), (742, 504), (742, 504), (742, 504), (742, 504), (742, 504)]
input_ids shape: torch.Size([10, 647])
attention_mask shape: torch.Size([10, 647])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 647


===== Batch 656 =====
QIDs: [1468, 1468, 1468, 1468, 1468, 1468, 1468, 1468, 1468, 1468]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [187, 92, 131, 197, 131, 188, 170, 110, 133, 141]
image sizes: [(460, 363), (460, 363), (460, 363), (460, 363), (460, 363), (460, 363), (460, 363), (460, 363), (460, 363), (460, 363)]
input_ids shape: torch.Size([10, 348])
attention_mask shape: torch.Size([10, 348])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 348


===== Batch 657 =====
QIDs: [17, 17, 17, 17, 17, 17, 17, 17, 17, 17]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [379, 276, 371, 386, 357, 370, 337, 256, 382, 389]
image sizes: [(1216, 286), (1216, 286), (1216, 286), (1216, 286), (1216, 286), (1216, 286), (1216, 286), (1216, 286), (1216, 286), (1216, 286)]
input_ids shape: torch.Size([10, 734])
attention_mask shape: torch.Size([10, 734])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 734


===== Batch 658 =====
QIDs: [1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [200, 155, 197, 205, 185, 193, 194, 150, 189, 213]
image sizes: [(522, 338), (522, 338), (522, 338), (522, 338), (522, 338), (522, 338), (522, 338), (522, 338), (522, 338), (522, 338)]
input_ids shape: torch.Size([10, 396])
attention_mask shape: torch.Size([10, 396])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 396


===== Batch 659 =====
QIDs: [1401, 1401, 1401, 1401, 1401, 1401, 1401, 1401, 1401, 1401]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [262, 191, 261, 267, 257, 260, 264, 147, 281, 273]
image sizes: [(955, 283), (955, 283), (955, 283), (955, 283), (955, 283), (955, 283), (955, 283), (955, 283), (955, 283), (955, 283)]
input_ids shape: torch.Size([10, 549])
attention_mask shape: torch.Size([10, 549])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 549


===== Batch 660 =====
QIDs: [1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153, 1153]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [420, 349, 436, 501, 419, 469, 408, 296, 493, 429]
image sizes: [(567, 264), (567, 264), (567, 264), (567, 264), (567, 264), (567, 264), (567, 264), (567, 264), (567, 264), (567, 264)]
input_ids shape: torch.Size([10, 508])
attention_mask shape: torch.Size([10, 508])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 508


===== Batch 661 =====
QIDs: [627, 627, 627, 627, 627, 627, 627, 627, 627, 627]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [421, 207, 469, 441, 389, 461, 418, 166, 482, 495]
image sizes: [(771, 779), (771, 779), (771, 779), (771, 779), (771, 779), (771, 779), (771, 779), (771, 779), (771, 779), (771, 779)]
input_ids shape: torch.Size([10, 1064])
attention_mask shape: torch.Size([10, 1064])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1064


===== Batch 662 =====
QIDs: [930, 930, 930, 930, 930, 930, 930, 930, 930, 930]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [77, 70, 92, 82, 74, 101, 98, 46, 95, 80]
image sizes: [(428, 487), (428, 487), (428, 487), (428, 487), (428, 487), (428, 487), (428, 487), (428, 487), (428, 487), (428, 487)]
input_ids shape: torch.Size([10, 336])
attention_mask shape: torch.Size([10, 336])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 336


===== Batch 663 =====
QIDs: [1323, 1323, 1323, 1323, 1323, 1323, 1323, 1323, 1323, 1323]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [294, 141, 303, 291, 310, 302, 323, 111, 326, 278]
image sizes: [(322, 480), (322, 480), (322, 480), (322, 480), (322, 480), (322, 480), (322, 480), (322, 480), (322, 480), (322, 480)]
input_ids shape: torch.Size([10, 432])
attention_mask shape: torch.Size([10, 432])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 432


===== Batch 664 =====
QIDs: [1161, 1161, 1161, 1161, 1161, 1161, 1161, 1161, 1161, 1161]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [350, 229, 369, 352, 284, 359, 299, 172, 378, 330]
image sizes: [(733, 237), (733, 237), (733, 237), (733, 237), (733, 237), (733, 237), (733, 237), (733, 237), (733, 237), (733, 237)]
input_ids shape: torch.Size([10, 437])
attention_mask shape: torch.Size([10, 437])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 437


===== Batch 665 =====
QIDs: [630, 630, 630, 630, 630, 630, 630, 630, 630, 630]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [575, 279, 539, 552, 469, 578, 508, 176, 599, 553]
image sizes: [(1246, 1059), (1246, 1059), (1246, 1059), (1246, 1059), (1246, 1059), (1246, 1059), (1246, 1059), (1246, 1059), (1246, 1059), (1246, 1059)]
input_ids shape: torch.Size([10, 1986])
attention_mask shape: torch.Size([10, 1986])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1986


===== Batch 666 =====
QIDs: [95, 95, 95, 95, 95, 95, 95, 95, 95, 95]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [339, 282, 333, 354, 308, 339, 332, 230, 324, 339]
image sizes: [(458, 209), (458, 209), (458, 209), (458, 209), (458, 209), (458, 209), (458, 209), (458, 209), (458, 209), (458, 209)]
input_ids shape: torch.Size([10, 391])
attention_mask shape: torch.Size([10, 391])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 391


===== Batch 667 =====
QIDs: [493, 493, 493, 493, 493, 493, 493, 493, 493, 493]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [568, 307, 526, 605, 374, 516, 455, 194, 498, 518]
image sizes: [(444, 316), (444, 316), (444, 316), (444, 316), (444, 316), (444, 316), (444, 316), (444, 316), (444, 316), (444, 316)]
input_ids shape: torch.Size([10, 494])
attention_mask shape: torch.Size([10, 494])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 494


===== Batch 668 =====
QIDs: [947, 947, 947, 947, 947, 947, 947, 947, 947, 947]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [181, 130, 161, 178, 155, 147, 174, 121, 169, 163]
image sizes: [(1278, 1568), (1278, 1568), (1278, 1568), (1278, 1568), (1278, 1568), (1278, 1568), (1278, 1568), (1278, 1568), (1278, 1568), (1278, 1568)]
input_ids shape: torch.Size([10, 2723])
attention_mask shape: torch.Size([10, 2723])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2723


===== Batch 669 =====
QIDs: [1382, 1382, 1382, 1382, 1382, 1382, 1382, 1382, 1382, 1382]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [645, 253, 674, 660, 528, 626, 793, 203, 716, 641]
image sizes: [(1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548)]
input_ids shape: torch.Size([10, 4190])
attention_mask shape: torch.Size([10, 4190])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 4190


===== Batch 670 =====
QIDs: [1387, 1387, 1387, 1387, 1387, 1387, 1387, 1387, 1387, 1387]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [262, 173, 251, 263, 257, 242, 218, 140, 241, 253]
image sizes: [(475, 154), (475, 154), (475, 154), (475, 154), (475, 154), (475, 154), (475, 154), (475, 154), (475, 154), (475, 154)]
input_ids shape: torch.Size([10, 284])
attention_mask shape: torch.Size([10, 284])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 284


===== Batch 671 =====
QIDs: [368, 368, 368, 368, 368, 368, 368, 368, 368, 368]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [118, 67, 119, 118, 117, 121, 108, 49, 133, 136]
image sizes: [(570, 556), (570, 556), (570, 556), (570, 556), (570, 556), (570, 556), (570, 556), (570, 556), (570, 556), (570, 556)]
input_ids shape: torch.Size([10, 484])
attention_mask shape: torch.Size([10, 484])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 484


===== Batch 672 =====
QIDs: [561, 561, 561, 561, 561, 561, 561, 561, 561, 561]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [669, 565, 682, 695, 645, 672, 659, 540, 698, 646]
image sizes: [(560, 519), (560, 519), (560, 519), (560, 519), (560, 519), (560, 519), (560, 519), (560, 519), (560, 519), (560, 519)]
input_ids shape: torch.Size([10, 823])
attention_mask shape: torch.Size([10, 823])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 823


===== Batch 673 =====
QIDs: [1402, 1402, 1402, 1402, 1402, 1402, 1402, 1402, 1402, 1402]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [218, 155, 218, 228, 221, 213, 203, 122, 268, 228]
image sizes: [(497, 474), (497, 474), (497, 474), (497, 474), (497, 474), (497, 474), (497, 474), (497, 474), (497, 474), (497, 474)]
input_ids shape: torch.Size([10, 480])
attention_mask shape: torch.Size([10, 480])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 480


===== Batch 674 =====
QIDs: [70, 70, 70, 70, 70, 70, 70, 70, 70, 70]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [393, 220, 382, 406, 302, 381, 355, 177, 449, 341]
image sizes: [(664, 154), (664, 154), (664, 154), (664, 154), (664, 154), (664, 154), (664, 154), (664, 154), (664, 154), (664, 154)]
input_ids shape: torch.Size([10, 356])
attention_mask shape: torch.Size([10, 356])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 356


===== Batch 675 =====
QIDs: [1281, 1281, 1281, 1281, 1281, 1281, 1281, 1281, 1281, 1281]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [204, 108, 201, 194, 215, 202, 199, 95, 217, 191]
image sizes: [(998, 882), (998, 882), (998, 882), (998, 882), (998, 882), (998, 882), (998, 882), (998, 882), (998, 882), (998, 882)]
input_ids shape: torch.Size([10, 1310])
attention_mask shape: torch.Size([10, 1310])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1310


===== Batch 676 =====
QIDs: [1514, 1514, 1514, 1514, 1514, 1514, 1514, 1514, 1514, 1514]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [409, 272, 448, 456, 405, 439, 388, 196, 456, 400]
image sizes: [(1231, 486), (1231, 486), (1231, 486), (1231, 486), (1231, 486), (1231, 486), (1231, 486), (1231, 486), (1231, 486), (1231, 486)]
input_ids shape: torch.Size([10, 1028])
attention_mask shape: torch.Size([10, 1028])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1028


===== Batch 677 =====
QIDs: [484, 484, 484, 484, 484, 484, 484, 484, 484, 484]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1194, 743, 1274, 1152, 1147, 1198, 1072, 632, 1374, 1118]
image sizes: [(703, 115), (703, 115), (703, 115), (703, 115), (703, 115), (703, 115), (703, 115), (703, 115), (703, 115), (703, 115)]
input_ids shape: torch.Size([10, 904])
attention_mask shape: torch.Size([10, 904])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 904


===== Batch 678 =====
QIDs: [727, 727, 727, 727, 727, 727, 727, 727, 727, 727]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [199, 118, 199, 221, 169, 194, 165, 94, 198, 215]
image sizes: [(567, 353), (567, 353), (567, 353), (567, 353), (567, 353), (567, 353), (567, 353), (567, 353), (567, 353), (567, 353)]
input_ids shape: torch.Size([10, 390])
attention_mask shape: torch.Size([10, 390])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 390


===== Batch 679 =====
QIDs: [1215, 1215, 1215, 1215, 1215, 1215, 1215, 1215, 1215, 1215]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [977, 869, 951, 946, 951, 937, 913, 859, 945, 958]
image sizes: [(238, 217), (238, 217), (238, 217), (238, 217), (238, 217), (238, 217), (238, 217), (238, 217), (238, 217), (238, 217)]
input_ids shape: torch.Size([10, 647])
attention_mask shape: torch.Size([10, 647])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 647


===== Batch 680 =====
QIDs: [1628, 1628, 1628, 1628, 1628, 1628, 1628, 1628, 1628, 1628]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [157, 96, 144, 129, 147, 144, 120, 83, 145, 119]
image sizes: [(1054, 156), (1054, 156), (1054, 156), (1054, 156), (1054, 156), (1054, 156), (1054, 156), (1054, 156), (1054, 156), (1054, 156)]
input_ids shape: torch.Size([10, 343])
attention_mask shape: torch.Size([10, 343])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 343


===== Batch 681 =====
QIDs: [1317, 1317, 1317, 1317, 1317, 1317, 1317, 1317, 1317, 1317]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [543, 272, 507, 537, 505, 538, 526, 189, 566, 506]
image sizes: [(316, 482), (316, 482), (316, 482), (316, 482), (316, 482), (316, 482), (316, 482), (316, 482), (316, 482), (316, 482)]
input_ids shape: torch.Size([10, 508])
attention_mask shape: torch.Size([10, 508])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 508


===== Batch 682 =====
QIDs: [428, 428, 428, 428, 428, 428, 428, 428, 428, 428]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [210, 114, 215, 206, 168, 208, 214, 92, 249, 186]
image sizes: [(538, 526), (538, 526), (538, 526), (538, 526), (538, 526), (538, 526), (538, 526), (538, 526), (538, 526), (538, 526)]
input_ids shape: torch.Size([10, 500])
attention_mask shape: torch.Size([10, 500])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 500


===== Batch 683 =====
QIDs: [825, 825, 825, 825, 825, 825, 825, 825, 825, 825]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [366, 307, 359, 358, 385, 351, 338, 286, 375, 349]
image sizes: [(595, 346), (595, 346), (595, 346), (595, 346), (595, 346), (595, 346), (595, 346), (595, 346), (595, 346), (595, 346)]
input_ids shape: torch.Size([10, 529])
attention_mask shape: torch.Size([10, 529])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 529


===== Batch 684 =====
QIDs: [370, 370, 370, 370, 370, 370, 370, 370, 370, 370]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [285, 177, 272, 310, 273, 278, 272, 124, 284, 260]
image sizes: [(741, 294), (741, 294), (741, 294), (741, 294), (741, 294), (741, 294), (741, 294), (741, 294), (741, 294), (741, 294)]
input_ids shape: torch.Size([10, 442])
attention_mask shape: torch.Size([10, 442])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 442


===== Batch 685 =====
QIDs: [1480, 1480, 1480, 1480, 1480, 1480, 1480, 1480, 1480, 1480]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [289, 148, 289, 322, 253, 267, 232, 114, 294, 324]
image sizes: [(340, 291), (340, 291), (340, 291), (340, 291), (340, 291), (340, 291), (340, 291), (340, 291), (340, 291), (340, 291)]
input_ids shape: torch.Size([10, 303])
attention_mask shape: torch.Size([10, 303])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 303


===== Batch 686 =====
QIDs: [1695, 1695, 1695, 1695, 1695, 1695, 1695, 1695, 1695, 1695]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [448, 245, 420, 442, 381, 394, 377, 228, 452, 417]
image sizes: [(595, 116), (595, 116), (595, 116), (595, 116), (595, 116), (595, 116), (595, 116), (595, 116), (595, 116), (595, 116)]
input_ids shape: torch.Size([10, 385])
attention_mask shape: torch.Size([10, 385])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 385


===== Batch 687 =====
QIDs: [819, 819, 819, 819, 819, 819, 819, 819, 819, 819]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [863, 428, 877, 778, 610, 849, 786, 316, 1039, 884]
image sizes: [(1209, 193), (1209, 193), (1209, 193), (1209, 193), (1209, 193), (1209, 193), (1209, 193), (1209, 193), (1209, 193), (1209, 193)]
input_ids shape: torch.Size([10, 778])
attention_mask shape: torch.Size([10, 778])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 778


===== Batch 688 =====
QIDs: [635, 635, 635, 635, 635, 635, 635, 635, 635, 635]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [123, 85, 116, 126, 103, 123, 107, 70, 127, 116]
image sizes: [(1111, 571), (1111, 571), (1111, 571), (1111, 571), (1111, 571), (1111, 571), (1111, 571), (1111, 571), (1111, 571), (1111, 571)]
input_ids shape: torch.Size([10, 882])
attention_mask shape: torch.Size([10, 882])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 882


===== Batch 689 =====
QIDs: [266, 266, 266, 266, 266, 266, 266, 266, 266, 266]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [826, 395, 684, 888, 619, 721, 792, 319, 801, 772]
image sizes: [(296, 305), (296, 305), (296, 305), (296, 305), (296, 305), (296, 305), (296, 305), (296, 305), (296, 305), (296, 305)]
input_ids shape: torch.Size([10, 585])
attention_mask shape: torch.Size([10, 585])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 585


===== Batch 690 =====
QIDs: [1424, 1424, 1424, 1424, 1424, 1424, 1424, 1424, 1424, 1424]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [499, 325, 504, 552, 494, 497, 431, 243, 546, 495]
image sizes: [(388, 312), (388, 312), (388, 312), (388, 312), (388, 312), (388, 312), (388, 312), (388, 312), (388, 312), (388, 312)]
input_ids shape: torch.Size([10, 466])
attention_mask shape: torch.Size([10, 466])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 466


===== Batch 691 =====
QIDs: [1590, 1590, 1590, 1590, 1590, 1590, 1590, 1590, 1590, 1590]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [575, 404, 582, 610, 662, 571, 477, 307, 627, 561]
image sizes: [(333, 110), (333, 110), (333, 110), (333, 110), (333, 110), (333, 110), (333, 110), (333, 110), (333, 110), (333, 110)]
input_ids shape: torch.Size([10, 414])
attention_mask shape: torch.Size([10, 414])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 414


===== Batch 692 =====
QIDs: [234, 234, 234, 234, 234, 234, 234, 234, 234, 234]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [227, 104, 215, 206, 265, 203, 238, 93, 217, 234]
image sizes: [(1032, 1132), (1032, 1132), (1032, 1132), (1032, 1132), (1032, 1132), (1032, 1132), (1032, 1132), (1032, 1132), (1032, 1132), (1032, 1132)]
input_ids shape: torch.Size([10, 1643])
attention_mask shape: torch.Size([10, 1643])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1643


===== Batch 693 =====
QIDs: [1341, 1341, 1341, 1341, 1341, 1341, 1341, 1341, 1341, 1341]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [169, 123, 168, 154, 155, 152, 151, 107, 163, 186]
image sizes: [(1500, 1524), (1500, 1524), (1500, 1524), (1500, 1524), (1500, 1524), (1500, 1524), (1500, 1524), (1500, 1524), (1500, 1524), (1500, 1524)]
input_ids shape: torch.Size([10, 3036])
attention_mask shape: torch.Size([10, 3036])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 3036


===== Batch 694 =====
QIDs: [462, 462, 462, 462, 462, 462, 462, 462, 462, 462]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [272, 179, 260, 301, 228, 251, 218, 142, 263, 250]
image sizes: [(1243, 432), (1243, 432), (1243, 432), (1243, 432), (1243, 432), (1243, 432), (1243, 432), (1243, 432), (1243, 432), (1243, 432)]
input_ids shape: torch.Size([10, 853])
attention_mask shape: torch.Size([10, 853])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 853


===== Batch 695 =====
QIDs: [1033, 1033, 1033, 1033, 1033, 1033, 1033, 1033, 1033, 1033]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [392, 186, 344, 348, 345, 314, 324, 131, 371, 347]
image sizes: [(648, 254), (648, 254), (648, 254), (648, 254), (648, 254), (648, 254), (648, 254), (648, 254), (648, 254), (648, 254)]
input_ids shape: torch.Size([10, 418])
attention_mask shape: torch.Size([10, 418])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 418


===== Batch 696 =====
QIDs: [765, 765, 765, 765, 765, 765, 765, 765, 765, 765]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [349, 171, 274, 342, 317, 257, 274, 142, 351, 413]
image sizes: [(1049, 510), (1049, 510), (1049, 510), (1049, 510), (1049, 510), (1049, 510), (1049, 510), (1049, 510), (1049, 510), (1049, 510)]
input_ids shape: torch.Size([10, 871])
attention_mask shape: torch.Size([10, 871])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 871


===== Batch 697 =====
QIDs: [704, 704, 704, 704, 704, 704, 704, 704, 704, 704]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [945, 488, 870, 1069, 807, 879, 825, 277, 1005, 1004]
image sizes: [(862, 702), (862, 702), (862, 702), (862, 702), (862, 702), (862, 702), (862, 702), (862, 702), (862, 702), (862, 702)]
input_ids shape: torch.Size([10, 1263])
attention_mask shape: torch.Size([10, 1263])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1263


===== Batch 698 =====
QIDs: [181, 181, 181, 181, 181, 181, 181, 181, 181, 181]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [108, 60, 101, 102, 85, 109, 107, 43, 98, 125]
image sizes: [(330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260)]
input_ids shape: torch.Size([10, 190])
attention_mask shape: torch.Size([10, 190])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 190


===== Batch 699 =====
QIDs: [997, 997, 997, 997, 997, 997, 997, 997, 997, 997]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [255, 124, 237, 260, 223, 221, 233, 93, 268, 192]
image sizes: [(837, 416), (837, 416), (837, 416), (837, 416), (837, 416), (837, 416), (837, 416), (837, 416), (837, 416), (837, 416)]
input_ids shape: torch.Size([10, 606])
attention_mask shape: torch.Size([10, 606])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 606


===== Batch 700 =====
QIDs: [801, 801, 801, 801, 801, 801, 801, 801, 801, 801]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [481, 287, 480, 550, 477, 468, 435, 231, 481, 484]
image sizes: [(1261, 360), (1261, 360), (1261, 360), (1261, 360), (1261, 360), (1261, 360), (1261, 360), (1261, 360), (1261, 360), (1261, 360)]
input_ids shape: torch.Size([10, 898])
attention_mask shape: torch.Size([10, 898])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 898


===== Batch 701 =====
QIDs: [761, 761, 761, 761, 761, 761, 761, 761, 761, 761]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [765, 398, 752, 888, 617, 763, 700, 260, 877, 819]
image sizes: [(483, 851), (483, 851), (483, 851), (483, 851), (483, 851), (483, 851), (483, 851), (483, 851), (483, 851), (483, 851)]
input_ids shape: torch.Size([10, 938])
attention_mask shape: torch.Size([10, 938])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 938


===== Batch 702 =====
QIDs: [549, 549, 549, 549, 549, 549, 549, 549, 549, 549]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [248, 211, 242, 264, 239, 231, 208, 186, 226, 245]
image sizes: [(612, 259), (612, 259), (612, 259), (612, 259), (612, 259), (612, 259), (612, 259), (612, 259), (612, 259), (612, 259)]
input_ids shape: torch.Size([10, 403])
attention_mask shape: torch.Size([10, 403])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 403


===== Batch 703 =====
QIDs: [67, 67, 67, 67, 67, 67, 67, 67, 67, 67]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [267, 128, 260, 255, 232, 232, 202, 96, 255, 253]
image sizes: [(674, 489), (674, 489), (674, 489), (674, 489), (674, 489), (674, 489), (674, 489), (674, 489), (674, 489), (674, 489)]
input_ids shape: torch.Size([10, 574])
attention_mask shape: torch.Size([10, 574])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 574


===== Batch 704 =====
QIDs: [270, 270, 270, 270, 270, 270, 270, 270, 270, 270]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [136, 111, 144, 130, 120, 122, 125, 100, 136, 134]
image sizes: [(556, 130), (556, 130), (556, 130), (556, 130), (556, 130), (556, 130), (556, 130), (556, 130), (556, 130), (556, 130)]
input_ids shape: torch.Size([10, 203])
attention_mask shape: torch.Size([10, 203])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 203


===== Batch 705 =====
QIDs: [1058, 1058, 1058, 1058, 1058, 1058, 1058, 1058, 1058, 1058]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1545, 724, 1446, 1584, 1269, 1506, 1339, 478, 1682, 1656]
image sizes: [(784, 824), (784, 824), (784, 824), (784, 824), (784, 824), (784, 824), (784, 824), (784, 824), (784, 824), (784, 824)]
input_ids shape: torch.Size([10, 1576])
attention_mask shape: torch.Size([10, 1576])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1576


===== Batch 706 =====
QIDs: [556, 556, 556, 556, 556, 556, 556, 556, 556, 556]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [468, 353, 456, 475, 451, 440, 473, 320, 468, 465]
image sizes: [(623, 336), (623, 336), (623, 336), (623, 336), (623, 336), (623, 336), (623, 336), (623, 336), (623, 336), (623, 336)]
input_ids shape: torch.Size([10, 610])
attention_mask shape: torch.Size([10, 610])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 610


===== Batch 707 =====
QIDs: [20, 20, 20, 20, 20, 20, 20, 20, 20, 20]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [255, 204, 266, 279, 235, 257, 240, 180, 249, 262]
image sizes: [(1234, 289), (1234, 289), (1234, 289), (1234, 289), (1234, 289), (1234, 289), (1234, 289), (1234, 289), (1234, 289), (1234, 289)]
input_ids shape: torch.Size([10, 652])
attention_mask shape: torch.Size([10, 652])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 652


===== Batch 708 =====
QIDs: [453, 453, 453, 453, 453, 453, 453, 453, 453, 453]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [569, 302, 601, 553, 480, 567, 528, 247, 610, 478]
image sizes: [(100, 175), (100, 175), (100, 175), (100, 175), (100, 175), (100, 175), (100, 175), (100, 175), (100, 175), (100, 175)]
input_ids shape: torch.Size([10, 382])
attention_mask shape: torch.Size([10, 382])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 382


===== Batch 709 =====
QIDs: [935, 935, 935, 935, 935, 935, 935, 935, 935, 935]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [140, 91, 131, 120, 114, 129, 115, 81, 132, 143]
image sizes: [(1613, 1284), (1613, 1284), (1613, 1284), (1613, 1284), (1613, 1284), (1613, 1284), (1613, 1284), (1613, 1284), (1613, 1284), (1613, 1284)]
input_ids shape: torch.Size([10, 2777])
attention_mask shape: torch.Size([10, 2777])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2777


===== Batch 710 =====
QIDs: [582, 582, 582, 582, 582, 582, 582, 582, 582, 582]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [322, 228, 319, 324, 275, 330, 354, 189, 348, 326]
image sizes: [(429, 159), (429, 159), (429, 159), (429, 159), (429, 159), (429, 159), (429, 159), (429, 159), (429, 159), (429, 159)]
input_ids shape: torch.Size([10, 377])
attention_mask shape: torch.Size([10, 377])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 377


===== Batch 711 =====
QIDs: [1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400, 1400]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [476, 289, 473, 492, 404, 417, 401, 172, 606, 496]
image sizes: [(442, 358), (442, 358), (442, 358), (442, 358), (442, 358), (442, 358), (442, 358), (442, 358), (442, 358), (442, 358)]
input_ids shape: torch.Size([10, 474])
attention_mask shape: torch.Size([10, 474])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 474


===== Batch 712 =====
QIDs: [275, 275, 275, 275, 275, 275, 275, 275, 275, 275]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [290, 130, 276, 280, 248, 248, 295, 96, 287, 256]
image sizes: [(1084, 600), (1084, 600), (1084, 600), (1084, 600), (1084, 600), (1084, 600), (1084, 600), (1084, 600), (1084, 600), (1084, 600)]
input_ids shape: torch.Size([10, 1012])
attention_mask shape: torch.Size([10, 1012])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1012


===== Batch 713 =====
QIDs: [1610, 1610, 1610, 1610, 1610, 1610, 1610, 1610, 1610, 1610]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [82, 65, 98, 121, 92, 110, 94, 50, 96, 71]
image sizes: [(429, 459), (429, 459), (429, 459), (429, 459), (429, 459), (429, 459), (429, 459), (429, 459), (429, 459), (429, 459)]
input_ids shape: torch.Size([10, 322])
attention_mask shape: torch.Size([10, 322])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 322


===== Batch 714 =====
QIDs: [49, 49, 49, 49, 49, 49, 49, 49, 49, 49]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [404, 193, 421, 372, 330, 399, 365, 147, 423, 412]
image sizes: [(417, 791), (417, 791), (417, 791), (417, 791), (417, 791), (417, 791), (417, 791), (417, 791), (417, 791), (417, 791)]
input_ids shape: torch.Size([10, 672])
attention_mask shape: torch.Size([10, 672])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 672


===== Batch 715 =====
QIDs: [605, 605, 605, 605, 605, 605, 605, 605, 605, 605]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [613, 564, 605, 619, 583, 624, 602, 537, 603, 621]
image sizes: [(638, 349), (638, 349), (638, 349), (638, 349), (638, 349), (638, 349), (638, 349), (638, 349), (638, 349), (638, 349)]
input_ids shape: torch.Size([10, 736])
attention_mask shape: torch.Size([10, 736])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 736


===== Batch 716 =====
QIDs: [288, 288, 288, 288, 288, 288, 288, 288, 288, 288]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1667, 882, 1394, 1539, 1157, 1359, 1278, 539, 1537, 1409]
image sizes: [(188, 458), (188, 458), (188, 458), (188, 458), (188, 458), (188, 458), (188, 458), (188, 458), (188, 458), (188, 458)]
input_ids shape: torch.Size([10, 900])
attention_mask shape: torch.Size([10, 900])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 900


===== Batch 717 =====
QIDs: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [231, 164, 119, 215, 227, 224, 200, 135, 220, 231]
image sizes: [(859, 407), (859, 407), (859, 407), (859, 407), (859, 407), (859, 407), (859, 407), (859, 407), (859, 407), (859, 407)]
input_ids shape: torch.Size([10, 625])
attention_mask shape: torch.Size([10, 625])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 625


===== Batch 718 =====
QIDs: [1178, 1178, 1178, 1178, 1178, 1178, 1178, 1178, 1178, 1178]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [230, 132, 241, 251, 196, 221, 243, 106, 258, 266]
image sizes: [(2560, 2133), (2560, 2133), (2560, 2133), (2560, 2133), (2560, 2133), (2560, 2133), (2560, 2133), (2560, 2133), (2560, 2133), (2560, 2133)]
input_ids shape: torch.Size([10, 7080])
attention_mask shape: torch.Size([10, 7080])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 7080


===== Batch 719 =====
QIDs: [1705, 1705, 1705, 1705, 1705, 1705, 1705, 1705, 1705, 1705]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [242, 168, 226, 247, 203, 231, 215, 149, 241, 239]
image sizes: [(608, 119), (608, 119), (608, 119), (608, 119), (608, 119), (608, 119), (608, 119), (608, 119), (608, 119), (608, 119)]
input_ids shape: torch.Size([10, 245])
attention_mask shape: torch.Size([10, 245])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 245


===== Batch 720 =====
QIDs: [1293, 1293, 1293, 1293, 1293, 1293, 1293, 1293, 1293, 1293]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [481, 327, 468, 480, 412, 474, 462, 264, 512, 499]
image sizes: [(1342, 1220), (1342, 1220), (1342, 1220), (1342, 1220), (1342, 1220), (1342, 1220), (1342, 1220), (1342, 1220), (1342, 1220), (1342, 1220)]
input_ids shape: torch.Size([10, 2432])
attention_mask shape: torch.Size([10, 2432])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2432


===== Batch 721 =====
QIDs: [901, 901, 901, 901, 901, 901, 901, 901, 901, 901]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [59, 46, 61, 70, 55, 58, 60, 39, 60, 59]
image sizes: [(526, 212), (526, 212), (526, 212), (526, 212), (526, 212), (526, 212), (526, 212), (526, 212), (526, 212), (526, 212)]
input_ids shape: torch.Size([10, 209])
attention_mask shape: torch.Size([10, 209])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 209


===== Batch 722 =====
QIDs: [1187, 1187, 1187, 1187, 1187, 1187, 1187, 1187, 1187, 1187]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [329, 165, 340, 334, 317, 323, 326, 137, 406, 315]
image sizes: [(1852, 2560), (1852, 2560), (1852, 2560), (1852, 2560), (1852, 2560), (1852, 2560), (1852, 2560), (1852, 2560), (1852, 2560), (1852, 2560)]
input_ids shape: torch.Size([10, 6207])
attention_mask shape: torch.Size([10, 6207])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 6207


===== Batch 723 =====
QIDs: [458, 458, 458, 458, 458, 458, 458, 458, 458, 458]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [978, 476, 905, 952, 805, 907, 941, 315, 1000, 930]
image sizes: [(1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604)]
input_ids shape: torch.Size([10, 4365])
attention_mask shape: torch.Size([10, 4365])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 4365


===== Batch 724 =====
QIDs: [122, 122, 122, 122, 122, 122, 122, 122, 122, 122]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [885, 464, 864, 946, 718, 829, 718, 284, 868, 918]
image sizes: [(1092, 682), (1092, 682), (1092, 682), (1092, 682), (1092, 682), (1092, 682), (1092, 682), (1092, 682), (1092, 682), (1092, 682)]
input_ids shape: torch.Size([10, 1395])
attention_mask shape: torch.Size([10, 1395])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1395


===== Batch 725 =====
QIDs: [354, 354, 354, 354, 354, 354, 354, 354, 354, 354]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [257, 186, 212, 257, 234, 263, 236, 174, 219, 240]
image sizes: [(329, 46), (329, 46), (329, 46), (329, 46), (329, 46), (329, 46), (329, 46), (329, 46), (329, 46), (329, 46)]
input_ids shape: torch.Size([10, 177])
attention_mask shape: torch.Size([10, 177])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 177


===== Batch 726 =====
QIDs: [1405, 1405, 1405, 1405, 1405, 1405, 1405, 1405, 1405, 1405]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [131, 79, 124, 145, 133, 116, 134, 55, 152, 138]
image sizes: [(437, 319), (437, 319), (437, 319), (437, 319), (437, 319), (437, 319), (437, 319), (437, 319), (437, 319), (437, 319)]
input_ids shape: torch.Size([10, 284])
attention_mask shape: torch.Size([10, 284])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 284


===== Batch 727 =====
QIDs: [1518, 1518, 1518, 1518, 1518, 1518, 1518, 1518, 1518, 1518]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1027, 496, 926, 1063, 1136, 856, 872, 300, 1049, 1068]
image sizes: [(1225, 315), (1225, 315), (1225, 315), (1225, 315), (1225, 315), (1225, 315), (1225, 315), (1225, 315), (1225, 315), (1225, 315)]
input_ids shape: torch.Size([10, 994])
attention_mask shape: torch.Size([10, 994])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 994


===== Batch 728 =====
QIDs: [873, 873, 873, 873, 873, 873, 873, 873, 873, 873]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [401, 244, 386, 401, 303, 390, 323, 139, 373, 375]
image sizes: [(479, 354), (479, 354), (479, 354), (479, 354), (479, 354), (479, 354), (479, 354), (479, 354), (479, 354), (479, 354)]
input_ids shape: torch.Size([10, 447])
attention_mask shape: torch.Size([10, 447])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 447


===== Batch 729 =====
QIDs: [21, 21, 21, 21, 21, 21, 21, 21, 21, 21]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [487, 293, 587, 657, 593, 502, 419, 217, 602, 507]
image sizes: [(348, 193), (348, 193), (348, 193), (348, 193), (348, 193), (348, 193), (348, 193), (348, 193), (348, 193), (348, 193)]
input_ids shape: torch.Size([10, 437])
attention_mask shape: torch.Size([10, 437])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 437


===== Batch 730 =====
QIDs: [1506, 1506, 1506, 1506, 1506, 1506, 1506, 1506, 1506, 1506]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [678, 507, 782, 740, 812, 710, 648, 408, 773, 795]
image sizes: [(591, 307), (591, 307), (591, 307), (591, 307), (591, 307), (591, 307), (591, 307), (591, 307), (591, 307), (591, 307)]
input_ids shape: torch.Size([10, 735])
attention_mask shape: torch.Size([10, 735])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 735


===== Batch 731 =====
QIDs: [1684, 1684, 1684, 1684, 1684, 1684, 1684, 1684, 1684, 1684]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [239, 109, 229, 211, 193, 210, 221, 96, 231, 203]
image sizes: [(878, 686), (878, 686), (878, 686), (878, 686), (878, 686), (878, 686), (878, 686), (878, 686), (878, 686), (878, 686)]
input_ids shape: torch.Size([10, 917])
attention_mask shape: torch.Size([10, 917])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 917


===== Batch 732 =====
QIDs: [907, 907, 907, 907, 907, 907, 907, 907, 907, 907]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [92, 64, 97, 99, 89, 97, 90, 47, 98, 103]
image sizes: [(488, 179), (488, 179), (488, 179), (488, 179), (488, 179), (488, 179), (488, 179), (488, 179), (488, 179), (488, 179)]
input_ids shape: torch.Size([10, 174])
attention_mask shape: torch.Size([10, 174])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 174


===== Batch 733 =====
QIDs: [69, 69, 69, 69, 69, 69, 69, 69, 69, 69]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [278, 174, 248, 255, 321, 249, 258, 121, 260, 261]
image sizes: [(640, 480), (640, 480), (640, 480), (640, 480), (640, 480), (640, 480), (640, 480), (640, 480), (640, 480), (640, 480)]
input_ids shape: torch.Size([10, 568])
attention_mask shape: torch.Size([10, 568])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 568


===== Batch 734 =====
QIDs: [578, 578, 578, 578, 578, 578, 578, 578, 578, 578]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [334, 252, 309, 352, 353, 317, 322, 218, 309, 338]
image sizes: [(297, 405), (297, 405), (297, 405), (297, 405), (297, 405), (297, 405), (297, 405), (297, 405), (297, 405), (297, 405)]
input_ids shape: torch.Size([10, 404])
attention_mask shape: torch.Size([10, 404])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 404


===== Batch 735 =====
QIDs: [1180, 1180, 1180, 1180, 1180, 1180, 1180, 1180, 1180, 1180]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [195, 121, 186, 201, 164, 180, 203, 101, 218, 225]
image sizes: [(1708, 2560), (1708, 2560), (1708, 2560), (1708, 2560), (1708, 2560), (1708, 2560), (1708, 2560), (1708, 2560), (1708, 2560), (1708, 2560)]
input_ids shape: torch.Size([10, 5706])
attention_mask shape: torch.Size([10, 5706])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 5706


===== Batch 736 =====
QIDs: [964, 964, 964, 964, 964, 964, 964, 964, 964, 964]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [354, 185, 317, 337, 315, 327, 345, 140, 324, 369]
image sizes: [(483, 433), (483, 433), (483, 433), (483, 433), (483, 433), (483, 433), (483, 433), (483, 433), (483, 433), (483, 433)]
input_ids shape: torch.Size([10, 530])
attention_mask shape: torch.Size([10, 530])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 530


===== Batch 737 =====
QIDs: [517, 517, 517, 517, 517, 517, 517, 517, 517, 517]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [471, 421, 469, 474, 456, 465, 452, 405, 468, 468]
image sizes: [(606, 238), (606, 238), (606, 238), (606, 238), (606, 238), (606, 238), (606, 238), (606, 238), (606, 238), (606, 238)]
input_ids shape: torch.Size([10, 464])
attention_mask shape: torch.Size([10, 464])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 464


===== Batch 738 =====
QIDs: [1008, 1008, 1008, 1008, 1008, 1008, 1008, 1008, 1008, 1008]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [521, 312, 473, 505, 420, 479, 447, 234, 489, 531]
image sizes: [(372, 270), (372, 270), (372, 270), (372, 270), (372, 270), (372, 270), (372, 270), (372, 270), (372, 270), (372, 270)]
input_ids shape: torch.Size([10, 440])
attention_mask shape: torch.Size([10, 440])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 440


===== Batch 739 =====
QIDs: [365, 365, 365, 365, 365, 365, 365, 365, 365, 365]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [402, 358, 409, 411, 415, 414, 405, 323, 400, 400]
image sizes: [(168, 156), (168, 156), (168, 156), (168, 156), (168, 156), (168, 156), (168, 156), (168, 156), (168, 156), (168, 156)]
input_ids shape: torch.Size([10, 331])
attention_mask shape: torch.Size([10, 331])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 331


===== Batch 740 =====
QIDs: [1135, 1135, 1135, 1135, 1135, 1135, 1135, 1135, 1135, 1135]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [447, 209, 415, 452, 336, 415, 343, 153, 437, 474]
image sizes: [(1024, 689), (1024, 689), (1024, 689), (1024, 689), (1024, 689), (1024, 689), (1024, 689), (1024, 689), (1024, 689), (1024, 689)]
input_ids shape: torch.Size([10, 1163])
attention_mask shape: torch.Size([10, 1163])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1163


===== Batch 741 =====
QIDs: [218, 218, 218, 218, 218, 218, 218, 218, 218, 218]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [236, 101, 223, 217, 202, 226, 210, 90, 253, 196]
image sizes: [(1095, 1113), (1095, 1113), (1095, 1113), (1095, 1113), (1095, 1113), (1095, 1113), (1095, 1113), (1095, 1113), (1095, 1113), (1095, 1113)]
input_ids shape: torch.Size([10, 1710])
attention_mask shape: torch.Size([10, 1710])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1710


===== Batch 742 =====
QIDs: [315, 315, 315, 315, 315, 315, 315, 315, 315, 315]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [273, 141, 273, 275, 225, 261, 274, 112, 277, 270]
image sizes: [(617, 428), (617, 428), (617, 428), (617, 428), (617, 428), (617, 428), (617, 428), (617, 428), (617, 428), (617, 428)]
input_ids shape: torch.Size([10, 518])
attention_mask shape: torch.Size([10, 518])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 518


===== Batch 743 =====
QIDs: [1329, 1329, 1329, 1329, 1329, 1329, 1329, 1329, 1329, 1329]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [541, 258, 489, 537, 415, 490, 550, 170, 588, 608]
image sizes: [(525, 776), (525, 776), (525, 776), (525, 776), (525, 776), (525, 776), (525, 776), (525, 776), (525, 776), (525, 776)]
input_ids shape: torch.Size([10, 852])
attention_mask shape: torch.Size([10, 852])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 852


===== Batch 744 =====
QIDs: [1710, 1710, 1710, 1710, 1710, 1710, 1710, 1710, 1710, 1710]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [370, 230, 376, 394, 352, 378, 356, 177, 395, 410]
image sizes: [(802, 143), (802, 143), (802, 143), (802, 143), (802, 143), (802, 143), (802, 143), (802, 143), (802, 143), (802, 143)]
input_ids shape: torch.Size([10, 415])
attention_mask shape: torch.Size([10, 415])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 415


===== Batch 745 =====
QIDs: [272, 272, 272, 272, 272, 272, 272, 272, 272, 272]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [129, 84, 119, 122, 123, 121, 112, 73, 134, 113]
image sizes: [(141, 66), (141, 66), (141, 66), (141, 66), (141, 66), (141, 66), (141, 66), (141, 66), (141, 66), (141, 66)]
input_ids shape: torch.Size([10, 115])
attention_mask shape: torch.Size([10, 115])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 115


===== Batch 746 =====
QIDs: [598, 598, 598, 598, 598, 598, 598, 598, 598, 598]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [488, 294, 481, 451, 411, 488, 425, 249, 521, 477]
image sizes: [(415, 148), (415, 148), (415, 148), (415, 148), (415, 148), (415, 148), (415, 148), (415, 148), (415, 148), (415, 148)]
input_ids shape: torch.Size([10, 375])
attention_mask shape: torch.Size([10, 375])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 375


===== Batch 747 =====
QIDs: [1207, 1207, 1207, 1207, 1207, 1207, 1207, 1207, 1207, 1207]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1149, 780, 1310, 1292, 1214, 1423, 1046, 550, 1320, 1329]
image sizes: [(723, 249), (723, 249), (723, 249), (723, 249), (723, 249), (723, 249), (723, 249), (723, 249), (723, 249), (723, 249)]
input_ids shape: torch.Size([10, 1009])
attention_mask shape: torch.Size([10, 1009])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1009


===== Batch 748 =====
QIDs: [540, 540, 540, 540, 540, 540, 540, 540, 540, 540]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [211, 196, 207, 216, 204, 207, 186, 158, 192, 202]
image sizes: [(222, 184), (222, 184), (222, 184), (222, 184), (222, 184), (222, 184), (222, 184), (222, 184), (222, 184), (222, 184)]
input_ids shape: torch.Size([10, 210])
attention_mask shape: torch.Size([10, 210])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 210


===== Batch 749 =====
QIDs: [244, 244, 244, 244, 244, 244, 244, 244, 244, 244]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [728, 372, 708, 738, 628, 693, 599, 245, 800, 759]
image sizes: [(1864, 778), (1864, 778), (1864, 778), (1864, 778), (1864, 778), (1864, 778), (1864, 778), (1864, 778), (1864, 778), (1864, 778)]
input_ids shape: torch.Size([10, 2264])
attention_mask shape: torch.Size([10, 2264])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2264


===== Batch 750 =====
QIDs: [213, 213, 213, 213, 213, 213, 213, 213, 213, 213]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [248, 105, 254, 245, 251, 239, 226, 96, 270, 239]
image sizes: [(900, 792), (900, 792), (900, 792), (900, 792), (900, 792), (900, 792), (900, 792), (900, 792), (900, 792), (900, 792)]
input_ids shape: torch.Size([10, 1066])
attention_mask shape: torch.Size([10, 1066])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1066


===== Batch 751 =====
QIDs: [1650, 1650, 1650, 1650, 1650, 1650, 1650, 1650, 1650, 1650]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [112, 66, 99, 104, 90, 109, 97, 48, 113, 103]
image sizes: [(357, 282), (357, 282), (357, 282), (357, 282), (357, 282), (357, 282), (357, 282), (357, 282), (357, 282), (357, 282)]
input_ids shape: torch.Size([10, 207])
attention_mask shape: torch.Size([10, 207])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 207


===== Batch 752 =====
QIDs: [41, 41, 41, 41, 41, 41, 41, 41, 41, 41]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [224, 173, 209, 216, 164, 202, 197, 107, 269, 231]
image sizes: [(2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536)]
input_ids shape: torch.Size([10, 4172])
attention_mask shape: torch.Size([10, 4172])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 4172


===== Batch 753 =====
QIDs: [1574, 1574, 1574, 1574, 1574, 1574, 1574, 1574, 1574, 1574]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [312, 217, 296, 314, 262, 278, 272, 170, 290, 313]
image sizes: [(546, 196), (546, 196), (546, 196), (546, 196), (546, 196), (546, 196), (546, 196), (546, 196), (546, 196), (546, 196)]
input_ids shape: torch.Size([10, 346])
attention_mask shape: torch.Size([10, 346])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 346


===== Batch 754 =====
QIDs: [833, 833, 833, 833, 833, 833, 833, 833, 833, 833]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [283, 120, 294, 252, 217, 284, 269, 97, 310, 243]
image sizes: [(160, 250), (160, 250), (160, 250), (160, 250), (160, 250), (160, 250), (160, 250), (160, 250), (160, 250), (160, 250)]
input_ids shape: torch.Size([10, 238])
attention_mask shape: torch.Size([10, 238])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 238


===== Batch 755 =====
QIDs: [253, 253, 253, 253, 253, 253, 253, 253, 253, 253]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [110, 72, 104, 110, 103, 108, 106, 61, 123, 117]
image sizes: [(772, 584), (772, 584), (772, 584), (772, 584), (772, 584), (772, 584), (772, 584), (772, 584), (772, 584), (772, 584)]
input_ids shape: torch.Size([10, 664])
attention_mask shape: torch.Size([10, 664])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 664


===== Batch 756 =====
QIDs: [614, 614, 614, 614, 614, 614, 614, 614, 614, 614]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [552, 348, 568, 593, 528, 535, 491, 253, 586, 561]
image sizes: [(273, 250), (273, 250), (273, 250), (273, 250), (273, 250), (273, 250), (273, 250), (273, 250), (273, 250), (273, 250)]
input_ids shape: torch.Size([10, 430])
attention_mask shape: torch.Size([10, 430])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 430


===== Batch 757 =====
QIDs: [795, 795, 795, 795, 795, 795, 795, 795, 795, 795]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [346, 190, 331, 353, 301, 319, 280, 144, 347, 373]
image sizes: [(1227, 244), (1227, 244), (1227, 244), (1227, 244), (1227, 244), (1227, 244), (1227, 244), (1227, 244), (1227, 244), (1227, 244)]
input_ids shape: torch.Size([10, 595])
attention_mask shape: torch.Size([10, 595])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 595


===== Batch 758 =====
QIDs: [1104, 1104, 1104, 1104, 1104, 1104, 1104, 1104, 1104, 1104]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [2348, 1054, 2225, 2425, 2077, 2156, 2029, 692, 2441, 2251]
image sizes: [(963, 291), (963, 291), (963, 291), (963, 291), (963, 291), (963, 291), (963, 291), (963, 291), (963, 291), (963, 291)]
input_ids shape: torch.Size([10, 1524])
attention_mask shape: torch.Size([10, 1524])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1524


===== Batch 759 =====
QIDs: [857, 857, 857, 857, 857, 857, 857, 857, 857, 857]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [173, 148, 195, 178, 157, 181, 190, 115, 191, 198]
image sizes: [(1002, 721), (1002, 721), (1002, 721), (1002, 721), (1002, 721), (1002, 721), (1002, 721), (1002, 721), (1002, 721), (1002, 721)]
input_ids shape: torch.Size([10, 1090])
attention_mask shape: torch.Size([10, 1090])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1090


===== Batch 760 =====
QIDs: [1077, 1077, 1077, 1077, 1077, 1077, 1077, 1077, 1077, 1077]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [378, 232, 386, 398, 363, 381, 367, 183, 407, 442]
image sizes: [(802, 143), (802, 143), (802, 143), (802, 143), (802, 143), (802, 143), (802, 143), (802, 143), (802, 143), (802, 143)]
input_ids shape: torch.Size([10, 422])
attention_mask shape: torch.Size([10, 422])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 422


===== Batch 761 =====
QIDs: [1183, 1183, 1183, 1183, 1183, 1183, 1183, 1183, 1183, 1183]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [316, 167, 332, 310, 285, 282, 298, 114, 344, 304]
image sizes: [(2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536)]
input_ids shape: torch.Size([10, 4212])
attention_mask shape: torch.Size([10, 4212])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 4212


===== Batch 762 =====
QIDs: [808, 808, 808, 808, 808, 808, 808, 808, 808, 808]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [308, 204, 288, 321, 278, 305, 267, 176, 330, 310]
image sizes: [(1258, 366), (1258, 366), (1258, 366), (1258, 366), (1258, 366), (1258, 366), (1258, 366), (1258, 366), (1258, 366), (1258, 366)]
input_ids shape: torch.Size([10, 795])
attention_mask shape: torch.Size([10, 795])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 795


===== Batch 763 =====
QIDs: [831, 831, 831, 831, 831, 831, 831, 831, 831, 831]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [614, 506, 608, 623, 588, 597, 575, 509, 618, 594]
image sizes: [(789, 327), (789, 327), (789, 327), (789, 327), (789, 327), (789, 327), (789, 327), (789, 327), (789, 327), (789, 327)]
input_ids shape: torch.Size([10, 739])
attention_mask shape: torch.Size([10, 739])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 739


===== Batch 764 =====
QIDs: [44, 44, 44, 44, 44, 44, 44, 44, 44, 44]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [175, 115, 189, 177, 139, 165, 156, 91, 192, 173]
image sizes: [(553, 369), (553, 369), (553, 369), (553, 369), (553, 369), (553, 369), (553, 369), (553, 369), (553, 369), (553, 369)]
input_ids shape: torch.Size([10, 391])
attention_mask shape: torch.Size([10, 391])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 391


===== Batch 765 =====
QIDs: [210, 210, 210, 210, 210, 210, 210, 210, 210, 210]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [281, 137, 275, 274, 249, 289, 279, 101, 322, 274]
image sizes: [(590, 290), (590, 290), (590, 290), (590, 290), (590, 290), (590, 290), (590, 290), (590, 290), (590, 290), (590, 290)]
input_ids shape: torch.Size([10, 412])
attention_mask shape: torch.Size([10, 412])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 412


===== Batch 766 =====
QIDs: [849, 849, 849, 849, 849, 849, 849, 849, 849, 849]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [908, 678, 911, 967, 827, 931, 835, 593, 950, 894]
image sizes: [(532, 370), (532, 370), (532, 370), (532, 370), (532, 370), (532, 370), (532, 370), (532, 370), (532, 370), (532, 370)]
input_ids shape: torch.Size([10, 779])
attention_mask shape: torch.Size([10, 779])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 779


===== Batch 767 =====
QIDs: [184, 184, 184, 184, 184, 184, 184, 184, 184, 184]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [95, 46, 83, 86, 78, 81, 93, 34, 87, 60]
image sizes: [(330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260)]
input_ids shape: torch.Size([10, 183])
attention_mask shape: torch.Size([10, 183])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 183


===== Batch 768 =====
QIDs: [1510, 1510, 1510, 1510, 1510, 1510, 1510, 1510, 1510, 1510]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [319, 151, 300, 263, 242, 284, 284, 121, 319, 301]
image sizes: [(567, 448), (567, 448), (567, 448), (567, 448), (567, 448), (567, 448), (567, 448), (567, 448), (567, 448), (567, 448)]
input_ids shape: torch.Size([10, 503])
attention_mask shape: torch.Size([10, 503])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 503


===== Batch 769 =====
QIDs: [63, 63, 63, 63, 63, 63, 63, 63, 63, 63]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [360, 201, 352, 374, 346, 344, 311, 142, 383, 359]
image sizes: [(580, 536), (580, 536), (580, 536), (580, 536), (580, 536), (580, 536), (580, 536), (580, 536), (580, 536), (580, 536)]
input_ids shape: torch.Size([10, 612])
attention_mask shape: torch.Size([10, 612])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 612


===== Batch 770 =====
QIDs: [285, 285, 285, 285, 285, 285, 285, 285, 285, 285]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [120, 90, 126, 133, 107, 120, 119, 73, 128, 146]
image sizes: [(205, 70), (205, 70), (205, 70), (205, 70), (205, 70), (205, 70), (205, 70), (205, 70), (205, 70), (205, 70)]
input_ids shape: torch.Size([10, 100])
attention_mask shape: torch.Size([10, 100])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 100


===== Batch 771 =====
QIDs: [1096, 1096, 1096, 1096, 1096, 1096, 1096, 1096, 1096, 1096]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [746, 395, 687, 736, 682, 700, 615, 281, 735, 727]
image sizes: [(755, 195), (755, 195), (755, 195), (755, 195), (755, 195), (755, 195), (755, 195), (755, 195), (755, 195), (755, 195)]
input_ids shape: torch.Size([10, 577])
attention_mask shape: torch.Size([10, 577])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 577


===== Batch 772 =====
QIDs: [344, 344, 344, 344, 344, 344, 344, 344, 344, 344]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [155, 100, 130, 159, 125, 138, 129, 85, 151, 122]
image sizes: [(300, 300), (300, 300), (300, 300), (300, 300), (300, 300), (300, 300), (300, 300), (300, 300), (300, 300), (300, 300)]
input_ids shape: torch.Size([10, 220])
attention_mask shape: torch.Size([10, 220])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 220


===== Batch 773 =====
QIDs: [1379, 1379, 1379, 1379, 1379, 1379, 1379, 1379, 1379, 1379]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [390, 171, 375, 339, 329, 366, 412, 133, 402, 321]
image sizes: [(1472, 346), (1472, 346), (1472, 346), (1472, 346), (1472, 346), (1472, 346), (1472, 346), (1472, 346), (1472, 346), (1472, 346)]
input_ids shape: torch.Size([10, 918])
attention_mask shape: torch.Size([10, 918])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 918


===== Batch 774 =====
QIDs: [316, 316, 316, 316, 316, 316, 316, 316, 316, 316]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [369, 171, 379, 386, 394, 332, 405, 124, 402, 326]
image sizes: [(226, 240), (226, 240), (226, 240), (226, 240), (226, 240), (226, 240), (226, 240), (226, 240), (226, 240), (226, 240)]
input_ids shape: torch.Size([10, 333])
attention_mask shape: torch.Size([10, 333])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 333


===== Batch 775 =====
QIDs: [511, 511, 511, 511, 511, 511, 511, 511, 511, 511]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [532, 514, 534, 557, 542, 531, 519, 505, 535, 534]
image sizes: [(517, 152), (517, 152), (517, 152), (517, 152), (517, 152), (517, 152), (517, 152), (517, 152), (517, 152), (517, 152)]
input_ids shape: torch.Size([10, 450])
attention_mask shape: torch.Size([10, 450])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 450


===== Batch 776 =====
QIDs: [105, 105, 105, 105, 105, 105, 105, 105, 105, 105]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [443, 332, 447, 461, 397, 425, 407, 290, 477, 454]
image sizes: [(473, 301), (473, 301), (473, 301), (473, 301), (473, 301), (473, 301), (473, 301), (473, 301), (473, 301), (473, 301)]
input_ids shape: torch.Size([10, 495])
attention_mask shape: torch.Size([10, 495])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 495


===== Batch 777 =====
QIDs: [440, 440, 440, 440, 440, 440, 440, 440, 440, 440]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [323, 168, 323, 337, 302, 338, 337, 160, 333, 306]
image sizes: [(694, 486), (694, 486), (694, 486), (694, 486), (694, 486), (694, 486), (694, 486), (694, 486), (694, 486), (694, 486)]
input_ids shape: torch.Size([10, 631])
attention_mask shape: torch.Size([10, 631])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 631


===== Batch 778 =====
QIDs: [1584, 1584, 1584, 1584, 1584, 1584, 1584, 1584, 1584, 1584]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [574, 383, 578, 600, 524, 554, 510, 319, 604, 581]
image sizes: [(134, 157), (134, 157), (134, 157), (134, 157), (134, 157), (134, 157), (134, 157), (134, 157), (134, 157), (134, 157)]
input_ids shape: torch.Size([10, 439])
attention_mask shape: torch.Size([10, 439])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 439


===== Batch 779 =====
QIDs: [925, 925, 925, 925, 925, 925, 925, 925, 925, 925]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [617, 608, 620, 630, 610, 621, 613, 598, 624, 626]
image sizes: [(903, 678), (903, 678), (903, 678), (903, 678), (903, 678), (903, 678), (903, 678), (903, 678), (903, 678), (903, 678)]
input_ids shape: torch.Size([10, 1164])
attention_mask shape: torch.Size([10, 1164])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1164


===== Batch 780 =====
QIDs: [319, 319, 319, 319, 319, 319, 319, 319, 319, 319]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [609, 333, 623, 666, 663, 633, 597, 231, 726, 581]
image sizes: [(504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330)]
input_ids shape: torch.Size([10, 596])
attention_mask shape: torch.Size([10, 596])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 596


===== Batch 781 =====
QIDs: [1533, 1533, 1533, 1533, 1533, 1533, 1533, 1533, 1533, 1533]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [145, 108, 123, 140, 129, 125, 129, 94, 144, 140]
image sizes: [(279, 232), (279, 232), (279, 232), (279, 232), (279, 232), (279, 232), (279, 232), (279, 232), (279, 232), (279, 232)]
input_ids shape: torch.Size([10, 207])
attention_mask shape: torch.Size([10, 207])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 207


===== Batch 782 =====
QIDs: [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [464, 214, 412, 433, 326, 409, 343, 144, 398, 406]
image sizes: [(524, 148), (524, 148), (524, 148), (524, 148), (524, 148), (524, 148), (524, 148), (524, 148), (524, 148), (524, 148)]
input_ids shape: torch.Size([10, 313])
attention_mask shape: torch.Size([10, 313])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 313


===== Batch 783 =====
QIDs: [1697, 1697, 1697, 1697, 1697, 1697, 1697, 1697, 1697, 1697]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [388, 230, 391, 455, 359, 380, 367, 155, 373, 398]
image sizes: [(890, 322), (890, 322), (890, 322), (890, 322), (890, 322), (890, 322), (890, 322), (890, 322), (890, 322), (890, 322)]
input_ids shape: torch.Size([10, 641])
attention_mask shape: torch.Size([10, 641])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 641


===== Batch 784 =====
QIDs: [1682, 1682, 1682, 1682, 1682, 1682, 1682, 1682, 1682, 1682]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1312, 648, 1172, 1331, 1093, 1117, 1044, 383, 1295, 1349]
image sizes: [(640, 702), (640, 702), (640, 702), (640, 702), (640, 702), (640, 702), (640, 702), (640, 702), (640, 702), (640, 702)]
input_ids shape: torch.Size([10, 1184])
attention_mask shape: torch.Size([10, 1184])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1184


===== Batch 785 =====
QIDs: [990, 990, 990, 990, 990, 990, 990, 990, 990, 990]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [176, 98, 167, 161, 175, 176, 171, 67, 164, 178]
image sizes: [(572, 190), (572, 190), (572, 190), (572, 190), (572, 190), (572, 190), (572, 190), (572, 190), (572, 190), (572, 190)]
input_ids shape: torch.Size([10, 259])
attention_mask shape: torch.Size([10, 259])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 259


===== Batch 786 =====
QIDs: [1043, 1043, 1043, 1043, 1043, 1043, 1043, 1043, 1043, 1043]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [783, 463, 747, 698, 579, 675, 686, 257, 745, 754]
image sizes: [(578, 434), (578, 434), (578, 434), (578, 434), (578, 434), (578, 434), (578, 434), (578, 434), (578, 434), (578, 434)]
input_ids shape: torch.Size([10, 727])
attention_mask shape: torch.Size([10, 727])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 727


===== Batch 787 =====
QIDs: [839, 839, 839, 839, 839, 839, 839, 839, 839, 839]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [159, 90, 142, 153, 127, 129, 133, 60, 151, 138]
image sizes: [(240, 315), (240, 315), (240, 315), (240, 315), (240, 315), (240, 315), (240, 315), (240, 315), (240, 315), (240, 315)]
input_ids shape: torch.Size([10, 191])
attention_mask shape: torch.Size([10, 191])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 191


===== Batch 788 =====
QIDs: [898, 898, 898, 898, 898, 898, 898, 898, 898, 898]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [252, 181, 253, 266, 265, 251, 230, 153, 272, 256]
image sizes: [(205, 211), (205, 211), (205, 211), (205, 211), (205, 211), (205, 211), (205, 211), (205, 211), (205, 211), (205, 211)]
input_ids shape: torch.Size([10, 237])
attention_mask shape: torch.Size([10, 237])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 237


===== Batch 789 =====
QIDs: [425, 425, 425, 425, 425, 425, 425, 425, 425, 425]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [311, 140, 294, 311, 303, 312, 276, 107, 317, 246]
image sizes: [(1038, 518), (1038, 518), (1038, 518), (1038, 518), (1038, 518), (1038, 518), (1038, 518), (1038, 518), (1038, 518), (1038, 518)]
input_ids shape: torch.Size([10, 856])
attention_mask shape: torch.Size([10, 856])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 856


===== Batch 790 =====
QIDs: [1568, 1568, 1568, 1568, 1568, 1568, 1568, 1568, 1568, 1568]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [503, 390, 469, 475, 441, 476, 479, 338, 482, 529]
image sizes: [(1004, 537), (1004, 537), (1004, 537), (1004, 537), (1004, 537), (1004, 537), (1004, 537), (1004, 537), (1004, 537), (1004, 537)]
input_ids shape: torch.Size([10, 973])
attention_mask shape: torch.Size([10, 973])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 973


===== Batch 791 =====
QIDs: [1599, 1599, 1599, 1599, 1599, 1599, 1599, 1599, 1599, 1599]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [92, 65, 98, 92, 90, 99, 89, 49, 97, 115]
image sizes: [(537, 249), (537, 249), (537, 249), (537, 249), (537, 249), (537, 249), (537, 249), (537, 249), (537, 249), (537, 249)]
input_ids shape: torch.Size([10, 245])
attention_mask shape: torch.Size([10, 245])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 245


===== Batch 792 =====
QIDs: [68, 68, 68, 68, 68, 68, 68, 68, 68, 68]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [885, 442, 901, 878, 828, 927, 943, 316, 988, 902]
image sizes: [(2560, 1700), (2560, 1700), (2560, 1700), (2560, 1700), (2560, 1700), (2560, 1700), (2560, 1700), (2560, 1700), (2560, 1700), (2560, 1700)]
input_ids shape: torch.Size([10, 6133])
attention_mask shape: torch.Size([10, 6133])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 6133


===== Batch 793 =====
QIDs: [48, 48, 48, 48, 48, 48, 48, 48, 48, 48]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [742, 460, 745, 755, 589, 673, 687, 262, 841, 822]
image sizes: [(389, 400), (389, 400), (389, 400), (389, 400), (389, 400), (389, 400), (389, 400), (389, 400), (389, 400), (389, 400)]
input_ids shape: torch.Size([10, 644])
attention_mask shape: torch.Size([10, 644])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 644


===== Batch 794 =====
QIDs: [691, 691, 691, 691, 691, 691, 691, 691, 691, 691]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [719, 283, 608, 712, 559, 610, 587, 191, 640, 707]
image sizes: [(1038, 286), (1038, 286), (1038, 286), (1038, 286), (1038, 286), (1038, 286), (1038, 286), (1038, 286), (1038, 286), (1038, 286)]
input_ids shape: torch.Size([10, 728])
attention_mask shape: torch.Size([10, 728])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 728


===== Batch 795 =====
QIDs: [277, 277, 277, 277, 277, 277, 277, 277, 277, 277]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [231, 121, 202, 222, 190, 208, 172, 91, 228, 206]
image sizes: [(1062, 163), (1062, 163), (1062, 163), (1062, 163), (1062, 163), (1062, 163), (1062, 163), (1062, 163), (1062, 163), (1062, 163)]
input_ids shape: torch.Size([10, 362])
attention_mask shape: torch.Size([10, 362])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 362


===== Batch 796 =====
QIDs: [251, 251, 251, 251, 251, 251, 251, 251, 251, 251]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [585, 296, 554, 588, 509, 524, 479, 187, 587, 603]
image sizes: [(318, 292), (318, 292), (318, 292), (318, 292), (318, 292), (318, 292), (318, 292), (318, 292), (318, 292), (318, 292)]
input_ids shape: torch.Size([10, 419])
attention_mask shape: torch.Size([10, 419])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 419


===== Batch 797 =====
QIDs: [1346, 1346, 1346, 1346, 1346, 1346, 1346, 1346, 1346, 1346]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [209, 123, 224, 216, 225, 235, 182, 90, 215, 243]
image sizes: [(1094, 156), (1094, 156), (1094, 156), (1094, 156), (1094, 156), (1094, 156), (1094, 156), (1094, 156), (1094, 156), (1094, 156)]
input_ids shape: torch.Size([10, 376])
attention_mask shape: torch.Size([10, 376])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 376


===== Batch 798 =====
QIDs: [299, 299, 299, 299, 299, 299, 299, 299, 299, 299]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [299, 151, 280, 279, 283, 278, 312, 118, 313, 322]
image sizes: [(892, 1187), (892, 1187), (892, 1187), (892, 1187), (892, 1187), (892, 1187), (892, 1187), (892, 1187), (892, 1187), (892, 1187)]
input_ids shape: torch.Size([10, 1550])
attention_mask shape: torch.Size([10, 1550])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1550


===== Batch 799 =====
QIDs: [1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [155, 107, 192, 175, 178, 179, 194, 85, 160, 225]
image sizes: [(1322, 744), (1322, 744), (1322, 744), (1322, 744), (1322, 744), (1322, 744), (1322, 744), (1322, 744), (1322, 744), (1322, 744)]
input_ids shape: torch.Size([10, 1424])
attention_mask shape: torch.Size([10, 1424])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1424


===== Batch 800 =====
QIDs: [1540, 1540, 1540, 1540, 1540, 1540, 1540, 1540, 1540, 1540]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [425, 256, 420, 430, 372, 393, 363, 209, 475, 413]
image sizes: [(759, 603), (759, 603), (759, 603), (759, 603), (759, 603), (759, 603), (759, 603), (759, 603), (759, 603), (759, 603)]
input_ids shape: torch.Size([10, 852])
attention_mask shape: torch.Size([10, 852])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 852


===== Batch 801 =====
QIDs: [1716, 1716, 1716, 1716, 1716, 1716, 1716, 1716, 1716, 1716]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [268, 127, 241, 293, 183, 239, 237, 131, 301, 286]
image sizes: [(680, 628), (680, 628), (680, 628), (680, 628), (680, 628), (680, 628), (680, 628), (680, 628), (680, 628), (680, 628)]
input_ids shape: torch.Size([10, 686])
attention_mask shape: torch.Size([10, 686])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 686


===== Batch 802 =====
QIDs: [8, 8, 8, 8, 8, 8, 8, 8, 8, 8]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [351, 231, 372, 357, 304, 375, 346, 173, 404, 362]
image sizes: [(673, 103), (673, 103), (673, 103), (673, 103), (673, 103), (673, 103), (673, 103), (673, 103), (673, 103), (673, 103)]
input_ids shape: torch.Size([10, 344])
attention_mask shape: torch.Size([10, 344])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 344


===== Batch 803 =====
QIDs: [1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [413, 283, 419, 424, 452, 396, 403, 248, 437, 393]
image sizes: [(713, 1049), (713, 1049), (713, 1049), (713, 1049), (713, 1049), (713, 1049), (713, 1049), (713, 1049), (713, 1049), (713, 1049)]
input_ids shape: torch.Size([10, 1219])
attention_mask shape: torch.Size([10, 1219])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1219


===== Batch 804 =====
QIDs: [520, 520, 520, 520, 520, 520, 520, 520, 520, 520]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [270, 274, 275, 284, 302, 277, 270, 266, 277, 273]
image sizes: [(571, 133), (571, 133), (571, 133), (571, 133), (571, 133), (571, 133), (571, 133), (571, 133), (571, 133), (571, 133)]
input_ids shape: torch.Size([10, 336])
attention_mask shape: torch.Size([10, 336])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 336


===== Batch 805 =====
QIDs: [1117, 1117, 1117, 1117, 1117, 1117, 1117, 1117, 1117, 1117]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [317, 193, 330, 403, 308, 318, 294, 103, 338, 350]
image sizes: [(1254, 1008), (1254, 1008), (1254, 1008), (1254, 1008), (1254, 1008), (1254, 1008), (1254, 1008), (1254, 1008), (1254, 1008), (1254, 1008)]
input_ids shape: torch.Size([10, 1818])
attention_mask shape: torch.Size([10, 1818])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1818


===== Batch 806 =====
QIDs: [239, 239, 239, 239, 239, 239, 239, 239, 239, 239]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [242, 148, 256, 245, 248, 276, 242, 123, 265, 276]
image sizes: [(1040, 682), (1040, 682), (1040, 682), (1040, 682), (1040, 682), (1040, 682), (1040, 682), (1040, 682), (1040, 682), (1040, 682)]
input_ids shape: torch.Size([10, 1062])
attention_mask shape: torch.Size([10, 1062])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1062


===== Batch 807 =====
QIDs: [320, 320, 320, 320, 320, 320, 320, 320, 320, 320]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [373, 179, 388, 368, 310, 381, 385, 144, 432, 346]
image sizes: [(328, 220), (328, 220), (328, 220), (328, 220), (328, 220), (328, 220), (328, 220), (328, 220), (328, 220), (328, 220)]
input_ids shape: torch.Size([10, 344])
attention_mask shape: torch.Size([10, 344])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 344


===== Batch 808 =====
QIDs: [1184, 1184, 1184, 1184, 1184, 1184, 1184, 1184, 1184, 1184]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [255, 141, 248, 238, 240, 235, 230, 110, 273, 262]
image sizes: [(2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536)]
input_ids shape: torch.Size([10, 4177])
attention_mask shape: torch.Size([10, 4177])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 4177


===== Batch 809 =====
QIDs: [447, 447, 447, 447, 447, 447, 447, 447, 447, 447]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [218, 172, 221, 204, 272, 224, 220, 101, 229, 297]
image sizes: [(704, 536), (704, 536), (704, 536), (704, 536), (704, 536), (704, 536), (704, 536), (704, 536), (704, 536), (704, 536)]
input_ids shape: torch.Size([10, 634])
attention_mask shape: torch.Size([10, 634])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 634


===== Batch 810 =====
QIDs: [486, 486, 486, 486, 486, 486, 486, 486, 486, 486]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [370, 260, 354, 367, 333, 348, 328, 202, 350, 389]
image sizes: [(478, 156), (478, 156), (478, 156), (478, 156), (478, 156), (478, 156), (478, 156), (478, 156), (478, 156), (478, 156)]
input_ids shape: torch.Size([10, 353])
attention_mask shape: torch.Size([10, 353])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 353


===== Batch 811 =====
QIDs: [502, 502, 502, 502, 502, 502, 502, 502, 502, 502]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [243, 213, 250, 262, 257, 245, 221, 202, 260, 265]
image sizes: [(826, 87), (826, 87), (826, 87), (826, 87), (826, 87), (826, 87), (826, 87), (826, 87), (826, 87), (826, 87)]
input_ids shape: torch.Size([10, 274])
attention_mask shape: torch.Size([10, 274])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 274


===== Batch 812 =====
QIDs: [287, 287, 287, 287, 287, 287, 287, 287, 287, 287]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [219, 197, 234, 219, 215, 210, 219, 189, 231, 226]
image sizes: [(706, 55), (706, 55), (706, 55), (706, 55), (706, 55), (706, 55), (706, 55), (706, 55), (706, 55), (706, 55)]
input_ids shape: torch.Size([10, 208])
attention_mask shape: torch.Size([10, 208])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 208


===== Batch 813 =====
QIDs: [1067, 1067, 1067, 1067, 1067, 1067, 1067, 1067, 1067, 1067]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [846, 411, 866, 884, 758, 843, 736, 293, 914, 871]
image sizes: [(766, 546), (766, 546), (766, 546), (766, 546), (766, 546), (766, 546), (766, 546), (766, 546), (766, 546), (766, 546)]
input_ids shape: torch.Size([10, 990])
attention_mask shape: torch.Size([10, 990])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 990


===== Batch 814 =====
QIDs: [1625, 1625, 1625, 1625, 1625, 1625, 1625, 1625, 1625, 1625]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [51, 51, 63, 51, 67, 52, 63, 51, 51, 51]
image sizes: [(1613, 1284), (1613, 1284), (1613, 1284), (1613, 1284), (1613, 1284), (1613, 1284), (1613, 1284), (1613, 1284), (1613, 1284), (1613, 1284)]
input_ids shape: torch.Size([10, 2731])
attention_mask shape: torch.Size([10, 2731])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2731


===== Batch 815 =====
QIDs: [612, 612, 612, 612, 612, 612, 612, 612, 612, 612]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [353, 181, 350, 349, 308, 354, 290, 147, 376, 320]
image sizes: [(733, 410), (733, 410), (733, 410), (733, 410), (733, 410), (733, 410), (733, 410), (733, 410), (733, 410), (733, 410)]
input_ids shape: torch.Size([10, 597])
attention_mask shape: torch.Size([10, 597])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 597


===== Batch 816 =====
QIDs: [1727, 1727, 1727, 1727, 1727, 1727, 1727, 1727, 1727, 1727]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [265, 121, 247, 253, 256, 232, 253, 98, 258, 260]
image sizes: [(540, 686), (540, 686), (540, 686), (540, 686), (540, 686), (540, 686), (540, 686), (540, 686), (540, 686), (540, 686)]
input_ids shape: torch.Size([10, 626])
attention_mask shape: torch.Size([10, 626])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 626


===== Batch 817 =====
QIDs: [470, 470, 470, 470, 470, 470, 470, 470, 470, 470]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [334, 236, 333, 378, 341, 319, 254, 171, 361, 290]
image sizes: [(589, 399), (589, 399), (589, 399), (589, 399), (589, 399), (589, 399), (589, 399), (589, 399), (589, 399), (589, 399)]
input_ids shape: torch.Size([10, 489])
attention_mask shape: torch.Size([10, 489])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 489


===== Batch 818 =====
QIDs: [1554, 1554, 1554, 1554, 1554, 1554, 1554, 1554, 1554, 1554]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [352, 265, 338, 352, 317, 340, 319, 233, 357, 347]
image sizes: [(483, 594), (483, 594), (483, 594), (483, 594), (483, 594), (483, 594), (483, 594), (483, 594), (483, 594), (483, 594)]
input_ids shape: torch.Size([10, 608])
attention_mask shape: torch.Size([10, 608])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 608


===== Batch 819 =====
QIDs: [371, 371, 371, 371, 371, 371, 371, 371, 371, 371]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [172, 125, 170, 165, 156, 171, 140, 105, 164, 169]
image sizes: [(206, 188), (206, 188), (206, 188), (206, 188), (206, 188), (206, 188), (206, 188), (206, 188), (206, 188), (206, 188)]
input_ids shape: torch.Size([10, 164])
attention_mask shape: torch.Size([10, 164])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 164


===== Batch 820 =====
QIDs: [1261, 1261, 1261, 1261, 1261, 1261, 1261, 1261, 1261, 1261]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [215, 111, 193, 157, 192, 207, 216, 87, 244, 155]
image sizes: [(259, 194), (259, 194), (259, 194), (259, 194), (259, 194), (259, 194), (259, 194), (259, 194), (259, 194), (259, 194)]
input_ids shape: torch.Size([10, 227])
attention_mask shape: torch.Size([10, 227])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 227


===== Batch 821 =====
QIDs: [295, 295, 295, 295, 295, 295, 295, 295, 295, 295]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [314, 191, 300, 307, 250, 293, 261, 164, 327, 275]
image sizes: [(1062, 163), (1062, 163), (1062, 163), (1062, 163), (1062, 163), (1062, 163), (1062, 163), (1062, 163), (1062, 163), (1062, 163)]
input_ids shape: torch.Size([10, 415])
attention_mask shape: torch.Size([10, 415])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 415


===== Batch 822 =====
QIDs: [1205, 1205, 1205, 1205, 1205, 1205, 1205, 1205, 1205, 1205]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1221, 1139, 1166, 1235, 1201, 1216, 1168, 1124, 1171, 1229]
image sizes: [(529, 149), (529, 149), (529, 149), (529, 149), (529, 149), (529, 149), (529, 149), (529, 149), (529, 149), (529, 149)]
input_ids shape: torch.Size([10, 951])
attention_mask shape: torch.Size([10, 951])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 951


===== Batch 823 =====
QIDs: [551, 551, 551, 551, 551, 551, 551, 551, 551, 551]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [458, 415, 465, 477, 434, 450, 434, 410, 454, 461]
image sizes: [(601, 186), (601, 186), (601, 186), (601, 186), (601, 186), (601, 186), (601, 186), (601, 186), (601, 186), (601, 186)]
input_ids shape: torch.Size([10, 453])
attention_mask shape: torch.Size([10, 453])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 453


===== Batch 824 =====
QIDs: [892, 892, 892, 892, 892, 892, 892, 892, 892, 892]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [251, 214, 254, 268, 222, 249, 231, 195, 252, 293]
image sizes: [(1114, 458), (1114, 458), (1114, 458), (1114, 458), (1114, 458), (1114, 458), (1114, 458), (1114, 458), (1114, 458), (1114, 458)]
input_ids shape: torch.Size([10, 815])
attention_mask shape: torch.Size([10, 815])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 815


===== Batch 825 =====
QIDs: [1642, 1642, 1642, 1642, 1642, 1642, 1642, 1642, 1642, 1642]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [905, 518, 910, 972, 723, 901, 781, 388, 862, 1042]
image sizes: [(470, 123), (470, 123), (470, 123), (470, 123), (470, 123), (470, 123), (470, 123), (470, 123), (470, 123), (470, 123)]
input_ids shape: torch.Size([10, 581])
attention_mask shape: torch.Size([10, 581])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 581


===== Batch 826 =====
QIDs: [843, 843, 843, 843, 843, 843, 843, 843, 843, 843]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [289, 201, 297, 282, 313, 277, 259, 164, 314, 276]
image sizes: [(376, 262), (376, 262), (376, 262), (376, 262), (376, 262), (376, 262), (376, 262), (376, 262), (376, 262), (376, 262)]
input_ids shape: torch.Size([10, 314])
attention_mask shape: torch.Size([10, 314])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 314


===== Batch 827 =====
QIDs: [1321, 1321, 1321, 1321, 1321, 1321, 1321, 1321, 1321, 1321]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [485, 231, 503, 501, 457, 463, 375, 167, 508, 459]
image sizes: [(504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330)]
input_ids shape: torch.Size([10, 492])
attention_mask shape: torch.Size([10, 492])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 492


===== Batch 828 =====
QIDs: [1461, 1461, 1461, 1461, 1461, 1461, 1461, 1461, 1461, 1461]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [177, 106, 167, 163, 164, 154, 176, 78, 186, 194]
image sizes: [(1208, 912), (1208, 912), (1208, 912), (1208, 912), (1208, 912), (1208, 912), (1208, 912), (1208, 912), (1208, 912), (1208, 912)]
input_ids shape: torch.Size([10, 1555])
attention_mask shape: torch.Size([10, 1555])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1555


===== Batch 829 =====
QIDs: [660, 660, 660, 660, 660, 660, 660, 660, 660, 660]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [286, 164, 304, 298, 301, 312, 277, 122, 363, 323]
image sizes: [(661, 604), (661, 604), (661, 604), (661, 604), (661, 604), (661, 604), (661, 604), (661, 604), (661, 604), (661, 604)]
input_ids shape: torch.Size([10, 709])
attention_mask shape: torch.Size([10, 709])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 709


===== Batch 830 =====
QIDs: [792, 792, 792, 792, 792, 792, 792, 792, 792, 792]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1060, 535, 990, 981, 834, 925, 922, 350, 1052, 1049]
image sizes: [(1216, 421), (1216, 421), (1216, 421), (1216, 421), (1216, 421), (1216, 421), (1216, 421), (1216, 421), (1216, 421), (1216, 421)]
input_ids shape: torch.Size([10, 1160])
attention_mask shape: torch.Size([10, 1160])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1160


===== Batch 831 =====
QIDs: [60, 60, 60, 60, 60, 60, 60, 60, 60, 60]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [295, 145, 293, 315, 276, 293, 261, 106, 313, 297]
image sizes: [(1536, 970), (1536, 970), (1536, 970), (1536, 970), (1536, 970), (1536, 970), (1536, 970), (1536, 970), (1536, 970), (1536, 970)]
input_ids shape: torch.Size([10, 2124])
attention_mask shape: torch.Size([10, 2124])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2124


===== Batch 832 =====
QIDs: [1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026, 1026]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [222, 126, 213, 245, 181, 210, 200, 75, 235, 235]
image sizes: [(594, 430), (594, 430), (594, 430), (594, 430), (594, 430), (594, 430), (594, 430), (594, 430), (594, 430), (594, 430)]
input_ids shape: torch.Size([10, 455])
attention_mask shape: torch.Size([10, 455])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 455


===== Batch 833 =====
QIDs: [129, 129, 129, 129, 129, 129, 129, 129, 129, 129]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [177, 112, 176, 188, 188, 183, 150, 109, 234, 190]
image sizes: [(1094, 614), (1094, 614), (1094, 614), (1094, 614), (1094, 614), (1094, 614), (1094, 614), (1094, 614), (1094, 614), (1094, 614)]
input_ids shape: torch.Size([10, 998])
attention_mask shape: torch.Size([10, 998])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 998


===== Batch 834 =====
QIDs: [832, 832, 832, 832, 832, 832, 832, 832, 832, 832]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [457, 300, 484, 480, 398, 447, 447, 245, 500, 439]
image sizes: [(675, 199), (675, 199), (675, 199), (675, 199), (675, 199), (675, 199), (675, 199), (675, 199), (675, 199), (675, 199)]
input_ids shape: torch.Size([10, 463])
attention_mask shape: torch.Size([10, 463])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 463


===== Batch 835 =====
QIDs: [626, 626, 626, 626, 626, 626, 626, 626, 626, 626]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1194, 940, 1432, 1235, 1375, 1201, 1197, 721, 1219, 1406]
image sizes: [(546, 228), (546, 228), (546, 228), (546, 228), (546, 228), (546, 228), (546, 228), (546, 228), (546, 228), (546, 228)]
input_ids shape: torch.Size([10, 1062])
attention_mask shape: torch.Size([10, 1062])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1062


===== Batch 836 =====
QIDs: [1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602, 1602]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [165, 124, 162, 182, 195, 166, 156, 99, 175, 166]
image sizes: [(762, 336), (762, 336), (762, 336), (762, 336), (762, 336), (762, 336), (762, 336), (762, 336), (762, 336), (762, 336)]
input_ids shape: torch.Size([10, 456])
attention_mask shape: torch.Size([10, 456])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 456


===== Batch 837 =====
QIDs: [966, 966, 966, 966, 966, 966, 966, 966, 966, 966]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [240, 171, 227, 226, 210, 229, 215, 144, 225, 229]
image sizes: [(272, 26), (272, 26), (272, 26), (272, 26), (272, 26), (272, 26), (272, 26), (272, 26), (272, 26), (272, 26)]
input_ids shape: torch.Size([10, 187])
attention_mask shape: torch.Size([10, 187])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 187


===== Batch 838 =====
QIDs: [1534, 1534, 1534, 1534, 1534, 1534, 1534, 1534, 1534, 1534]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [285, 158, 259, 291, 234, 286, 218, 108, 328, 277]
image sizes: [(364, 331), (364, 331), (364, 331), (364, 331), (364, 331), (364, 331), (364, 331), (364, 331), (364, 331), (364, 331)]
input_ids shape: torch.Size([10, 316])
attention_mask shape: torch.Size([10, 316])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 316


===== Batch 839 =====
QIDs: [1130, 1130, 1130, 1130, 1130, 1130, 1130, 1130, 1130, 1130]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [563, 296, 578, 622, 473, 550, 527, 199, 616, 590]
image sizes: [(1688, 1110), (1688, 1110), (1688, 1110), (1688, 1110), (1688, 1110), (1688, 1110), (1688, 1110), (1688, 1110), (1688, 1110), (1688, 1110)]
input_ids shape: torch.Size([10, 2711])
attention_mask shape: torch.Size([10, 2711])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2711


===== Batch 840 =====
QIDs: [917, 917, 917, 917, 917, 917, 917, 917, 917, 917]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [436, 283, 418, 438, 382, 421, 426, 224, 453, 427]
image sizes: [(848, 278), (848, 278), (848, 278), (848, 278), (848, 278), (848, 278), (848, 278), (848, 278), (848, 278), (848, 278)]
input_ids shape: torch.Size([10, 587])
attention_mask shape: torch.Size([10, 587])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 587


===== Batch 841 =====
QIDs: [1170, 1170, 1170, 1170, 1170, 1170, 1170, 1170, 1170, 1170]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [300, 193, 302, 329, 282, 316, 371, 143, 328, 284]
image sizes: [(336, 169), (336, 169), (336, 169), (336, 169), (336, 169), (336, 169), (336, 169), (336, 169), (336, 169), (336, 169)]
input_ids shape: torch.Size([10, 275])
attention_mask shape: torch.Size([10, 275])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 275


===== Batch 842 =====
QIDs: [1014, 1014, 1014, 1014, 1014, 1014, 1014, 1014, 1014, 1014]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [628, 304, 584, 533, 583, 586, 557, 202, 625, 614]
image sizes: [(914, 542), (914, 542), (914, 542), (914, 542), (914, 542), (914, 542), (914, 542), (914, 542), (914, 542), (914, 542)]
input_ids shape: torch.Size([10, 964])
attention_mask shape: torch.Size([10, 964])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 964


===== Batch 843 =====
QIDs: [1132, 1132, 1132, 1132, 1132, 1132, 1132, 1132, 1132, 1132]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [486, 197, 464, 569, 400, 480, 468, 152, 498, 443]
image sizes: [(820, 400), (820, 400), (820, 400), (820, 400), (820, 400), (820, 400), (820, 400), (820, 400), (820, 400), (820, 400)]
input_ids shape: torch.Size([10, 693])
attention_mask shape: torch.Size([10, 693])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 693


===== Batch 844 =====
QIDs: [324, 324, 324, 324, 324, 324, 324, 324, 324, 324]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [500, 215, 451, 470, 465, 444, 431, 167, 511, 461]
image sizes: [(504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331)]
input_ids shape: torch.Size([10, 512])
attention_mask shape: torch.Size([10, 512])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 512


===== Batch 845 =====
QIDs: [657, 657, 657, 657, 657, 657, 657, 657, 657, 657]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [250, 184, 254, 243, 236, 244, 229, 164, 259, 258]
image sizes: [(176, 138), (176, 138), (176, 138), (176, 138), (176, 138), (176, 138), (176, 138), (176, 138), (176, 138), (176, 138)]
input_ids shape: torch.Size([10, 212])
attention_mask shape: torch.Size([10, 212])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 212


===== Batch 846 =====
QIDs: [820, 820, 820, 820, 820, 820, 820, 820, 820, 820]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [951, 468, 946, 973, 735, 879, 791, 303, 1030, 1015]
image sizes: [(1221, 373), (1221, 373), (1221, 373), (1221, 373), (1221, 373), (1221, 373), (1221, 373), (1221, 373), (1221, 373), (1221, 373)]
input_ids shape: torch.Size([10, 1032])
attention_mask shape: torch.Size([10, 1032])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1032


===== Batch 847 =====
QIDs: [1034, 1034, 1034, 1034, 1034, 1034, 1034, 1034, 1034, 1034]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [423, 226, 392, 398, 381, 376, 368, 140, 443, 364]
image sizes: [(580, 248), (580, 248), (580, 248), (580, 248), (580, 248), (580, 248), (580, 248), (580, 248), (580, 248), (580, 248)]
input_ids shape: torch.Size([10, 429])
attention_mask shape: torch.Size([10, 429])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 429


===== Batch 848 =====
QIDs: [321, 321, 321, 321, 321, 321, 321, 321, 321, 321]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [669, 324, 657, 652, 561, 670, 726, 232, 737, 664]
image sizes: [(504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331)]
input_ids shape: torch.Size([10, 662])
attention_mask shape: torch.Size([10, 662])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 662


===== Batch 849 =====
QIDs: [337, 337, 337, 337, 337, 337, 337, 337, 337, 337]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [296, 143, 297, 331, 300, 309, 254, 111, 300, 301]
image sizes: [(890, 675), (890, 675), (890, 675), (890, 675), (890, 675), (890, 675), (890, 675), (890, 675), (890, 675), (890, 675)]
input_ids shape: torch.Size([10, 960])
attention_mask shape: torch.Size([10, 960])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 960


===== Batch 850 =====
QIDs: [489, 489, 489, 489, 489, 489, 489, 489, 489, 489]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [387, 217, 345, 349, 336, 362, 335, 162, 380, 347]
image sizes: [(500, 360), (500, 360), (500, 360), (500, 360), (500, 360), (500, 360), (500, 360), (500, 360), (500, 360), (500, 360)]
input_ids shape: torch.Size([10, 458])
attention_mask shape: torch.Size([10, 458])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 458


===== Batch 851 =====
QIDs: [1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600, 1600]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [59, 46, 62, 69, 55, 58, 60, 39, 60, 60]
image sizes: [(480, 204), (480, 204), (480, 204), (480, 204), (480, 204), (480, 204), (480, 204), (480, 204), (480, 204), (480, 204)]
input_ids shape: torch.Size([10, 176])
attention_mask shape: torch.Size([10, 176])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 176


===== Batch 852 =====
QIDs: [1303, 1303, 1303, 1303, 1303, 1303, 1303, 1303, 1303, 1303]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [137, 97, 147, 146, 141, 136, 127, 85, 142, 139]
image sizes: [(471, 102), (471, 102), (471, 102), (471, 102), (471, 102), (471, 102), (471, 102), (471, 102), (471, 102), (471, 102)]
input_ids shape: torch.Size([10, 163])
attention_mask shape: torch.Size([10, 163])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 163


===== Batch 853 =====
QIDs: [963, 963, 963, 963, 963, 963, 963, 963, 963, 963]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [405, 240, 377, 476, 385, 403, 337, 185, 424, 391]
image sizes: [(436, 404), (436, 404), (436, 404), (436, 404), (436, 404), (436, 404), (436, 404), (436, 404), (436, 404), (436, 404)]
input_ids shape: torch.Size([10, 479])
attention_mask shape: torch.Size([10, 479])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 479


===== Batch 854 =====
QIDs: [477, 477, 477, 477, 477, 477, 477, 477, 477, 477]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [359, 237, 352, 328, 305, 326, 344, 208, 332, 383]
image sizes: [(319, 358), (319, 358), (319, 358), (319, 358), (319, 358), (319, 358), (319, 358), (319, 358), (319, 358), (319, 358)]
input_ids shape: torch.Size([10, 383])
attention_mask shape: torch.Size([10, 383])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 383


===== Batch 855 =====
QIDs: [562, 562, 562, 562, 562, 562, 562, 562, 562, 562]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [397, 265, 393, 421, 349, 380, 368, 201, 386, 415]
image sizes: [(276, 399), (276, 399), (276, 399), (276, 399), (276, 399), (276, 399), (276, 399), (276, 399), (276, 399), (276, 399)]
input_ids shape: torch.Size([10, 417])
attention_mask shape: torch.Size([10, 417])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 417


===== Batch 856 =====
QIDs: [1315, 1315, 1315, 1315, 1315, 1315, 1315, 1315, 1315, 1315]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [519, 257, 559, 519, 513, 500, 508, 198, 648, 476]
image sizes: [(302, 382), (302, 382), (302, 382), (302, 382), (302, 382), (302, 382), (302, 382), (302, 382), (302, 382), (302, 382)]
input_ids shape: torch.Size([10, 461])
attention_mask shape: torch.Size([10, 461])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 461


===== Batch 857 =====
QIDs: [304, 304, 304, 304, 304, 304, 304, 304, 304, 304]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [560, 286, 547, 582, 501, 557, 500, 206, 600, 535]
image sizes: [(504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330)]
input_ids shape: torch.Size([10, 510])
attention_mask shape: torch.Size([10, 510])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 510


===== Batch 858 =====
QIDs: [1168, 1168, 1168, 1168, 1168, 1168, 1168, 1168, 1168, 1168]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [201, 163, 232, 205, 218, 220, 189, 133, 255, 222]
image sizes: [(820, 370), (820, 370), (820, 370), (820, 370), (820, 370), (820, 370), (820, 370), (820, 370), (820, 370), (820, 370)]
input_ids shape: torch.Size([10, 543])
attention_mask shape: torch.Size([10, 543])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 543


===== Batch 859 =====
QIDs: [757, 757, 757, 757, 757, 757, 757, 757, 757, 757]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [669, 319, 629, 640, 515, 645, 565, 227, 636, 683]
image sizes: [(477, 330), (477, 330), (477, 330), (477, 330), (477, 330), (477, 330), (477, 330), (477, 330), (477, 330), (477, 330)]
input_ids shape: torch.Size([10, 561])
attention_mask shape: torch.Size([10, 561])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 561


===== Batch 860 =====
QIDs: [1209, 1209, 1209, 1209, 1209, 1209, 1209, 1209, 1209, 1209]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [561, 338, 517, 551, 486, 531, 489, 281, 563, 565]
image sizes: [(709, 286), (709, 286), (709, 286), (709, 286), (709, 286), (709, 286), (709, 286), (709, 286), (709, 286), (709, 286)]
input_ids shape: torch.Size([10, 601])
attention_mask shape: torch.Size([10, 601])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 601


===== Batch 861 =====
QIDs: [230, 230, 230, 230, 230, 230, 230, 230, 230, 230]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [370, 179, 357, 371, 332, 300, 309, 122, 418, 377]
image sizes: [(2075, 758), (2075, 758), (2075, 758), (2075, 758), (2075, 758), (2075, 758), (2075, 758), (2075, 758), (2075, 758), (2075, 758)]
input_ids shape: torch.Size([10, 2210])
attention_mask shape: torch.Size([10, 2210])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2210


===== Batch 862 =====
QIDs: [763, 763, 763, 763, 763, 763, 763, 763, 763, 763]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [310, 183, 319, 324, 286, 302, 288, 144, 333, 327]
image sizes: [(1052, 754), (1052, 754), (1052, 754), (1052, 754), (1052, 754), (1052, 754), (1052, 754), (1052, 754), (1052, 754), (1052, 754)]
input_ids shape: torch.Size([10, 1239])
attention_mask shape: torch.Size([10, 1239])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1239


===== Batch 863 =====
QIDs: [1367, 1367, 1367, 1367, 1367, 1367, 1367, 1367, 1367, 1367]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [160, 87, 146, 139, 148, 146, 158, 66, 157, 164]
image sizes: [(1054, 780), (1054, 780), (1054, 780), (1054, 780), (1054, 780), (1054, 780), (1054, 780), (1054, 780), (1054, 780), (1054, 780)]
input_ids shape: torch.Size([10, 1171])
attention_mask shape: torch.Size([10, 1171])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1171


===== Batch 864 =====
QIDs: [609, 609, 609, 609, 609, 609, 609, 609, 609, 609]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [662, 444, 652, 733, 682, 627, 620, 271, 730, 686]
image sizes: [(866, 316), (866, 316), (866, 316), (866, 316), (866, 316), (866, 316), (866, 316), (866, 316), (866, 316), (866, 316)]
input_ids shape: torch.Size([10, 724])
attention_mask shape: torch.Size([10, 724])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 724


===== Batch 865 =====
QIDs: [1640, 1640, 1640, 1640, 1640, 1640, 1640, 1640, 1640, 1640]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [342, 179, 312, 321, 281, 337, 255, 116, 363, 308]
image sizes: [(89, 58), (89, 58), (89, 58), (89, 58), (89, 58), (89, 58), (89, 58), (89, 58), (89, 58), (89, 58)]
input_ids shape: torch.Size([10, 187])
attention_mask shape: torch.Size([10, 187])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 187


===== Batch 866 =====
QIDs: [828, 828, 828, 828, 828, 828, 828, 828, 828, 828]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [317, 191, 281, 293, 261, 288, 261, 158, 310, 272]
image sizes: [(309, 267), (309, 267), (309, 267), (309, 267), (309, 267), (309, 267), (309, 267), (309, 267), (309, 267), (309, 267)]
input_ids shape: torch.Size([10, 296])
attention_mask shape: torch.Size([10, 296])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 296


===== Batch 867 =====
QIDs: [1093, 1093, 1093, 1093, 1093, 1093, 1093, 1093, 1093, 1093]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [346, 168, 341, 339, 292, 350, 322, 124, 422, 315]
image sizes: [(1486, 892), (1486, 892), (1486, 892), (1486, 892), (1486, 892), (1486, 892), (1486, 892), (1486, 892), (1486, 892), (1486, 892)]
input_ids shape: torch.Size([10, 1909])
attention_mask shape: torch.Size([10, 1909])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1909


===== Batch 868 =====
QIDs: [781, 781, 781, 781, 781, 781, 781, 781, 781, 781]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [521, 233, 473, 563, 426, 469, 404, 163, 535, 523]
image sizes: [(1039, 911), (1039, 911), (1039, 911), (1039, 911), (1039, 911), (1039, 911), (1039, 911), (1039, 911), (1039, 911), (1039, 911)]
input_ids shape: torch.Size([10, 1501])
attention_mask shape: torch.Size([10, 1501])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1501


===== Batch 869 =====
QIDs: [305, 305, 305, 305, 305, 305, 305, 305, 305, 305]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [148, 113, 159, 157, 166, 146, 166, 87, 170, 180]
image sizes: [(1062, 616), (1062, 616), (1062, 616), (1062, 616), (1062, 616), (1062, 616), (1062, 616), (1062, 616), (1062, 616), (1062, 616)]
input_ids shape: torch.Size([10, 951])
attention_mask shape: torch.Size([10, 951])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 951


===== Batch 870 =====
QIDs: [488, 488, 488, 488, 488, 488, 488, 488, 488, 488]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [443, 294, 414, 486, 388, 388, 468, 222, 431, 447]
image sizes: [(444, 316), (444, 316), (444, 316), (444, 316), (444, 316), (444, 316), (444, 316), (444, 316), (444, 316), (444, 316)]
input_ids shape: torch.Size([10, 464])
attention_mask shape: torch.Size([10, 464])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 464


===== Batch 871 =====
QIDs: [675, 675, 675, 675, 675, 675, 675, 675, 675, 675]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [236, 137, 233, 262, 216, 235, 218, 118, 230, 256]
image sizes: [(744, 470), (744, 470), (744, 470), (744, 470), (744, 470), (744, 470), (744, 470), (744, 470), (744, 470), (744, 470)]
input_ids shape: torch.Size([10, 615])
attention_mask shape: torch.Size([10, 615])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 615


===== Batch 872 =====
QIDs: [1549, 1549, 1549, 1549, 1549, 1549, 1549, 1549, 1549, 1549]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [324, 246, 307, 316, 299, 316, 311, 207, 327, 311]
image sizes: [(1026, 153), (1026, 153), (1026, 153), (1026, 153), (1026, 153), (1026, 153), (1026, 153), (1026, 153), (1026, 153), (1026, 153)]
input_ids shape: torch.Size([10, 428])
attention_mask shape: torch.Size([10, 428])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 428


===== Batch 873 =====
QIDs: [294, 294, 294, 294, 294, 294, 294, 294, 294, 294]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [325, 184, 309, 313, 268, 305, 270, 138, 301, 281]
image sizes: [(264, 233), (264, 233), (264, 233), (264, 233), (264, 233), (264, 233), (264, 233), (264, 233), (264, 233), (264, 233)]
input_ids shape: torch.Size([10, 241])
attention_mask shape: torch.Size([10, 241])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 241


===== Batch 874 =====
QIDs: [179, 179, 179, 179, 179, 179, 179, 179, 179, 179]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [340, 160, 350, 343, 303, 327, 430, 128, 341, 361]
image sizes: [(215, 260), (215, 260), (215, 260), (215, 260), (215, 260), (215, 260), (215, 260), (215, 260), (215, 260), (215, 260)]
input_ids shape: torch.Size([10, 288])
attention_mask shape: torch.Size([10, 288])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 288


===== Batch 875 =====
QIDs: [1365, 1365, 1365, 1365, 1365, 1365, 1365, 1365, 1365, 1365]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [330, 176, 336, 334, 326, 331, 343, 124, 330, 358]
image sizes: [(1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604)]
input_ids shape: torch.Size([10, 4065])
attention_mask shape: torch.Size([10, 4065])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 4065


===== Batch 876 =====
QIDs: [461, 461, 461, 461, 461, 461, 461, 461, 461, 461]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [463, 242, 462, 480, 483, 448, 518, 187, 562, 507]
image sizes: [(786, 948), (786, 948), (786, 948), (786, 948), (786, 948), (786, 948), (786, 948), (786, 948), (786, 948), (786, 948)]
input_ids shape: torch.Size([10, 1293])
attention_mask shape: torch.Size([10, 1293])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1293


===== Batch 877 =====
QIDs: [927, 927, 927, 927, 927, 927, 927, 927, 927, 927]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [85, 58, 81, 99, 74, 86, 70, 43, 86, 74]
image sizes: [(546, 188), (546, 188), (546, 188), (546, 188), (546, 188), (546, 188), (546, 188), (546, 188), (546, 188), (546, 188)]
input_ids shape: torch.Size([10, 215])
attention_mask shape: torch.Size([10, 215])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 215


===== Batch 878 =====
QIDs: [1687, 1687, 1687, 1687, 1687, 1687, 1687, 1687, 1687, 1687]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [442, 208, 435, 447, 392, 387, 403, 147, 446, 487]
image sizes: [(896, 506), (896, 506), (896, 506), (896, 506), (896, 506), (896, 506), (896, 506), (896, 506), (896, 506), (896, 506)]
input_ids shape: torch.Size([10, 838])
attention_mask shape: torch.Size([10, 838])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 838


===== Batch 879 =====
QIDs: [1389, 1389, 1389, 1389, 1389, 1389, 1389, 1389, 1389, 1389]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [387, 264, 378, 385, 360, 374, 345, 205, 389, 395]
image sizes: [(478, 156), (478, 156), (478, 156), (478, 156), (478, 156), (478, 156), (478, 156), (478, 156), (478, 156), (478, 156)]
input_ids shape: torch.Size([10, 364])
attention_mask shape: torch.Size([10, 364])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 364


===== Batch 880 =====
QIDs: [1449, 1449, 1449, 1449, 1449, 1449, 1449, 1449, 1449, 1449]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [262, 198, 272, 274, 245, 278, 246, 161, 289, 288]
image sizes: [(598, 232), (598, 232), (598, 232), (598, 232), (598, 232), (598, 232), (598, 232), (598, 232), (598, 232), (598, 232)]
input_ids shape: torch.Size([10, 367])
attention_mask shape: torch.Size([10, 367])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 367


===== Batch 881 =====
QIDs: [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [479, 334, 490, 509, 439, 460, 442, 255, 509, 485]
image sizes: [(345, 168), (345, 168), (345, 168), (345, 168), (345, 168), (345, 168), (345, 168), (345, 168), (345, 168), (345, 168)]
input_ids shape: torch.Size([10, 378])
attention_mask shape: torch.Size([10, 378])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 378


===== Batch 882 =====
QIDs: [812, 812, 812, 812, 812, 812, 812, 812, 812, 812]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [529, 377, 540, 524, 571, 515, 497, 294, 548, 524]
image sizes: [(1224, 355), (1224, 355), (1224, 355), (1224, 355), (1224, 355), (1224, 355), (1224, 355), (1224, 355), (1224, 355), (1224, 355)]
input_ids shape: torch.Size([10, 874])
attention_mask shape: torch.Size([10, 874])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 874


===== Batch 883 =====
QIDs: [842, 842, 842, 842, 842, 842, 842, 842, 842, 842]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [279, 206, 266, 291, 251, 260, 235, 173, 261, 259]
image sizes: [(642, 427), (642, 427), (642, 427), (642, 427), (642, 427), (642, 427), (642, 427), (642, 427), (642, 427), (642, 427)]
input_ids shape: torch.Size([10, 561])
attention_mask shape: torch.Size([10, 561])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 561


===== Batch 884 =====
QIDs: [1689, 1689, 1689, 1689, 1689, 1689, 1689, 1689, 1689, 1689]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [455, 221, 446, 454, 360, 453, 387, 168, 440, 455]
image sizes: [(1110, 216), (1110, 216), (1110, 216), (1110, 216), (1110, 216), (1110, 216), (1110, 216), (1110, 216), (1110, 216), (1110, 216)]
input_ids shape: torch.Size([10, 593])
attention_mask shape: torch.Size([10, 593])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 593


===== Batch 885 =====
QIDs: [90, 90, 90, 90, 90, 90, 90, 90, 90, 90]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [516, 330, 529, 570, 443, 559, 498, 318, 598, 544]
image sizes: [(329, 121), (329, 121), (329, 121), (329, 121), (329, 121), (329, 121), (329, 121), (329, 121), (329, 121), (329, 121)]
input_ids shape: torch.Size([10, 425])
attention_mask shape: torch.Size([10, 425])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 425


===== Batch 886 =====
QIDs: [1719, 1719, 1719, 1719, 1719, 1719, 1719, 1719, 1719, 1719]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1027, 467, 930, 1135, 776, 971, 812, 293, 1023, 1092]
image sizes: [(1057, 1204), (1057, 1204), (1057, 1204), (1057, 1204), (1057, 1204), (1057, 1204), (1057, 1204), (1057, 1204), (1057, 1204), (1057, 1204)]
input_ids shape: torch.Size([10, 2142])
attention_mask shape: torch.Size([10, 2142])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2142


===== Batch 887 =====
QIDs: [1249, 1249, 1249, 1249, 1249, 1249, 1249, 1249, 1249, 1249]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [150, 96, 160, 147, 169, 144, 132, 84, 149, 137]
image sizes: [(666, 788), (666, 788), (666, 788), (666, 788), (666, 788), (666, 788), (666, 788), (666, 788), (666, 788), (666, 788)]
input_ids shape: torch.Size([10, 784])
attention_mask shape: torch.Size([10, 784])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 784


===== Batch 888 =====
QIDs: [1082, 1082, 1082, 1082, 1082, 1082, 1082, 1082, 1082, 1082]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [991, 478, 964, 1021, 868, 935, 897, 323, 1105, 973]
image sizes: [(847, 221), (847, 221), (847, 221), (847, 221), (847, 221), (847, 221), (847, 221), (847, 221), (847, 221), (847, 221)]
input_ids shape: torch.Size([10, 811])
attention_mask shape: torch.Size([10, 811])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 811


===== Batch 889 =====
QIDs: [1126, 1126, 1126, 1126, 1126, 1126, 1126, 1126, 1126, 1126]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [135, 82, 135, 159, 116, 148, 122, 57, 139, 150]
image sizes: [(1011, 950), (1011, 950), (1011, 950), (1011, 950), (1011, 950), (1011, 950), (1011, 950), (1011, 950), (1011, 950), (1011, 950)]
input_ids shape: torch.Size([10, 1320])
attention_mask shape: torch.Size([10, 1320])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1320


===== Batch 890 =====
QIDs: [1564, 1564, 1564, 1564, 1564, 1564, 1564, 1564, 1564, 1564]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [554, 433, 577, 579, 518, 549, 544, 357, 578, 555]
image sizes: [(555, 396), (555, 396), (555, 396), (555, 396), (555, 396), (555, 396), (555, 396), (555, 396), (555, 396), (555, 396)]
input_ids shape: torch.Size([10, 609])
attention_mask shape: torch.Size([10, 609])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 609


===== Batch 891 =====
QIDs: [193, 193, 193, 193, 193, 193, 193, 193, 193, 193]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [223, 99, 240, 218, 171, 228, 177, 93, 229, 159]
image sizes: [(472, 377), (472, 377), (472, 377), (472, 377), (472, 377), (472, 377), (472, 377), (472, 377), (472, 377), (472, 377)]
input_ids shape: torch.Size([10, 375])
attention_mask shape: torch.Size([10, 375])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 375


===== Batch 892 =====
QIDs: [1670, 1670, 1670, 1670, 1670, 1670, 1670, 1670, 1670, 1670]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [264, 183, 254, 278, 225, 240, 224, 144, 236, 267]
image sizes: [(508, 378), (508, 378), (508, 378), (508, 378), (508, 378), (508, 378), (508, 378), (508, 378), (508, 378), (508, 378)]
input_ids shape: torch.Size([10, 441])
attention_mask shape: torch.Size([10, 441])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 441


===== Batch 893 =====
QIDs: [58, 58, 58, 58, 58, 58, 58, 58, 58, 58]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [215, 154, 202, 205, 184, 204, 199, 107, 239, 249]
image sizes: [(1034, 566), (1034, 566), (1034, 566), (1034, 566), (1034, 566), (1034, 566), (1034, 566), (1034, 566), (1034, 566), (1034, 566)]
input_ids shape: torch.Size([10, 877])
attention_mask shape: torch.Size([10, 877])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 877


===== Batch 894 =====
QIDs: [547, 547, 547, 547, 547, 547, 547, 547, 547, 547]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [563, 513, 550, 575, 556, 554, 550, 512, 558, 559]
image sizes: [(330, 215), (330, 215), (330, 215), (330, 215), (330, 215), (330, 215), (330, 215), (330, 215), (330, 215), (330, 215)]
input_ids shape: torch.Size([10, 426])
attention_mask shape: torch.Size([10, 426])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 426


===== Batch 895 =====
QIDs: [329, 329, 329, 329, 329, 329, 329, 329, 329, 329]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [928, 453, 925, 962, 868, 893, 941, 297, 1043, 876]
image sizes: [(504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330)]
input_ids shape: torch.Size([10, 797])
attention_mask shape: torch.Size([10, 797])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 797


===== Batch 896 =====
QIDs: [405, 405, 405, 405, 405, 405, 405, 405, 405, 405]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [188, 96, 172, 160, 138, 170, 179, 85, 184, 222]
image sizes: [(250, 221), (250, 221), (250, 221), (250, 221), (250, 221), (250, 221), (250, 221), (250, 221), (250, 221), (250, 221)]
input_ids shape: torch.Size([10, 207])
attention_mask shape: torch.Size([10, 207])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 207


===== Batch 897 =====
QIDs: [1657, 1657, 1657, 1657, 1657, 1657, 1657, 1657, 1657, 1657]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [547, 270, 504, 492, 463, 479, 420, 165, 517, 509]
image sizes: [(732, 676), (732, 676), (732, 676), (732, 676), (732, 676), (732, 676), (732, 676), (732, 676), (732, 676), (732, 676)]
input_ids shape: torch.Size([10, 892])
attention_mask shape: torch.Size([10, 892])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 892


===== Batch 898 =====
QIDs: [823, 823, 823, 823, 823, 823, 823, 823, 823, 823]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [433, 357, 425, 477, 426, 438, 418, 305, 461, 427]
image sizes: [(577, 532), (577, 532), (577, 532), (577, 532), (577, 532), (577, 532), (577, 532), (577, 532), (577, 532), (577, 532)]
input_ids shape: torch.Size([10, 698])
attention_mask shape: torch.Size([10, 698])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 698


===== Batch 899 =====
QIDs: [1182, 1182, 1182, 1182, 1182, 1182, 1182, 1182, 1182, 1182]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [766, 396, 735, 880, 689, 714, 581, 238, 843, 743]
image sizes: [(1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560)]
input_ids shape: torch.Size([10, 6699])
attention_mask shape: torch.Size([10, 6699])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 6699


===== Batch 900 =====
QIDs: [248, 248, 248, 248, 248, 248, 248, 248, 248, 248]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [159, 106, 145, 161, 133, 154, 134, 87, 158, 144]
image sizes: [(556, 464), (556, 464), (556, 464), (556, 464), (556, 464), (556, 464), (556, 464), (556, 464), (556, 464), (556, 464)]
input_ids shape: torch.Size([10, 436])
attention_mask shape: torch.Size([10, 436])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 436


===== Batch 901 =====
QIDs: [1372, 1372, 1372, 1372, 1372, 1372, 1372, 1372, 1372, 1372]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [580, 242, 643, 700, 450, 548, 566, 193, 614, 538]
image sizes: [(930, 800), (930, 800), (930, 800), (930, 800), (930, 800), (930, 800), (930, 800), (930, 800), (930, 800), (930, 800)]
input_ids shape: torch.Size([10, 1315])
attention_mask shape: torch.Size([10, 1315])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1315


===== Batch 902 =====
QIDs: [931, 931, 931, 931, 931, 931, 931, 931, 931, 931]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [125, 94, 123, 128, 111, 131, 115, 70, 129, 113]
image sizes: [(638, 176), (638, 176), (638, 176), (638, 176), (638, 176), (638, 176), (638, 176), (638, 176), (638, 176), (638, 176)]
input_ids shape: torch.Size([10, 226])
attention_mask shape: torch.Size([10, 226])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 226


===== Batch 903 =====
QIDs: [289, 289, 289, 289, 289, 289, 289, 289, 289, 289]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [163, 130, 156, 159, 161, 154, 149, 119, 156, 159]
image sizes: [(115, 105), (115, 105), (115, 105), (115, 105), (115, 105), (115, 105), (115, 105), (115, 105), (115, 105), (115, 105)]
input_ids shape: torch.Size([10, 137])
attention_mask shape: torch.Size([10, 137])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 137


===== Batch 904 =====
QIDs: [92, 92, 92, 92, 92, 92, 92, 92, 92, 92]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [622, 303, 546, 579, 472, 607, 535, 231, 617, 561]
image sizes: [(604, 121), (604, 121), (604, 121), (604, 121), (604, 121), (604, 121), (604, 121), (604, 121), (604, 121), (604, 121)]
input_ids shape: torch.Size([10, 427])
attention_mask shape: torch.Size([10, 427])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 427


===== Batch 905 =====
QIDs: [1501, 1501, 1501, 1501, 1501, 1501, 1501, 1501, 1501, 1501]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1983, 1012, 1874, 2176, 1920, 1943, 1828, 667, 2217, 2151]
image sizes: [(1022, 649), (1022, 649), (1022, 649), (1022, 649), (1022, 649), (1022, 649), (1022, 649), (1022, 649), (1022, 649), (1022, 649)]
input_ids shape: torch.Size([10, 1808])
attention_mask shape: torch.Size([10, 1808])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1808


===== Batch 906 =====
QIDs: [6, 6, 6, 6, 6, 6, 6, 6, 6, 6]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [509, 367, 520, 538, 496, 543, 502, 311, 535, 547]
image sizes: [(343, 118), (343, 118), (343, 118), (343, 118), (343, 118), (343, 118), (343, 118), (343, 118), (343, 118), (343, 118)]
input_ids shape: torch.Size([10, 408])
attention_mask shape: torch.Size([10, 408])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 408


===== Batch 907 =====
QIDs: [474, 474, 474, 474, 474, 474, 474, 474, 474, 474]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [502, 269, 459, 509, 393, 448, 424, 172, 538, 466]
image sizes: [(304, 112), (304, 112), (304, 112), (304, 112), (304, 112), (304, 112), (304, 112), (304, 112), (304, 112), (304, 112)]
input_ids shape: torch.Size([10, 310])
attention_mask shape: torch.Size([10, 310])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 310


===== Batch 908 =====
QIDs: [80, 80, 80, 80, 80, 80, 80, 80, 80, 80]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [204, 191, 218, 302, 259, 348, 249, 159, 284, 211]
image sizes: [(441, 279), (441, 279), (441, 279), (441, 279), (441, 279), (441, 279), (441, 279), (441, 279), (441, 279), (441, 279)]
input_ids shape: torch.Size([10, 376])
attention_mask shape: torch.Size([10, 376])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 376


===== Batch 909 =====
QIDs: [1591, 1591, 1591, 1591, 1591, 1591, 1591, 1591, 1591, 1591]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [683, 417, 712, 736, 674, 678, 695, 312, 846, 629]
image sizes: [(215, 127), (215, 127), (215, 127), (215, 127), (215, 127), (215, 127), (215, 127), (215, 127), (215, 127), (215, 127)]
input_ids shape: torch.Size([10, 571])
attention_mask shape: torch.Size([10, 571])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 571


===== Batch 910 =====
QIDs: [1708, 1708, 1708, 1708, 1708, 1708, 1708, 1708, 1708, 1708]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [502, 243, 477, 500, 383, 466, 414, 193, 544, 489]
image sizes: [(675, 144), (675, 144), (675, 144), (675, 144), (675, 144), (675, 144), (675, 144), (675, 144), (675, 144), (675, 144)]
input_ids shape: torch.Size([10, 404])
attention_mask shape: torch.Size([10, 404])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 404


===== Batch 911 =====
QIDs: [443, 443, 443, 443, 443, 443, 443, 443, 443, 443]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [363, 210, 344, 351, 365, 370, 370, 135, 356, 375]
image sizes: [(1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604)]
input_ids shape: torch.Size([10, 4082])
attention_mask shape: torch.Size([10, 4082])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 4082


===== Batch 912 =====
QIDs: [512, 512, 512, 512, 512, 512, 512, 512, 512, 512]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [296, 221, 289, 324, 258, 294, 279, 199, 298, 283]
image sizes: [(485, 220), (485, 220), (485, 220), (485, 220), (485, 220), (485, 220), (485, 220), (485, 220), (485, 220), (485, 220)]
input_ids shape: torch.Size([10, 359])
attention_mask shape: torch.Size([10, 359])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 359


===== Batch 913 =====
QIDs: [980, 980, 980, 980, 980, 980, 980, 980, 980, 980]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [96, 75, 116, 84, 91, 100, 84, 63, 96, 93]
image sizes: [(224, 140), (224, 140), (224, 140), (224, 140), (224, 140), (224, 140), (224, 140), (224, 140), (224, 140), (224, 140)]
input_ids shape: torch.Size([10, 126])
attention_mask shape: torch.Size([10, 126])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 126


===== Batch 914 =====
QIDs: [1557, 1557, 1557, 1557, 1557, 1557, 1557, 1557, 1557, 1557]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [536, 357, 519, 549, 445, 513, 443, 296, 539, 511]
image sizes: [(412, 483), (412, 483), (412, 483), (412, 483), (412, 483), (412, 483), (412, 483), (412, 483), (412, 483), (412, 483)]
input_ids shape: torch.Size([10, 544])
attention_mask shape: torch.Size([10, 544])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 544


===== Batch 915 =====
QIDs: [940, 940, 940, 940, 940, 940, 940, 940, 940, 940]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [111, 93, 121, 117, 147, 116, 112, 85, 117, 116]
image sizes: [(1503, 657), (1503, 657), (1503, 657), (1503, 657), (1503, 657), (1503, 657), (1503, 657), (1503, 657), (1503, 657), (1503, 657)]
input_ids shape: torch.Size([10, 1352])
attention_mask shape: torch.Size([10, 1352])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1352


===== Batch 916 =====
QIDs: [1645, 1645, 1645, 1645, 1645, 1645, 1645, 1645, 1645, 1645]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [433, 260, 464, 452, 451, 483, 438, 178, 553, 425]
image sizes: [(145, 129), (145, 129), (145, 129), (145, 129), (145, 129), (145, 129), (145, 129), (145, 129), (145, 129), (145, 129)]
input_ids shape: torch.Size([10, 329])
attention_mask shape: torch.Size([10, 329])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 329


===== Batch 917 =====
QIDs: [908, 908, 908, 908, 908, 908, 908, 908, 908, 908]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [936, 718, 884, 920, 787, 878, 786, 620, 926, 854]
image sizes: [(819, 794), (819, 794), (819, 794), (819, 794), (819, 794), (819, 794), (819, 794), (819, 794), (819, 794), (819, 794)]
input_ids shape: torch.Size([10, 1291])
attention_mask shape: torch.Size([10, 1291])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1291


===== Batch 918 =====
QIDs: [444, 444, 444, 444, 444, 444, 444, 444, 444, 444]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [325, 174, 296, 320, 297, 306, 262, 115, 316, 305]
image sizes: [(198, 175), (198, 175), (198, 175), (198, 175), (198, 175), (198, 175), (198, 175), (198, 175), (198, 175), (198, 175)]
input_ids shape: torch.Size([10, 228])
attention_mask shape: torch.Size([10, 228])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 228


===== Batch 919 =====
QIDs: [631, 631, 631, 631, 631, 631, 631, 631, 631, 631]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [298, 145, 291, 320, 259, 271, 326, 124, 330, 340]
image sizes: [(480, 340), (480, 340), (480, 340), (480, 340), (480, 340), (480, 340), (480, 340), (480, 340), (480, 340), (480, 340)]
input_ids shape: torch.Size([10, 402])
attention_mask shape: torch.Size([10, 402])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 402


===== Batch 920 =====
QIDs: [258, 258, 258, 258, 258, 258, 258, 258, 258, 258]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [270, 177, 249, 267, 220, 245, 244, 154, 302, 251]
image sizes: [(682, 99), (682, 99), (682, 99), (682, 99), (682, 99), (682, 99), (682, 99), (682, 99), (682, 99), (682, 99)]
input_ids shape: torch.Size([10, 260])
attention_mask shape: torch.Size([10, 260])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 260


===== Batch 921 =====
QIDs: [1097, 1097, 1097, 1097, 1097, 1097, 1097, 1097, 1097, 1097]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [251, 169, 237, 245, 232, 247, 216, 134, 255, 258]
image sizes: [(533, 158), (533, 158), (533, 158), (533, 158), (533, 158), (533, 158), (533, 158), (533, 158), (533, 158), (533, 158)]
input_ids shape: torch.Size([10, 295])
attention_mask shape: torch.Size([10, 295])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 295


===== Batch 922 =====
QIDs: [318, 318, 318, 318, 318, 318, 318, 318, 318, 318]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [254, 129, 262, 305, 267, 274, 248, 102, 276, 236]
image sizes: [(360, 480), (360, 480), (360, 480), (360, 480), (360, 480), (360, 480), (360, 480), (360, 480), (360, 480), (360, 480)]
input_ids shape: torch.Size([10, 402])
attention_mask shape: torch.Size([10, 402])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 402


===== Batch 923 =====
QIDs: [874, 874, 874, 874, 874, 874, 874, 874, 874, 874]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [328, 251, 298, 324, 288, 314, 315, 235, 299, 308]
image sizes: [(1004, 484), (1004, 484), (1004, 484), (1004, 484), (1004, 484), (1004, 484), (1004, 484), (1004, 484), (1004, 484), (1004, 484)]
input_ids shape: torch.Size([10, 829])
attention_mask shape: torch.Size([10, 829])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 829


===== Batch 924 =====
QIDs: [180, 180, 180, 180, 180, 180, 180, 180, 180, 180]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [222, 120, 208, 222, 223, 213, 225, 100, 220, 246]
image sizes: [(900, 600), (900, 600), (900, 600), (900, 600), (900, 600), (900, 600), (900, 600), (900, 600), (900, 600), (900, 600)]
input_ids shape: torch.Size([10, 831])
attention_mask shape: torch.Size([10, 831])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 831


===== Batch 925 =====
QIDs: [498, 498, 498, 498, 498, 498, 498, 498, 498, 498]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [294, 175, 304, 308, 286, 319, 286, 146, 337, 330]
image sizes: [(1063, 283), (1063, 283), (1063, 283), (1063, 283), (1063, 283), (1063, 283), (1063, 283), (1063, 283), (1063, 283), (1063, 283)]
input_ids shape: torch.Size([10, 589])
attention_mask shape: torch.Size([10, 589])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 589


===== Batch 926 =====
QIDs: [1275, 1275, 1275, 1275, 1275, 1275, 1275, 1275, 1275, 1275]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [139, 87, 132, 132, 127, 141, 131, 61, 150, 133]
image sizes: [(378, 347), (378, 347), (378, 347), (378, 347), (378, 347), (378, 347), (378, 347), (378, 347), (378, 347), (378, 347)]
input_ids shape: torch.Size([10, 269])
attention_mask shape: torch.Size([10, 269])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 269


===== Batch 927 =====
QIDs: [1404, 1404, 1404, 1404, 1404, 1404, 1404, 1404, 1404, 1404]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [236, 147, 231, 271, 477, 300, 243, 108, 330, 259]
image sizes: [(697, 652), (697, 652), (697, 652), (697, 652), (697, 652), (697, 652), (697, 652), (697, 652), (697, 652), (697, 652)]
input_ids shape: torch.Size([10, 757])
attention_mask shape: torch.Size([10, 757])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 757


===== Batch 928 =====
QIDs: [1006, 1006, 1006, 1006, 1006, 1006, 1006, 1006, 1006, 1006]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [565, 280, 484, 513, 480, 493, 397, 196, 474, 504]
image sizes: [(652, 444), (652, 444), (652, 444), (652, 444), (652, 444), (652, 444), (652, 444), (652, 444), (652, 444), (652, 444)]
input_ids shape: torch.Size([10, 618])
attention_mask shape: torch.Size([10, 618])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 618


===== Batch 929 =====
QIDs: [1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200, 1200]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [263, 210, 270, 270, 240, 249, 249, 181, 265, 265]
image sizes: [(426, 382), (426, 382), (426, 382), (426, 382), (426, 382), (426, 382), (426, 382), (426, 382), (426, 382), (426, 382)]
input_ids shape: torch.Size([10, 402])
attention_mask shape: torch.Size([10, 402])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 402


===== Batch 930 =====
QIDs: [1633, 1633, 1633, 1633, 1633, 1633, 1633, 1633, 1633, 1633]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [197, 117, 172, 196, 173, 191, 162, 94, 201, 167]
image sizes: [(735, 131), (735, 131), (735, 131), (735, 131), (735, 131), (735, 131), (735, 131), (735, 131), (735, 131), (735, 131)]
input_ids shape: torch.Size([10, 250])
attention_mask shape: torch.Size([10, 250])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 250


===== Batch 931 =====
QIDs: [521, 521, 521, 521, 521, 521, 521, 521, 521, 521]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1039, 997, 1041, 1046, 1031, 1040, 1022, 982, 1037, 1042]
image sizes: [(613, 177), (613, 177), (613, 177), (613, 177), (613, 177), (613, 177), (613, 177), (613, 177), (613, 177), (613, 177)]
input_ids shape: torch.Size([10, 682])
attention_mask shape: torch.Size([10, 682])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 682


===== Batch 932 =====
QIDs: [226, 226, 226, 226, 226, 226, 226, 226, 226, 226]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [353, 241, 340, 394, 336, 369, 379, 201, 369, 404]
image sizes: [(1204, 610), (1204, 610), (1204, 610), (1204, 610), (1204, 610), (1204, 610), (1204, 610), (1204, 610), (1204, 610), (1204, 610)]
input_ids shape: torch.Size([10, 1206])
attention_mask shape: torch.Size([10, 1206])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1206


===== Batch 933 =====
QIDs: [1146, 1146, 1146, 1146, 1146, 1146, 1146, 1146, 1146, 1146]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [273, 128, 287, 276, 236, 264, 246, 116, 281, 258]
image sizes: [(944, 507), (944, 507), (944, 507), (944, 507), (944, 507), (944, 507), (944, 507), (944, 507), (944, 507), (944, 507)]
input_ids shape: torch.Size([10, 780])
attention_mask shape: torch.Size([10, 780])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 780


===== Batch 934 =====
QIDs: [783, 783, 783, 783, 783, 783, 783, 783, 783, 783]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [142, 111, 143, 162, 169, 147, 137, 101, 157, 142]
image sizes: [(619, 288), (619, 288), (619, 288), (619, 288), (619, 288), (619, 288), (619, 288), (619, 288), (619, 288), (619, 288)]
input_ids shape: torch.Size([10, 371])
attention_mask shape: torch.Size([10, 371])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 371


===== Batch 935 =====
QIDs: [494, 494, 494, 494, 494, 494, 494, 494, 494, 494]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [371, 235, 342, 345, 314, 358, 315, 153, 417, 340]
image sizes: [(310, 166), (310, 166), (310, 166), (310, 166), (310, 166), (310, 166), (310, 166), (310, 166), (310, 166), (310, 166)]
input_ids shape: torch.Size([10, 285])
attention_mask shape: torch.Size([10, 285])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 285


===== Batch 936 =====
QIDs: [961, 961, 961, 961, 961, 961, 961, 961, 961, 961]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [586, 313, 590, 598, 482, 630, 464, 278, 679, 519]
image sizes: [(580, 185), (580, 185), (580, 185), (580, 185), (580, 185), (580, 185), (580, 185), (580, 185), (580, 185), (580, 185)]
input_ids shape: torch.Size([10, 500])
attention_mask shape: torch.Size([10, 500])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 500


===== Batch 937 =====
QIDs: [505, 505, 505, 505, 505, 505, 505, 505, 505, 505]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [311, 277, 305, 318, 294, 302, 286, 265, 310, 310]
image sizes: [(383, 153), (383, 153), (383, 153), (383, 153), (383, 153), (383, 153), (383, 153), (383, 153), (383, 153), (383, 153)]
input_ids shape: torch.Size([10, 285])
attention_mask shape: torch.Size([10, 285])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 285


===== Batch 938 =====
QIDs: [233, 233, 233, 233, 233, 233, 233, 233, 233, 233]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [822, 451, 806, 806, 704, 769, 713, 285, 885, 771]
image sizes: [(684, 668), (684, 668), (684, 668), (684, 668), (684, 668), (684, 668), (684, 668), (684, 668), (684, 668), (684, 668)]
input_ids shape: torch.Size([10, 1034])
attention_mask shape: torch.Size([10, 1034])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1034


===== Batch 939 =====
QIDs: [1199, 1199, 1199, 1199, 1199, 1199, 1199, 1199, 1199, 1199]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [924, 820, 897, 924, 870, 912, 891, 754, 939, 910]
image sizes: [(774, 346), (774, 346), (774, 346), (774, 346), (774, 346), (774, 346), (774, 346), (774, 346), (774, 346), (774, 346)]
input_ids shape: torch.Size([10, 1044])
attention_mask shape: torch.Size([10, 1044])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1044


===== Batch 940 =====
QIDs: [1479, 1479, 1479, 1479, 1479, 1479, 1479, 1479, 1479, 1479]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [350, 173, 329, 359, 304, 332, 265, 124, 392, 345]
image sizes: [(591, 378), (591, 378), (591, 378), (591, 378), (591, 378), (591, 378), (591, 378), (591, 378), (591, 378), (591, 378)]
input_ids shape: torch.Size([10, 482])
attention_mask shape: torch.Size([10, 482])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 482


===== Batch 941 =====
QIDs: [1057, 1057, 1057, 1057, 1057, 1057, 1057, 1057, 1057, 1057]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [2289, 1015, 2087, 2250, 1713, 2018, 1653, 601, 2191, 2155]
image sizes: [(1272, 690), (1272, 690), (1272, 690), (1272, 690), (1272, 690), (1272, 690), (1272, 690), (1272, 690), (1272, 690), (1272, 690)]
input_ids shape: torch.Size([10, 2105])
attention_mask shape: torch.Size([10, 2105])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2105


===== Batch 942 =====
QIDs: [903, 903, 903, 903, 903, 903, 903, 903, 903, 903]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [92, 65, 97, 99, 88, 98, 90, 48, 98, 106]
image sizes: [(560, 242), (560, 242), (560, 242), (560, 242), (560, 242), (560, 242), (560, 242), (560, 242), (560, 242), (560, 242)]
input_ids shape: torch.Size([10, 252])
attention_mask shape: torch.Size([10, 252])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 252


===== Batch 943 =====
QIDs: [1673, 1673, 1673, 1673, 1673, 1673, 1673, 1673, 1673, 1673]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [526, 355, 526, 544, 447, 504, 433, 283, 525, 521]
image sizes: [(632, 244), (632, 244), (632, 244), (632, 244), (632, 244), (632, 244), (632, 244), (632, 244), (632, 244), (632, 244)]
input_ids shape: torch.Size([10, 462])
attention_mask shape: torch.Size([10, 462])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 462


===== Batch 944 =====
QIDs: [490, 490, 490, 490, 490, 490, 490, 490, 490, 490]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [304, 249, 301, 323, 324, 293, 278, 216, 305, 299]
image sizes: [(496, 220), (496, 220), (496, 220), (496, 220), (496, 220), (496, 220), (496, 220), (496, 220), (496, 220), (496, 220)]
input_ids shape: torch.Size([10, 369])
attention_mask shape: torch.Size([10, 369])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 369


===== Batch 945 =====
QIDs: [871, 871, 871, 871, 871, 871, 871, 871, 871, 871]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [212, 140, 219, 243, 195, 219, 185, 133, 209, 223]
image sizes: [(721, 313), (721, 313), (721, 313), (721, 313), (721, 313), (721, 313), (721, 313), (721, 313), (721, 313), (721, 313)]
input_ids shape: torch.Size([10, 435])
attention_mask shape: torch.Size([10, 435])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 435


===== Batch 946 =====
QIDs: [274, 274, 274, 274, 274, 274, 274, 274, 274, 274]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [255, 142, 233, 215, 224, 210, 187, 95, 255, 241]
image sizes: [(472, 444), (472, 444), (472, 444), (472, 444), (472, 444), (472, 444), (472, 444), (472, 444), (472, 444), (472, 444)]
input_ids shape: torch.Size([10, 424])
attention_mask shape: torch.Size([10, 424])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 424


===== Batch 947 =====
QIDs: [1660, 1660, 1660, 1660, 1660, 1660, 1660, 1660, 1660, 1660]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [158, 90, 153, 163, 137, 161, 132, 60, 167, 148]
image sizes: [(1016, 224), (1016, 224), (1016, 224), (1016, 224), (1016, 224), (1016, 224), (1016, 224), (1016, 224), (1016, 224), (1016, 224)]
input_ids shape: torch.Size([10, 388])
attention_mask shape: torch.Size([10, 388])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 388


===== Batch 948 =====
QIDs: [1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608, 1608]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [81, 56, 79, 87, 69, 77, 82, 40, 109, 87]
image sizes: [(1640, 1290), (1640, 1290), (1640, 1290), (1640, 1290), (1640, 1290), (1640, 1290), (1640, 1290), (1640, 1290), (1640, 1290), (1640, 1290)]
input_ids shape: torch.Size([10, 2786])
attention_mask shape: torch.Size([10, 2786])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2786


===== Batch 949 =====
QIDs: [1330, 1330, 1330, 1330, 1330, 1330, 1330, 1330, 1330, 1330]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [268, 158, 251, 276, 212, 245, 249, 101, 309, 278]
image sizes: [(492, 720), (492, 720), (492, 720), (492, 720), (492, 720), (492, 720), (492, 720), (492, 720), (492, 720), (492, 720)]
input_ids shape: torch.Size([10, 627])
attention_mask shape: torch.Size([10, 627])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 627


===== Batch 950 =====
QIDs: [364, 364, 364, 364, 364, 364, 364, 364, 364, 364]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [251, 180, 245, 269, 249, 289, 239, 125, 305, 239]
image sizes: [(188, 155), (188, 155), (188, 155), (188, 155), (188, 155), (188, 155), (188, 155), (188, 155), (188, 155), (188, 155)]
input_ids shape: torch.Size([10, 218])
attention_mask shape: torch.Size([10, 218])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 218


===== Batch 951 =====
QIDs: [1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [77, 56, 75, 72, 73, 79, 71, 46, 79, 61]
image sizes: [(762, 632), (762, 632), (762, 632), (762, 632), (762, 632), (762, 632), (762, 632), (762, 632), (762, 632), (762, 632)]
input_ids shape: torch.Size([10, 689])
attention_mask shape: torch.Size([10, 689])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 689


===== Batch 952 =====
QIDs: [341, 341, 341, 341, 341, 341, 341, 341, 341, 341]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [569, 459, 569, 594, 530, 561, 506, 361, 560, 523]
image sizes: [(129, 222), (129, 222), (129, 222), (129, 222), (129, 222), (129, 222), (129, 222), (129, 222), (129, 222), (129, 222)]
input_ids shape: torch.Size([10, 470])
attention_mask shape: torch.Size([10, 470])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 470


===== Batch 953 =====
QIDs: [271, 271, 271, 271, 271, 271, 271, 271, 271, 271]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [111, 77, 116, 93, 105, 113, 110, 64, 129, 104]
image sizes: [(148, 109), (148, 109), (148, 109), (148, 109), (148, 109), (148, 109), (148, 109), (148, 109), (148, 109), (148, 109)]
input_ids shape: torch.Size([10, 105])
attention_mask shape: torch.Size([10, 105])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 105


===== Batch 954 =====
QIDs: [979, 979, 979, 979, 979, 979, 979, 979, 979, 979]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [445, 221, 376, 373, 375, 405, 410, 165, 441, 462]
image sizes: [(500, 216), (500, 216), (500, 216), (500, 216), (500, 216), (500, 216), (500, 216), (500, 216), (500, 216), (500, 216)]
input_ids shape: torch.Size([10, 427])
attention_mask shape: torch.Size([10, 427])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 427


===== Batch 955 =====
QIDs: [476, 476, 476, 476, 476, 476, 476, 476, 476, 476]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [102, 80, 102, 100, 93, 99, 95, 71, 97, 104]
image sizes: [(697, 652), (697, 652), (697, 652), (697, 652), (697, 652), (697, 652), (697, 652), (697, 652), (697, 652), (697, 652)]
input_ids shape: torch.Size([10, 671])
attention_mask shape: torch.Size([10, 671])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 671


===== Batch 956 =====
QIDs: [222, 222, 222, 222, 222, 222, 222, 222, 222, 222]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [249, 112, 239, 230, 229, 242, 226, 89, 276, 219]
image sizes: [(292, 438), (292, 438), (292, 438), (292, 438), (292, 438), (292, 438), (292, 438), (292, 438), (292, 438), (292, 438)]
input_ids shape: torch.Size([10, 336])
attention_mask shape: torch.Size([10, 336])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 336


===== Batch 957 =====
QIDs: [900, 900, 900, 900, 900, 900, 900, 900, 900, 900]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [900, 736, 858, 881, 841, 858, 863, 689, 900, 870]
image sizes: [(958, 620), (958, 620), (958, 620), (958, 620), (958, 620), (958, 620), (958, 620), (958, 620), (958, 620), (958, 620)]
input_ids shape: torch.Size([10, 1274])
attention_mask shape: torch.Size([10, 1274])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1274


===== Batch 958 =====
QIDs: [532, 532, 532, 532, 532, 532, 532, 532, 532, 532]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [599, 534, 588, 618, 589, 598, 580, 504, 603, 594]
image sizes: [(554, 178), (554, 178), (554, 178), (554, 178), (554, 178), (554, 178), (554, 178), (554, 178), (554, 178), (554, 178)]
input_ids shape: torch.Size([10, 533])
attention_mask shape: torch.Size([10, 533])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 533


===== Batch 959 =====
QIDs: [666, 666, 666, 666, 666, 666, 666, 666, 666, 666]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [89, 68, 94, 83, 91, 105, 72, 53, 94, 102]
image sizes: [(1063, 1213), (1063, 1213), (1063, 1213), (1063, 1213), (1063, 1213), (1063, 1213), (1063, 1213), (1063, 1213), (1063, 1213), (1063, 1213)]
input_ids shape: torch.Size([10, 1703])
attention_mask shape: torch.Size([10, 1703])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1703


===== Batch 960 =====
QIDs: [942, 942, 942, 942, 942, 942, 942, 942, 942, 942]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [121, 82, 104, 111, 97, 107, 88, 76, 117, 118]
image sizes: [(252, 154), (252, 154), (252, 154), (252, 154), (252, 154), (252, 154), (252, 154), (252, 154), (252, 154), (252, 154)]
input_ids shape: torch.Size([10, 131])
attention_mask shape: torch.Size([10, 131])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 131


===== Batch 961 =====
QIDs: [1528, 1528, 1528, 1528, 1528, 1528, 1528, 1528, 1528, 1528]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [477, 394, 476, 499, 422, 476, 473, 359, 494, 468]
image sizes: [(1227, 373), (1227, 373), (1227, 373), (1227, 373), (1227, 373), (1227, 373), (1227, 373), (1227, 373), (1227, 373), (1227, 373)]
input_ids shape: torch.Size([10, 882])
attention_mask shape: torch.Size([10, 882])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 882


===== Batch 962 =====
QIDs: [256, 256, 256, 256, 256, 256, 256, 256, 256, 256]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [412, 263, 402, 420, 371, 387, 360, 231, 459, 409]
image sizes: [(444, 246), (444, 246), (444, 246), (444, 246), (444, 246), (444, 246), (444, 246), (444, 246), (444, 246), (444, 246)]
input_ids shape: torch.Size([10, 380])
attention_mask shape: torch.Size([10, 380])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 380


===== Batch 963 =====
QIDs: [1543, 1543, 1543, 1543, 1543, 1543, 1543, 1543, 1543, 1543]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [449, 332, 451, 435, 399, 434, 385, 316, 470, 443]
image sizes: [(828, 229), (828, 229), (828, 229), (828, 229), (828, 229), (828, 229), (828, 229), (828, 229), (828, 229), (828, 229)]
input_ids shape: torch.Size([10, 501])
attention_mask shape: torch.Size([10, 501])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 501


===== Batch 964 =====
QIDs: [949, 949, 949, 949, 949, 949, 949, 949, 949, 949]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [135, 99, 123, 130, 124, 137, 112, 77, 138, 124]
image sizes: [(1525, 129), (1525, 129), (1525, 129), (1525, 129), (1525, 129), (1525, 129), (1525, 129), (1525, 129), (1525, 129), (1525, 129)]
input_ids shape: torch.Size([10, 367])
attention_mask shape: torch.Size([10, 367])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 367


===== Batch 965 =====
QIDs: [1171, 1171, 1171, 1171, 1171, 1171, 1171, 1171, 1171, 1171]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [320, 169, 327, 337, 303, 323, 306, 119, 352, 360]
image sizes: [(286, 187), (286, 187), (286, 187), (286, 187), (286, 187), (286, 187), (286, 187), (286, 187), (286, 187), (286, 187)]
input_ids shape: torch.Size([10, 269])
attention_mask shape: torch.Size([10, 269])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 269


===== Batch 966 =====
QIDs: [563, 563, 563, 563, 563, 563, 563, 563, 563, 563]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [230, 196, 285, 255, 285, 232, 222, 171, 275, 224]
image sizes: [(313, 243), (313, 243), (313, 243), (313, 243), (313, 243), (313, 243), (313, 243), (313, 243), (313, 243), (313, 243)]
input_ids shape: torch.Size([10, 315])
attention_mask shape: torch.Size([10, 315])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 315


===== Batch 967 =====
QIDs: [510, 510, 510, 510, 510, 510, 510, 510, 510, 510]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [382, 368, 385, 412, 380, 390, 373, 357, 376, 374]
image sizes: [(324, 252), (324, 252), (324, 252), (324, 252), (324, 252), (324, 252), (324, 252), (324, 252), (324, 252), (324, 252)]
input_ids shape: torch.Size([10, 382])
attention_mask shape: torch.Size([10, 382])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 382


===== Batch 968 =====
QIDs: [1683, 1683, 1683, 1683, 1683, 1683, 1683, 1683, 1683, 1683]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1806, 822, 1633, 1782, 1336, 1635, 1434, 561, 1733, 1896]
image sizes: [(1354, 860), (1354, 860), (1354, 860), (1354, 860), (1354, 860), (1354, 860), (1354, 860), (1354, 860), (1354, 860), (1354, 860)]
input_ids shape: torch.Size([10, 2313])
attention_mask shape: torch.Size([10, 2313])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2313


===== Batch 969 =====
QIDs: [1264, 1264, 1264, 1264, 1264, 1264, 1264, 1264, 1264, 1264]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [74, 46, 70, 65, 75, 69, 71, 43, 73, 64]
image sizes: [(497, 315), (497, 315), (497, 315), (497, 315), (497, 315), (497, 315), (497, 315), (497, 315), (497, 315), (497, 315)]
input_ids shape: torch.Size([10, 267])
attention_mask shape: torch.Size([10, 267])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 267


===== Batch 970 =====
QIDs: [1031, 1031, 1031, 1031, 1031, 1031, 1031, 1031, 1031, 1031]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [332, 185, 337, 351, 282, 316, 269, 110, 361, 333]
image sizes: [(190, 224), (190, 224), (190, 224), (190, 224), (190, 224), (190, 224), (190, 224), (190, 224), (190, 224), (190, 224)]
input_ids shape: torch.Size([10, 245])
attention_mask shape: torch.Size([10, 245])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 245


===== Batch 971 =====
QIDs: [850, 850, 850, 850, 850, 850, 850, 850, 850, 850]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [265, 188, 242, 257, 240, 244, 235, 161, 271, 260]
image sizes: [(417, 264), (417, 264), (417, 264), (417, 264), (417, 264), (417, 264), (417, 264), (417, 264), (417, 264), (417, 264)]
input_ids shape: torch.Size([10, 321])
attention_mask shape: torch.Size([10, 321])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 321


===== Batch 972 =====
QIDs: [1654, 1654, 1654, 1654, 1654, 1654, 1654, 1654, 1654, 1654]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [290, 163, 282, 277, 223, 252, 296, 134, 285, 273]
image sizes: [(802, 342), (802, 342), (802, 342), (802, 342), (802, 342), (802, 342), (802, 342), (802, 342), (802, 342), (802, 342)]
input_ids shape: torch.Size([10, 568])
attention_mask shape: torch.Size([10, 568])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 568


===== Batch 973 =====
QIDs: [1567, 1567, 1567, 1567, 1567, 1567, 1567, 1567, 1567, 1567]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [883, 608, 882, 1030, 848, 782, 839, 415, 836, 935]
image sizes: [(510, 179), (510, 179), (510, 179), (510, 179), (510, 179), (510, 179), (510, 179), (510, 179), (510, 179), (510, 179)]
input_ids shape: torch.Size([10, 681])
attention_mask shape: torch.Size([10, 681])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 681


===== Batch 974 =====
QIDs: [1018, 1018, 1018, 1018, 1018, 1018, 1018, 1018, 1018, 1018]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [480, 318, 426, 440, 423, 434, 379, 264, 478, 430]
image sizes: [(396, 292), (396, 292), (396, 292), (396, 292), (396, 292), (396, 292), (396, 292), (396, 292), (396, 292), (396, 292)]
input_ids shape: torch.Size([10, 419])
attention_mask shape: torch.Size([10, 419])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 419


===== Batch 975 =====
QIDs: [1712, 1712, 1712, 1712, 1712, 1712, 1712, 1712, 1712, 1712]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [957, 511, 940, 1015, 834, 924, 874, 350, 1077, 1020]
image sizes: [(847, 301), (847, 301), (847, 301), (847, 301), (847, 301), (847, 301), (847, 301), (847, 301), (847, 301), (847, 301)]
input_ids shape: torch.Size([10, 880])
attention_mask shape: torch.Size([10, 880])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 880


===== Batch 976 =====
QIDs: [1301, 1301, 1301, 1301, 1301, 1301, 1301, 1301, 1301, 1301]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [277, 152, 245, 243, 229, 243, 213, 108, 309, 226]
image sizes: [(658, 410), (658, 410), (658, 410), (658, 410), (658, 410), (658, 410), (658, 410), (658, 410), (658, 410), (658, 410)]
input_ids shape: torch.Size([10, 518])
attention_mask shape: torch.Size([10, 518])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 518


===== Batch 977 =====
QIDs: [829, 829, 829, 829, 829, 829, 829, 829, 829, 829]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [365, 277, 364, 381, 339, 378, 373, 224, 383, 343]
image sizes: [(606, 537), (606, 537), (606, 537), (606, 537), (606, 537), (606, 537), (606, 537), (606, 537), (606, 537), (606, 537)]
input_ids shape: torch.Size([10, 700])
attention_mask shape: torch.Size([10, 700])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 700


===== Batch 978 =====
QIDs: [1284, 1284, 1284, 1284, 1284, 1284, 1284, 1284, 1284, 1284]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [116, 75, 116, 109, 99, 115, 114, 60, 135, 111]
image sizes: [(728, 196), (728, 196), (728, 196), (728, 196), (728, 196), (728, 196), (728, 196), (728, 196), (728, 196), (728, 196)]
input_ids shape: torch.Size([10, 274])
attention_mask shape: torch.Size([10, 274])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 274


===== Batch 979 =====
QIDs: [1079, 1079, 1079, 1079, 1079, 1079, 1079, 1079, 1079, 1079]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [697, 384, 693, 661, 759, 674, 647, 275, 709, 796]
image sizes: [(1614, 918), (1614, 918), (1614, 918), (1614, 918), (1614, 918), (1614, 918), (1614, 918), (1614, 918), (1614, 918), (1614, 918)]
input_ids shape: torch.Size([10, 2319])
attention_mask shape: torch.Size([10, 2319])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2319


===== Batch 980 =====
QIDs: [1060, 1060, 1060, 1060, 1060, 1060, 1060, 1060, 1060, 1060]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1720, 680, 1544, 1639, 1442, 1518, 1488, 449, 1702, 1552]
image sizes: [(874, 825), (874, 825), (874, 825), (874, 825), (874, 825), (874, 825), (874, 825), (874, 825), (874, 825), (874, 825)]
input_ids shape: torch.Size([10, 1688])
attention_mask shape: torch.Size([10, 1688])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1688


===== Batch 981 =====
QIDs: [1659, 1659, 1659, 1659, 1659, 1659, 1659, 1659, 1659, 1659]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [394, 241, 419, 476, 407, 449, 381, 188, 430, 420]
image sizes: [(508, 246), (508, 246), (508, 246), (508, 246), (508, 246), (508, 246), (508, 246), (508, 246), (508, 246), (508, 246)]
input_ids shape: torch.Size([10, 444])
attention_mask shape: torch.Size([10, 444])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 444


===== Batch 982 =====
QIDs: [822, 822, 822, 822, 822, 822, 822, 822, 822, 822]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [296, 225, 309, 290, 282, 285, 291, 201, 306, 305]
image sizes: [(811, 202), (811, 202), (811, 202), (811, 202), (811, 202), (811, 202), (811, 202), (811, 202), (811, 202), (811, 202)]
input_ids shape: torch.Size([10, 430])
attention_mask shape: torch.Size([10, 430])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 430


===== Batch 983 =====
QIDs: [659, 659, 659, 659, 659, 659, 659, 659, 659, 659]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [123, 84, 114, 119, 133, 113, 139, 73, 133, 126]
image sizes: [(1289, 1096), (1289, 1096), (1289, 1096), (1289, 1096), (1289, 1096), (1289, 1096), (1289, 1096), (1289, 1096), (1289, 1096), (1289, 1096)]
input_ids shape: torch.Size([10, 1916])
attention_mask shape: torch.Size([10, 1916])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1916


===== Batch 984 =====
QIDs: [487, 487, 487, 487, 487, 487, 487, 487, 487, 487]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [419, 258, 429, 472, 392, 447, 411, 194, 468, 477]
image sizes: [(556, 242), (556, 242), (556, 242), (556, 242), (556, 242), (556, 242), (556, 242), (556, 242), (556, 242), (556, 242)]
input_ids shape: torch.Size([10, 459])
attention_mask shape: torch.Size([10, 459])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 459


===== Batch 985 =====
QIDs: [1319, 1319, 1319, 1319, 1319, 1319, 1319, 1319, 1319, 1319]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [355, 146, 376, 335, 324, 349, 327, 121, 402, 253]
image sizes: [(1262, 634), (1262, 634), (1262, 634), (1262, 634), (1262, 634), (1262, 634), (1262, 634), (1262, 634), (1262, 634), (1262, 634)]
input_ids shape: torch.Size([10, 1260])
attention_mask shape: torch.Size([10, 1260])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1260


===== Batch 986 =====
QIDs: [342, 342, 342, 342, 342, 342, 342, 342, 342, 342]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [178, 151, 177, 175, 183, 179, 181, 124, 200, 190]
image sizes: [(759, 216), (759, 216), (759, 216), (759, 216), (759, 216), (759, 216), (759, 216), (759, 216), (759, 216), (759, 216)]
input_ids shape: torch.Size([10, 372])
attention_mask shape: torch.Size([10, 372])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 372


===== Batch 987 =====
QIDs: [1311, 1311, 1311, 1311, 1311, 1311, 1311, 1311, 1311, 1311]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [674, 319, 614, 630, 540, 611, 598, 218, 667, 636]
image sizes: [(512, 114), (512, 114), (512, 114), (512, 114), (512, 114), (512, 114), (512, 114), (512, 114), (512, 114), (512, 114)]
input_ids shape: torch.Size([10, 437])
attention_mask shape: torch.Size([10, 437])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 437


===== Batch 988 =====
QIDs: [38, 38, 38, 38, 38, 38, 38, 38, 38, 38]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [517, 244, 522, 537, 422, 522, 413, 173, 541, 508]
image sizes: [(1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560)]
input_ids shape: torch.Size([10, 6557])
attention_mask shape: torch.Size([10, 6557])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 6557


===== Batch 989 =====
QIDs: [542, 542, 542, 542, 542, 542, 542, 542, 542, 542]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [401, 388, 399, 434, 421, 401, 393, 380, 403, 417]
image sizes: [(254, 129), (254, 129), (254, 129), (254, 129), (254, 129), (254, 129), (254, 129), (254, 129), (254, 129), (254, 129)]
input_ids shape: torch.Size([10, 332])
attention_mask shape: torch.Size([10, 332])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 332


===== Batch 990 =====
QIDs: [238, 238, 238, 238, 238, 238, 238, 238, 238, 238]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [413, 252, 389, 454, 338, 406, 399, 201, 420, 387]
image sizes: [(1148, 568), (1148, 568), (1148, 568), (1148, 568), (1148, 568), (1148, 568), (1148, 568), (1148, 568), (1148, 568), (1148, 568)]
input_ids shape: torch.Size([10, 1087])
attention_mask shape: torch.Size([10, 1087])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1087


===== Batch 991 =====
QIDs: [525, 525, 525, 525, 525, 525, 525, 525, 525, 525]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [307, 248, 312, 333, 320, 310, 305, 221, 347, 315]
image sizes: [(362, 198), (362, 198), (362, 198), (362, 198), (362, 198), (362, 198), (362, 198), (362, 198), (362, 198), (362, 198)]
input_ids shape: torch.Size([10, 325])
attention_mask shape: torch.Size([10, 325])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 325


===== Batch 992 =====
QIDs: [773, 773, 773, 773, 773, 773, 773, 773, 773, 773]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [428, 236, 468, 448, 359, 452, 331, 151, 473, 402]
image sizes: [(965, 192), (965, 192), (965, 192), (965, 192), (965, 192), (965, 192), (965, 192), (965, 192), (965, 192), (965, 192)]
input_ids shape: torch.Size([10, 488])
attention_mask shape: torch.Size([10, 488])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 488


===== Batch 993 =====
QIDs: [1559, 1559, 1559, 1559, 1559, 1559, 1559, 1559, 1559, 1559]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [299, 203, 313, 307, 348, 286, 270, 178, 317, 278]
image sizes: [(576, 262), (576, 262), (576, 262), (576, 262), (576, 262), (576, 262), (576, 262), (576, 262), (576, 262), (576, 262)]
input_ids shape: torch.Size([10, 417])
attention_mask shape: torch.Size([10, 417])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 417


===== Batch 994 =====
QIDs: [373, 373, 373, 373, 373, 373, 373, 373, 373, 373]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [325, 224, 311, 327, 311, 313, 338, 172, 327, 306]
image sizes: [(429, 167), (429, 167), (429, 167), (429, 167), (429, 167), (429, 167), (429, 167), (429, 167), (429, 167), (429, 167)]
input_ids shape: torch.Size([10, 346])
attention_mask shape: torch.Size([10, 346])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 346


===== Batch 995 =====
QIDs: [623, 623, 623, 623, 623, 623, 623, 623, 623, 623]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [809, 469, 793, 832, 687, 767, 713, 335, 845, 835]
image sizes: [(338, 269), (338, 269), (338, 269), (338, 269), (338, 269), (338, 269), (338, 269), (338, 269), (338, 269), (338, 269)]
input_ids shape: torch.Size([10, 591])
attention_mask shape: torch.Size([10, 591])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 591


===== Batch 996 =====
QIDs: [330, 330, 330, 330, 330, 330, 330, 330, 330, 330]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [263, 144, 255, 298, 283, 284, 330, 116, 340, 342]
image sizes: [(1178, 796), (1178, 796), (1178, 796), (1178, 796), (1178, 796), (1178, 796), (1178, 796), (1178, 796), (1178, 796), (1178, 796)]
input_ids shape: torch.Size([10, 1390])
attention_mask shape: torch.Size([10, 1390])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1390


===== Batch 997 =====
QIDs: [450, 450, 450, 450, 450, 450, 450, 450, 450, 450]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [73, 45, 78, 64, 61, 86, 66, 38, 84, 73]
image sizes: [(700, 290), (700, 290), (700, 290), (700, 290), (700, 290), (700, 290), (700, 290), (700, 290), (700, 290), (700, 290)]
input_ids shape: torch.Size([10, 311])
attention_mask shape: torch.Size([10, 311])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 311


===== Batch 998 =====
QIDs: [1279, 1279, 1279, 1279, 1279, 1279, 1279, 1279, 1279, 1279]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [263, 114, 277, 305, 250, 282, 243, 94, 317, 230]
image sizes: [(310, 429), (310, 429), (310, 429), (310, 429), (310, 429), (310, 429), (310, 429), (310, 429), (310, 429), (310, 429)]
input_ids shape: torch.Size([10, 342])
attention_mask shape: torch.Size([10, 342])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 342


===== Batch 999 =====
QIDs: [941, 941, 941, 941, 941, 941, 941, 941, 941, 941]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [543, 255, 455, 566, 446, 387, 386, 171, 429, 547]
image sizes: [(425, 104), (425, 104), (425, 104), (425, 104), (425, 104), (425, 104), (425, 104), (425, 104), (425, 104), (425, 104)]
input_ids shape: torch.Size([10, 330])
attention_mask shape: torch.Size([10, 330])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 330


===== Batch 1000 =====
QIDs: [263, 263, 263, 263, 263, 263, 263, 263, 263, 263]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [187, 149, 186, 184, 171, 191, 164, 133, 184, 166]
image sizes: [(346, 235), (346, 235), (346, 235), (346, 235), (346, 235), (346, 235), (346, 235), (346, 235), (346, 235), (346, 235)]
input_ids shape: torch.Size([10, 269])
attention_mask shape: torch.Size([10, 269])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 269


===== Batch 1001 =====
QIDs: [1532, 1532, 1532, 1532, 1532, 1532, 1532, 1532, 1532, 1532]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [275, 193, 290, 300, 276, 286, 266, 163, 317, 305]
image sizes: [(1264, 304), (1264, 304), (1264, 304), (1264, 304), (1264, 304), (1264, 304), (1264, 304), (1264, 304), (1264, 304), (1264, 304)]
input_ids shape: torch.Size([10, 711])
attention_mask shape: torch.Size([10, 711])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 711


===== Batch 1002 =====
QIDs: [1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517, 1517]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [430, 262, 458, 470, 397, 432, 386, 230, 473, 484]
image sizes: [(511, 465), (511, 465), (511, 465), (511, 465), (511, 465), (511, 465), (511, 465), (511, 465), (511, 465), (511, 465)]
input_ids shape: torch.Size([10, 614])
attention_mask shape: torch.Size([10, 614])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 614


===== Batch 1003 =====
QIDs: [333, 333, 333, 333, 333, 333, 333, 333, 333, 333]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [686, 369, 711, 685, 650, 700, 701, 240, 736, 644]
image sizes: [(504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331)]
input_ids shape: torch.Size([10, 648])
attention_mask shape: torch.Size([10, 648])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 648


===== Batch 1004 =====
QIDs: [1030, 1030, 1030, 1030, 1030, 1030, 1030, 1030, 1030, 1030]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [240, 145, 271, 269, 214, 251, 240, 99, 289, 268]
image sizes: [(594, 424), (594, 424), (594, 424), (594, 424), (594, 424), (594, 424), (594, 424), (594, 424), (594, 424), (594, 424)]
input_ids shape: torch.Size([10, 471])
attention_mask shape: torch.Size([10, 471])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 471


===== Batch 1005 =====
QIDs: [943, 943, 943, 943, 943, 943, 943, 943, 943, 943]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [87, 55, 88, 92, 76, 90, 63, 41, 95, 73]
image sizes: [(1640, 1290), (1640, 1290), (1640, 1290), (1640, 1290), (1640, 1290), (1640, 1290), (1640, 1290), (1640, 1290), (1640, 1290), (1640, 1290)]
input_ids shape: torch.Size([10, 2783])
attention_mask shape: torch.Size([10, 2783])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2783


===== Batch 1006 =====
QIDs: [1241, 1241, 1241, 1241, 1241, 1241, 1241, 1241, 1241, 1241]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [200, 114, 192, 220, 164, 173, 153, 99, 176, 191]
image sizes: [(878, 608), (878, 608), (878, 608), (878, 608), (878, 608), (878, 608), (878, 608), (878, 608), (878, 608), (878, 608)]
input_ids shape: torch.Size([10, 808])
attention_mask shape: torch.Size([10, 808])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 808


===== Batch 1007 =====
QIDs: [967, 967, 967, 967, 967, 967, 967, 967, 967, 967]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [308, 168, 348, 335, 227, 354, 279, 108, 391, 308]
image sizes: [(557, 219), (557, 219), (557, 219), (557, 219), (557, 219), (557, 219), (557, 219), (557, 219), (557, 219), (557, 219)]
input_ids shape: torch.Size([10, 350])
attention_mask shape: torch.Size([10, 350])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 350


===== Batch 1008 =====
QIDs: [663, 663, 663, 663, 663, 663, 663, 663, 663, 663]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [516, 255, 473, 516, 427, 414, 442, 161, 511, 463]
image sizes: [(480, 303), (480, 303), (480, 303), (480, 303), (480, 303), (480, 303), (480, 303), (480, 303), (480, 303), (480, 303)]
input_ids shape: torch.Size([10, 476])
attention_mask shape: torch.Size([10, 476])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 476


===== Batch 1009 =====
QIDs: [1666, 1666, 1666, 1666, 1666, 1666, 1666, 1666, 1666, 1666]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [325, 173, 277, 300, 246, 263, 293, 145, 299, 315]
image sizes: [(696, 506), (696, 506), (696, 506), (696, 506), (696, 506), (696, 506), (696, 506), (696, 506), (696, 506), (696, 506)]
input_ids shape: torch.Size([10, 631])
attention_mask shape: torch.Size([10, 631])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 631


===== Batch 1010 =====
QIDs: [633, 633, 633, 633, 633, 633, 633, 633, 633, 633]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [225, 140, 226, 257, 191, 211, 185, 119, 218, 210]
image sizes: [(172, 88), (172, 88), (172, 88), (172, 88), (172, 88), (172, 88), (172, 88), (172, 88), (172, 88), (172, 88)]
input_ids shape: torch.Size([10, 179])
attention_mask shape: torch.Size([10, 179])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 179


===== Batch 1011 =====
QIDs: [64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [195, 117, 190, 207, 158, 187, 194, 92, 222, 190]
image sizes: [(1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560)]
input_ids shape: torch.Size([10, 6421])
attention_mask shape: torch.Size([10, 6421])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 6421


===== Batch 1012 =====
QIDs: [1485, 1485, 1485, 1485, 1485, 1485, 1485, 1485, 1485, 1485]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [345, 177, 335, 408, 307, 349, 312, 125, 415, 342]
image sizes: [(267, 400), (267, 400), (267, 400), (267, 400), (267, 400), (267, 400), (267, 400), (267, 400), (267, 400), (267, 400)]
input_ids shape: torch.Size([10, 355])
attention_mask shape: torch.Size([10, 355])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 355


===== Batch 1013 =====
QIDs: [1112, 1112, 1112, 1112, 1112, 1112, 1112, 1112, 1112, 1112]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [443, 245, 444, 469, 415, 434, 347, 205, 501, 396]
image sizes: [(609, 305), (609, 305), (609, 305), (609, 305), (609, 305), (609, 305), (609, 305), (609, 305), (609, 305), (609, 305)]
input_ids shape: torch.Size([10, 534])
attention_mask shape: torch.Size([10, 534])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 534


===== Batch 1014 =====
QIDs: [1422, 1422, 1422, 1422, 1422, 1422, 1422, 1422, 1422, 1422]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [378, 306, 390, 434, 423, 394, 374, 273, 407, 385]
image sizes: [(286, 219), (286, 219), (286, 219), (286, 219), (286, 219), (286, 219), (286, 219), (286, 219), (286, 219), (286, 219)]
input_ids shape: torch.Size([10, 378])
attention_mask shape: torch.Size([10, 378])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 378


===== Batch 1015 =====
QIDs: [54, 54, 54, 54, 54, 54, 54, 54, 54, 54]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [212, 104, 212, 208, 171, 205, 211, 96, 240, 216]
image sizes: [(2560, 1708), (2560, 1708), (2560, 1708), (2560, 1708), (2560, 1708), (2560, 1708), (2560, 1708), (2560, 1708), (2560, 1708), (2560, 1708)]
input_ids shape: torch.Size([10, 5715])
attention_mask shape: torch.Size([10, 5715])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 5715


===== Batch 1016 =====
QIDs: [1447, 1447, 1447, 1447, 1447, 1447, 1447, 1447, 1447, 1447]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [208, 147, 198, 189, 179, 200, 195, 130, 218, 190]
image sizes: [(248, 124), (248, 124), (248, 124), (248, 124), (248, 124), (248, 124), (248, 124), (248, 124), (248, 124), (248, 124)]
input_ids shape: torch.Size([10, 206])
attention_mask shape: torch.Size([10, 206])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 206


===== Batch 1017 =====
QIDs: [1694, 1694, 1694, 1694, 1694, 1694, 1694, 1694, 1694, 1694]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [530, 262, 479, 490, 430, 457, 442, 195, 530, 495]
image sizes: [(632, 91), (632, 91), (632, 91), (632, 91), (632, 91), (632, 91), (632, 91), (632, 91), (632, 91), (632, 91)]
input_ids shape: torch.Size([10, 335])
attention_mask shape: torch.Size([10, 335])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 335


===== Batch 1018 =====
QIDs: [325, 325, 325, 325, 325, 325, 325, 325, 325, 325]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [737, 371, 707, 664, 582, 646, 570, 223, 752, 696]
image sizes: [(504, 332), (504, 332), (504, 332), (504, 332), (504, 332), (504, 332), (504, 332), (504, 332), (504, 332), (504, 332)]
input_ids shape: torch.Size([10, 596])
attention_mask shape: torch.Size([10, 596])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 596


===== Batch 1019 =====
QIDs: [322, 322, 322, 322, 322, 322, 322, 322, 322, 322]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [458, 193, 451, 448, 441, 406, 458, 160, 517, 447]
image sizes: [(504, 414), (504, 414), (504, 414), (504, 414), (504, 414), (504, 414), (504, 414), (504, 414), (504, 414), (504, 414)]
input_ids shape: torch.Size([10, 556])
attention_mask shape: torch.Size([10, 556])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 556


===== Batch 1020 =====
QIDs: [1366, 1366, 1366, 1366, 1366, 1366, 1366, 1366, 1366, 1366]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [259, 115, 252, 241, 236, 260, 257, 94, 264, 208]
image sizes: [(200, 143), (200, 143), (200, 143), (200, 143), (200, 143), (200, 143), (200, 143), (200, 143), (200, 143), (200, 143)]
input_ids shape: torch.Size([10, 211])
attention_mask shape: torch.Size([10, 211])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 211


===== Batch 1021 =====
QIDs: [1295, 1295, 1295, 1295, 1295, 1295, 1295, 1295, 1295, 1295]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [603, 255, 544, 520, 498, 617, 465, 181, 636, 672]
image sizes: [(950, 706), (950, 706), (950, 706), (950, 706), (950, 706), (950, 706), (950, 706), (950, 706), (950, 706), (950, 706)]
input_ids shape: torch.Size([10, 1188])
attention_mask shape: torch.Size([10, 1188])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1188


===== Batch 1022 =====
QIDs: [1639, 1639, 1639, 1639, 1639, 1639, 1639, 1639, 1639, 1639]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [194, 106, 203, 211, 194, 181, 171, 78, 217, 175]
image sizes: [(394, 402), (394, 402), (394, 402), (394, 402), (394, 402), (394, 402), (394, 402), (394, 402), (394, 402), (394, 402)]
input_ids shape: torch.Size([10, 323])
attention_mask shape: torch.Size([10, 323])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 323


===== Batch 1023 =====
QIDs: [1123, 1123, 1123, 1123, 1123, 1123, 1123, 1123, 1123, 1123]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [162, 105, 152, 167, 157, 167, 144, 96, 182, 306]
image sizes: [(706, 852), (706, 852), (706, 852), (706, 852), (706, 852), (706, 852), (706, 852), (706, 852), (706, 852), (706, 852)]
input_ids shape: torch.Size([10, 874])
attention_mask shape: torch.Size([10, 874])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 874


===== Batch 1024 =====
QIDs: [884, 884, 884, 884, 884, 884, 884, 884, 884, 884]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [983, 486, 934, 1152, 791, 964, 842, 288, 1022, 991]
image sizes: [(555, 218), (555, 218), (555, 218), (555, 218), (555, 218), (555, 218), (555, 218), (555, 218), (555, 218), (555, 218)]
input_ids shape: torch.Size([10, 687])
attention_mask shape: torch.Size([10, 687])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 687


===== Batch 1025 =====
QIDs: [1185, 1185, 1185, 1185, 1185, 1185, 1185, 1185, 1185, 1185]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [441, 230, 450, 483, 400, 406, 440, 167, 493, 489]
image sizes: [(2492, 2124), (2492, 2124), (2492, 2124), (2492, 2124), (2492, 2124), (2492, 2124), (2492, 2124), (2492, 2124), (2492, 2124), (2492, 2124)]
input_ids shape: torch.Size([10, 7042])
attention_mask shape: torch.Size([10, 7042])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 7042


===== Batch 1026 =====
QIDs: [1570, 1570, 1570, 1570, 1570, 1570, 1570, 1570, 1570, 1570]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [789, 513, 728, 797, 678, 702, 646, 438, 741, 716]
image sizes: [(400, 496), (400, 496), (400, 496), (400, 496), (400, 496), (400, 496), (400, 496), (400, 496), (400, 496), (400, 496)]
input_ids shape: torch.Size([10, 679])
attention_mask shape: torch.Size([10, 679])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 679


===== Batch 1027 =====
QIDs: [1487, 1487, 1487, 1487, 1487, 1487, 1487, 1487, 1487, 1487]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [468, 239, 399, 471, 370, 410, 419, 168, 469, 507]
image sizes: [(812, 1144), (812, 1144), (812, 1144), (812, 1144), (812, 1144), (812, 1144), (812, 1144), (812, 1144), (812, 1144), (812, 1144)]
input_ids shape: torch.Size([10, 1461])
attention_mask shape: torch.Size([10, 1461])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1461


===== Batch 1028 =====
QIDs: [577, 577, 577, 577, 577, 577, 577, 577, 577, 577]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [272, 192, 252, 270, 355, 258, 246, 158, 288, 243]
image sizes: [(265, 250), (265, 250), (265, 250), (265, 250), (265, 250), (265, 250), (265, 250), (265, 250), (265, 250), (265, 250)]
input_ids shape: torch.Size([10, 301])
attention_mask shape: torch.Size([10, 301])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 301


===== Batch 1029 =====
QIDs: [452, 452, 452, 452, 452, 452, 452, 452, 452, 452]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [526, 246, 566, 540, 540, 524, 621, 212, 608, 585]
image sizes: [(1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548)]
input_ids shape: torch.Size([10, 4094])
attention_mask shape: torch.Size([10, 4094])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 4094


===== Batch 1030 =====
QIDs: [1417, 1417, 1417, 1417, 1417, 1417, 1417, 1417, 1417, 1417]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [616, 334, 587, 591, 554, 562, 556, 277, 670, 601]
image sizes: [(708, 694), (708, 694), (708, 694), (708, 694), (708, 694), (708, 694), (708, 694), (708, 694), (708, 694), (708, 694)]
input_ids shape: torch.Size([10, 992])
attention_mask shape: torch.Size([10, 992])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 992


===== Batch 1031 =====
QIDs: [79, 79, 79, 79, 79, 79, 79, 79, 79, 79]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [222, 121, 198, 188, 149, 153, 175, 110, 198, 206]
image sizes: [(436, 271), (436, 271), (436, 271), (436, 271), (436, 271), (436, 271), (436, 271), (436, 271), (436, 271), (436, 271)]
input_ids shape: torch.Size([10, 304])
attention_mask shape: torch.Size([10, 304])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 304


===== Batch 1032 =====
QIDs: [1503, 1503, 1503, 1503, 1503, 1503, 1503, 1503, 1503, 1503]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [387, 179, 395, 377, 339, 381, 342, 131, 385, 359]
image sizes: [(547, 356), (547, 356), (547, 356), (547, 356), (547, 356), (547, 356), (547, 356), (547, 356), (547, 356), (547, 356)]
input_ids shape: torch.Size([10, 475])
attention_mask shape: torch.Size([10, 475])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 475


===== Batch 1033 =====
QIDs: [752, 752, 752, 752, 752, 752, 752, 752, 752, 752]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1099, 668, 1325, 1265, 1058, 1209, 1013, 483, 1250, 1033]
image sizes: [(728, 246), (728, 246), (728, 246), (728, 246), (728, 246), (728, 246), (728, 246), (728, 246), (728, 246), (728, 246)]
input_ids shape: torch.Size([10, 965])
attention_mask shape: torch.Size([10, 965])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 965


===== Batch 1034 =====
QIDs: [1699, 1699, 1699, 1699, 1699, 1699, 1699, 1699, 1699, 1699]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [385, 254, 351, 379, 367, 356, 351, 212, 386, 385]
image sizes: [(434, 84), (434, 84), (434, 84), (434, 84), (434, 84), (434, 84), (434, 84), (434, 84), (434, 84), (434, 84)]
input_ids shape: torch.Size([10, 323])
attention_mask shape: torch.Size([10, 323])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 323


===== Batch 1035 =====
QIDs: [499, 499, 499, 499, 499, 499, 499, 499, 499, 499]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [197, 135, 196, 198, 203, 189, 174, 112, 196, 308]
image sizes: [(345, 174), (345, 174), (345, 174), (345, 174), (345, 174), (345, 174), (345, 174), (345, 174), (345, 174), (345, 174)]
input_ids shape: torch.Size([10, 251])
attention_mask shape: torch.Size([10, 251])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 251


===== Batch 1036 =====
QIDs: [1384, 1384, 1384, 1384, 1384, 1384, 1384, 1384, 1384, 1384]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [351, 173, 333, 320, 299, 320, 307, 112, 363, 363]
image sizes: [(1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548)]
input_ids shape: torch.Size([10, 3932])
attention_mask shape: torch.Size([10, 3932])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 3932


===== Batch 1037 =====
QIDs: [372, 372, 372, 372, 372, 372, 372, 372, 372, 372]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [220, 114, 177, 200, 184, 184, 160, 91, 231, 198]
image sizes: [(700, 192), (700, 192), (700, 192), (700, 192), (700, 192), (700, 192), (700, 192), (700, 192), (700, 192), (700, 192)]
input_ids shape: torch.Size([10, 289])
attention_mask shape: torch.Size([10, 289])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 289


===== Batch 1038 =====
QIDs: [26, 26, 26, 26, 26, 26, 26, 26, 26, 26]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [290, 195, 319, 313, 272, 308, 296, 166, 337, 331]
image sizes: [(448, 193), (448, 193), (448, 193), (448, 193), (448, 193), (448, 193), (448, 193), (448, 193), (448, 193), (448, 193)]
input_ids shape: torch.Size([10, 347])
attention_mask shape: torch.Size([10, 347])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 347


===== Batch 1039 =====
QIDs: [297, 297, 297, 297, 297, 297, 297, 297, 297, 297]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [392, 374, 376, 390, 501, 376, 384, 332, 378, 378]
image sizes: [(537, 450), (537, 450), (537, 450), (537, 450), (537, 450), (537, 450), (537, 450), (537, 450), (537, 450), (537, 450)]
input_ids shape: torch.Size([10, 544])
attention_mask shape: torch.Size([10, 544])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 544


===== Batch 1040 =====
QIDs: [1309, 1309, 1309, 1309, 1309, 1309, 1309, 1309, 1309, 1309]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [145, 103, 136, 147, 142, 152, 148, 90, 167, 156]
image sizes: [(178, 85), (178, 85), (178, 85), (178, 85), (178, 85), (178, 85), (178, 85), (178, 85), (178, 85), (178, 85)]
input_ids shape: torch.Size([10, 139])
attention_mask shape: torch.Size([10, 139])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 139


===== Batch 1041 =====
QIDs: [886, 886, 886, 886, 886, 886, 886, 886, 886, 886]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [383, 245, 340, 355, 349, 355, 348, 197, 424, 356]
image sizes: [(558, 385), (558, 385), (558, 385), (558, 385), (558, 385), (558, 385), (558, 385), (558, 385), (558, 385), (558, 385)]
input_ids shape: torch.Size([10, 546])
attention_mask shape: torch.Size([10, 546])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 546


===== Batch 1042 =====
QIDs: [1647, 1647, 1647, 1647, 1647, 1647, 1647, 1647, 1647, 1647]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [320, 159, 301, 293, 271, 312, 257, 96, 345, 287]
image sizes: [(341, 367), (341, 367), (341, 367), (341, 367), (341, 367), (341, 367), (341, 367), (341, 367), (341, 367), (341, 367)]
input_ids shape: torch.Size([10, 325])
attention_mask shape: torch.Size([10, 325])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 325


===== Batch 1043 =====
QIDs: [302, 302, 302, 302, 302, 302, 302, 302, 302, 302]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [472, 222, 470, 459, 475, 447, 423, 156, 503, 424]
image sizes: [(1008, 750), (1008, 750), (1008, 750), (1008, 750), (1008, 750), (1008, 750), (1008, 750), (1008, 750), (1008, 750), (1008, 750)]
input_ids shape: torch.Size([10, 1243])
attention_mask shape: torch.Size([10, 1243])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1243


===== Batch 1044 =====
QIDs: [734, 734, 734, 734, 734, 734, 734, 734, 734, 734]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [337, 195, 347, 378, 336, 352, 308, 140, 370, 391]
image sizes: [(500, 447), (500, 447), (500, 447), (500, 447), (500, 447), (500, 447), (500, 447), (500, 447), (500, 447), (500, 447)]
input_ids shape: torch.Size([10, 495])
attention_mask shape: torch.Size([10, 495])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 495


===== Batch 1045 =====
QIDs: [1111, 1111, 1111, 1111, 1111, 1111, 1111, 1111, 1111, 1111]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [2362, 1064, 2265, 2414, 2155, 2180, 1962, 717, 2467, 2277]
image sizes: [(963, 291), (963, 291), (963, 291), (963, 291), (963, 291), (963, 291), (963, 291), (963, 291), (963, 291), (963, 291)]
input_ids shape: torch.Size([10, 1520])
attention_mask shape: torch.Size([10, 1520])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1520


===== Batch 1046 =====
QIDs: [1289, 1289, 1289, 1289, 1289, 1289, 1289, 1289, 1289, 1289]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [457, 204, 473, 470, 354, 418, 409, 144, 478, 402]
image sizes: [(1018, 480), (1018, 480), (1018, 480), (1018, 480), (1018, 480), (1018, 480), (1018, 480), (1018, 480), (1018, 480), (1018, 480)]
input_ids shape: torch.Size([10, 863])
attention_mask shape: torch.Size([10, 863])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 863


===== Batch 1047 =====
QIDs: [1214, 1214, 1214, 1214, 1214, 1214, 1214, 1214, 1214, 1214]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [588, 367, 558, 610, 541, 526, 491, 268, 601, 570]
image sizes: [(441, 268), (441, 268), (441, 268), (441, 268), (441, 268), (441, 268), (441, 268), (441, 268), (441, 268), (441, 268)]
input_ids shape: torch.Size([10, 521])
attention_mask shape: torch.Size([10, 521])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 521


===== Batch 1048 =====
QIDs: [317, 317, 317, 317, 317, 317, 317, 317, 317, 317]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [166, 90, 171, 157, 146, 179, 162, 79, 194, 149]
image sizes: [(1192, 1276), (1192, 1276), (1192, 1276), (1192, 1276), (1192, 1276), (1192, 1276), (1192, 1276), (1192, 1276), (1192, 1276), (1192, 1276)]
input_ids shape: torch.Size([10, 2108])
attention_mask shape: torch.Size([10, 2108])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2108


===== Batch 1049 =====
QIDs: [1619, 1619, 1619, 1619, 1619, 1619, 1619, 1619, 1619, 1619]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [480, 220, 447, 476, 445, 448, 446, 163, 546, 380]
image sizes: [(159, 76), (159, 76), (159, 76), (159, 76), (159, 76), (159, 76), (159, 76), (159, 76), (159, 76), (159, 76)]
input_ids shape: torch.Size([10, 333])
attention_mask shape: torch.Size([10, 333])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 333


===== Batch 1050 =====
QIDs: [1456, 1456, 1456, 1456, 1456, 1456, 1456, 1456, 1456, 1456]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [609, 346, 556, 547, 500, 569, 568, 246, 686, 660]
image sizes: [(593, 220), (593, 220), (593, 220), (593, 220), (593, 220), (593, 220), (593, 220), (593, 220), (593, 220), (593, 220)]
input_ids shape: torch.Size([10, 509])
attention_mask shape: torch.Size([10, 509])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 509


===== Batch 1051 =====
QIDs: [1594, 1594, 1594, 1594, 1594, 1594, 1594, 1594, 1594, 1594]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [596, 488, 609, 617, 562, 584, 619, 425, 626, 563]
image sizes: [(631, 344), (631, 344), (631, 344), (631, 344), (631, 344), (631, 344), (631, 344), (631, 344), (631, 344), (631, 344)]
input_ids shape: torch.Size([10, 701])
attention_mask shape: torch.Size([10, 701])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 701


===== Batch 1052 =====
QIDs: [1586, 1586, 1586, 1586, 1586, 1586, 1586, 1586, 1586, 1586]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [301, 231, 272, 311, 266, 276, 267, 196, 277, 268]
image sizes: [(566, 763), (566, 763), (566, 763), (566, 763), (566, 763), (566, 763), (566, 763), (566, 763), (566, 763), (566, 763)]
input_ids shape: torch.Size([10, 768])
attention_mask shape: torch.Size([10, 768])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 768


===== Batch 1053 =====
QIDs: [1671, 1671, 1671, 1671, 1671, 1671, 1671, 1671, 1671, 1671]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [291, 197, 275, 281, 255, 271, 338, 174, 312, 281]
image sizes: [(2354, 240), (2354, 240), (2354, 240), (2354, 240), (2354, 240), (2354, 240), (2354, 240), (2354, 240), (2354, 240), (2354, 240)]
input_ids shape: torch.Size([10, 1021])
attention_mask shape: torch.Size([10, 1021])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1021


===== Batch 1054 =====
QIDs: [1446, 1446, 1446, 1446, 1446, 1446, 1446, 1446, 1446, 1446]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [330, 208, 321, 349, 288, 320, 316, 158, 374, 339]
image sizes: [(656, 191), (656, 191), (656, 191), (656, 191), (656, 191), (656, 191), (656, 191), (656, 191), (656, 191), (656, 191)]
input_ids shape: torch.Size([10, 392])
attention_mask shape: torch.Size([10, 392])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 392


===== Batch 1055 =====
QIDs: [278, 278, 278, 278, 278, 278, 278, 278, 278, 278]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [504, 311, 552, 508, 437, 430, 435, 221, 453, 481]
image sizes: [(604, 231), (604, 231), (604, 231), (604, 231), (604, 231), (604, 231), (604, 231), (604, 231), (604, 231), (604, 231)]
input_ids shape: torch.Size([10, 495])
attention_mask shape: torch.Size([10, 495])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 495


===== Batch 1056 =====
QIDs: [1375, 1375, 1375, 1375, 1375, 1375, 1375, 1375, 1375, 1375]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [357, 179, 360, 360, 372, 378, 434, 136, 384, 397]
image sizes: [(1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604)]
input_ids shape: torch.Size([10, 4119])
attention_mask shape: torch.Size([10, 4119])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 4119


===== Batch 1057 =====
QIDs: [1576, 1576, 1576, 1576, 1576, 1576, 1576, 1576, 1576, 1576]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [384, 319, 380, 400, 357, 389, 368, 284, 402, 409]
image sizes: [(553, 529), (553, 529), (553, 529), (553, 529), (553, 529), (553, 529), (553, 529), (553, 529), (553, 529), (553, 529)]
input_ids shape: torch.Size([10, 620])
attention_mask shape: torch.Size([10, 620])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 620


===== Batch 1058 =====
QIDs: [793, 793, 793, 793, 793, 793, 793, 793, 793, 793]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [398, 239, 375, 400, 327, 373, 326, 187, 382, 429]
image sizes: [(1227, 244), (1227, 244), (1227, 244), (1227, 244), (1227, 244), (1227, 244), (1227, 244), (1227, 244), (1227, 244), (1227, 244)]
input_ids shape: torch.Size([10, 634])
attention_mask shape: torch.Size([10, 634])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 634


===== Batch 1059 =====
QIDs: [212, 212, 212, 212, 212, 212, 212, 212, 212, 212]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [738, 321, 701, 705, 667, 707, 686, 199, 863, 686]
image sizes: [(1068, 1428), (1068, 1428), (1068, 1428), (1068, 1428), (1068, 1428), (1068, 1428), (1068, 1428), (1068, 1428), (1068, 1428), (1068, 1428)]
input_ids shape: torch.Size([10, 2350])
attention_mask shape: torch.Size([10, 2350])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2350


===== Batch 1060 =====
QIDs: [1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477, 1477]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [907, 409, 867, 979, 742, 847, 727, 264, 953, 860]
image sizes: [(726, 454), (726, 454), (726, 454), (726, 454), (726, 454), (726, 454), (726, 454), (726, 454), (726, 454), (726, 454)]
input_ids shape: torch.Size([10, 858])
attention_mask shape: torch.Size([10, 858])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 858


===== Batch 1061 =====
QIDs: [1420, 1420, 1420, 1420, 1420, 1420, 1420, 1420, 1420, 1420]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [314, 211, 291, 356, 326, 309, 344, 178, 316, 338]
image sizes: [(1062, 855), (1062, 855), (1062, 855), (1062, 855), (1062, 855), (1062, 855), (1062, 855), (1062, 855), (1062, 855), (1062, 855)]
input_ids shape: torch.Size([10, 1427])
attention_mask shape: torch.Size([10, 1427])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1427


===== Batch 1062 =====
QIDs: [567, 567, 567, 567, 567, 567, 567, 567, 567, 567]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [339, 265, 321, 350, 330, 336, 322, 209, 332, 317]
image sizes: [(399, 231), (399, 231), (399, 231), (399, 231), (399, 231), (399, 231), (399, 231), (399, 231), (399, 231), (399, 231)]
input_ids shape: torch.Size([10, 359])
attention_mask shape: torch.Size([10, 359])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 359


===== Batch 1063 =====
QIDs: [788, 788, 788, 788, 788, 788, 788, 788, 788, 788]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [443, 258, 433, 460, 394, 444, 388, 206, 442, 418]
image sizes: [(1228, 501), (1228, 501), (1228, 501), (1228, 501), (1228, 501), (1228, 501), (1228, 501), (1228, 501), (1228, 501), (1228, 501)]
input_ids shape: torch.Size([10, 1061])
attention_mask shape: torch.Size([10, 1061])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1061


===== Batch 1064 =====
QIDs: [1726, 1726, 1726, 1726, 1726, 1726, 1726, 1726, 1726, 1726]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [194, 179, 189, 199, 185, 187, 193, 157, 196, 197]
image sizes: [(820, 400), (820, 400), (820, 400), (820, 400), (820, 400), (820, 400), (820, 400), (820, 400), (820, 400), (820, 400)]
input_ids shape: torch.Size([10, 533])
attention_mask shape: torch.Size([10, 533])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 533


===== Batch 1065 =====
QIDs: [918, 918, 918, 918, 918, 918, 918, 918, 918, 918]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [555, 363, 566, 602, 503, 567, 497, 318, 579, 531]
image sizes: [(136, 173), (136, 173), (136, 173), (136, 173), (136, 173), (136, 173), (136, 173), (136, 173), (136, 173), (136, 173)]
input_ids shape: torch.Size([10, 396])
attention_mask shape: torch.Size([10, 396])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 396


===== Batch 1066 =====
QIDs: [383, 383, 383, 383, 383, 383, 383, 383, 383, 383]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [576, 306, 574, 638, 469, 596, 496, 222, 602, 614]
image sizes: [(990, 994), (990, 994), (990, 994), (990, 994), (990, 994), (990, 994), (990, 994), (990, 994), (990, 994), (990, 994)]
input_ids shape: torch.Size([10, 1560])
attention_mask shape: torch.Size([10, 1560])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1560


===== Batch 1067 =====
QIDs: [340, 340, 340, 340, 340, 340, 340, 340, 340, 340]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [475, 288, 461, 520, 437, 455, 417, 249, 459, 488]
image sizes: [(1708, 1044), (1708, 1044), (1708, 1044), (1708, 1044), (1708, 1044), (1708, 1044), (1708, 1044), (1708, 1044), (1708, 1044), (1708, 1044)]
input_ids shape: torch.Size([10, 2508])
attention_mask shape: torch.Size([10, 2508])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2508


===== Batch 1068 =====
QIDs: [86, 86, 86, 86, 86, 86, 86, 86, 86, 86]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1320, 643, 1277, 1372, 1078, 1266, 1375, 548, 1348, 1149]
image sizes: [(532, 70), (532, 70), (532, 70), (532, 70), (532, 70), (532, 70), (532, 70), (532, 70), (532, 70), (532, 70)]
input_ids shape: torch.Size([10, 801])
attention_mask shape: torch.Size([10, 801])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 801


===== Batch 1069 =====
QIDs: [300, 300, 300, 300, 300, 300, 300, 300, 300, 300]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [297, 133, 275, 278, 258, 276, 269, 105, 305, 285]
image sizes: [(342, 259), (342, 259), (342, 259), (342, 259), (342, 259), (342, 259), (342, 259), (342, 259), (342, 259), (342, 259)]
input_ids shape: torch.Size([10, 282])
attention_mask shape: torch.Size([10, 282])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 282


===== Batch 1070 =====
QIDs: [1310, 1310, 1310, 1310, 1310, 1310, 1310, 1310, 1310, 1310]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [263, 149, 247, 249, 210, 245, 232, 127, 242, 269]
image sizes: [(170, 298), (170, 298), (170, 298), (170, 298), (170, 298), (170, 298), (170, 298), (170, 298), (170, 298), (170, 298)]
input_ids shape: torch.Size([10, 240])
attention_mask shape: torch.Size([10, 240])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 240


===== Batch 1071 =====
QIDs: [1119, 1119, 1119, 1119, 1119, 1119, 1119, 1119, 1119, 1119]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [316, 173, 309, 323, 237, 284, 267, 97, 315, 341]
image sizes: [(1334, 798), (1334, 798), (1334, 798), (1334, 798), (1334, 798), (1334, 798), (1334, 798), (1334, 798), (1334, 798), (1334, 798)]
input_ids shape: torch.Size([10, 1512])
attention_mask shape: torch.Size([10, 1512])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1512


===== Batch 1072 =====
QIDs: [655, 655, 655, 655, 655, 655, 655, 655, 655, 655]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [123, 76, 111, 123, 116, 122, 102, 58, 132, 142]
image sizes: [(836, 551), (836, 551), (836, 551), (836, 551), (836, 551), (836, 551), (836, 551), (836, 551), (836, 551), (836, 551)]
input_ids shape: torch.Size([10, 679])
attention_mask shape: torch.Size([10, 679])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 679


===== Batch 1073 =====
QIDs: [261, 261, 261, 261, 261, 261, 261, 261, 261, 261]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [195, 111, 172, 197, 164, 175, 160, 89, 180, 210]
image sizes: [(496, 758), (496, 758), (496, 758), (496, 758), (496, 758), (496, 758), (496, 758), (496, 758), (496, 758), (496, 758)]
input_ids shape: torch.Size([10, 604])
attention_mask shape: torch.Size([10, 604])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 604


===== Batch 1074 =====
QIDs: [93, 93, 93, 93, 93, 93, 93, 93, 93, 93]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [596, 445, 619, 671, 621, 583, 515, 391, 674, 723]
image sizes: [(494, 273), (494, 273), (494, 273), (494, 273), (494, 273), (494, 273), (494, 273), (494, 273), (494, 273), (494, 273)]
input_ids shape: torch.Size([10, 589])
attention_mask shape: torch.Size([10, 589])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 589


===== Batch 1075 =====
QIDs: [791, 791, 791, 791, 791, 791, 791, 791, 791, 791]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [321, 209, 340, 326, 305, 316, 274, 168, 339, 349]
image sizes: [(1201, 312), (1201, 312), (1201, 312), (1201, 312), (1201, 312), (1201, 312), (1201, 312), (1201, 312), (1201, 312), (1201, 312)]
input_ids shape: torch.Size([10, 696])
attention_mask shape: torch.Size([10, 696])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 696


===== Batch 1076 =====
QIDs: [491, 491, 491, 491, 491, 491, 491, 491, 491, 491]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [353, 199, 339, 350, 297, 331, 284, 149, 337, 352]
image sizes: [(1237, 316), (1237, 316), (1237, 316), (1237, 316), (1237, 316), (1237, 316), (1237, 316), (1237, 316), (1237, 316), (1237, 316)]
input_ids shape: torch.Size([10, 706])
attention_mask shape: torch.Size([10, 706])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 706


===== Batch 1077 =====
QIDs: [877, 877, 877, 877, 877, 877, 877, 877, 877, 877]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [356, 221, 384, 373, 328, 369, 306, 167, 396, 379]
image sizes: [(524, 177), (524, 177), (524, 177), (524, 177), (524, 177), (524, 177), (524, 177), (524, 177), (524, 177), (524, 177)]
input_ids shape: torch.Size([10, 351])
attention_mask shape: torch.Size([10, 351])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 351


===== Batch 1078 =====
QIDs: [905, 905, 905, 905, 905, 905, 905, 905, 905, 905]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [547, 420, 499, 547, 486, 518, 507, 373, 529, 535]
image sizes: [(812, 808), (812, 808), (812, 808), (812, 808), (812, 808), (812, 808), (812, 808), (812, 808), (812, 808), (812, 808)]
input_ids shape: torch.Size([10, 1173])
attention_mask shape: torch.Size([10, 1173])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1173


===== Batch 1079 =====
QIDs: [1421, 1421, 1421, 1421, 1421, 1421, 1421, 1421, 1421, 1421]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [540, 336, 522, 553, 577, 521, 503, 297, 563, 507]
image sizes: [(968, 760), (968, 760), (968, 760), (968, 760), (968, 760), (968, 760), (968, 760), (968, 760), (968, 760), (968, 760)]
input_ids shape: torch.Size([10, 1275])
attention_mask shape: torch.Size([10, 1275])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1275


===== Batch 1080 =====
QIDs: [291, 291, 291, 291, 291, 291, 291, 291, 291, 291]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [186, 106, 183, 182, 525, 188, 182, 91, 197, 174]
image sizes: [(138, 120), (138, 120), (138, 120), (138, 120), (138, 120), (138, 120), (138, 120), (138, 120), (138, 120), (138, 120)]
input_ids shape: torch.Size([10, 227])
attention_mask shape: torch.Size([10, 227])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 227


===== Batch 1081 =====
QIDs: [1547, 1547, 1547, 1547, 1547, 1547, 1547, 1547, 1547, 1547]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [356, 243, 332, 365, 329, 326, 350, 184, 348, 359]
image sizes: [(612, 576), (612, 576), (612, 576), (612, 576), (612, 576), (612, 576), (612, 576), (612, 576), (612, 576), (612, 576)]
input_ids shape: torch.Size([10, 711])
attention_mask shape: torch.Size([10, 711])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 711


===== Batch 1082 =====
QIDs: [42, 42, 42, 42, 42, 42, 42, 42, 42, 42]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [685, 344, 667, 755, 565, 630, 579, 233, 661, 610]
image sizes: [(1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560)]
input_ids shape: torch.Size([10, 6648])
attention_mask shape: torch.Size([10, 6648])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 6648


===== Batch 1083 =====
QIDs: [1502, 1502, 1502, 1502, 1502, 1502, 1502, 1502, 1502, 1502]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [655, 537, 686, 880, 787, 687, 618, 481, 698, 690]
image sizes: [(921, 204), (921, 204), (921, 204), (921, 204), (921, 204), (921, 204), (921, 204), (921, 204), (921, 204), (921, 204)]
input_ids shape: torch.Size([10, 755])
attention_mask shape: torch.Size([10, 755])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 755


===== Batch 1084 =====
QIDs: [1538, 1538, 1538, 1538, 1538, 1538, 1538, 1538, 1538, 1538]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [268, 190, 261, 282, 316, 246, 223, 169, 287, 256]
image sizes: [(466, 294), (466, 294), (466, 294), (466, 294), (466, 294), (466, 294), (466, 294), (466, 294), (466, 294), (466, 294)]
input_ids shape: torch.Size([10, 378])
attention_mask shape: torch.Size([10, 378])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 378


===== Batch 1085 =====
QIDs: [1019, 1019, 1019, 1019, 1019, 1019, 1019, 1019, 1019, 1019]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [269, 191, 279, 267, 237, 256, 243, 149, 286, 277]
image sizes: [(772, 420), (772, 420), (772, 420), (772, 420), (772, 420), (772, 420), (772, 420), (772, 420), (772, 420), (772, 420)]
input_ids shape: torch.Size([10, 616])
attention_mask shape: torch.Size([10, 616])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 616


===== Batch 1086 =====
QIDs: [1162, 1162, 1162, 1162, 1162, 1162, 1162, 1162, 1162, 1162]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [317, 208, 322, 332, 268, 316, 344, 167, 325, 288]
image sizes: [(342, 310), (342, 310), (342, 310), (342, 310), (342, 310), (342, 310), (342, 310), (342, 310), (342, 310), (342, 310)]
input_ids shape: torch.Size([10, 372])
attention_mask shape: torch.Size([10, 372])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 372


===== Batch 1087 =====
QIDs: [894, 894, 894, 894, 894, 894, 894, 894, 894, 894]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [807, 636, 897, 997, 835, 968, 753, 652, 921, 871]
image sizes: [(849, 552), (849, 552), (849, 552), (849, 552), (849, 552), (849, 552), (849, 552), (849, 552), (849, 552), (849, 552)]
input_ids shape: torch.Size([10, 1144])
attention_mask shape: torch.Size([10, 1144])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1144


===== Batch 1088 =====
QIDs: [39, 39, 39, 39, 39, 39, 39, 39, 39, 39]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [277, 142, 326, 272, 250, 283, 254, 109, 336, 280]
image sizes: [(800, 600), (800, 600), (800, 600), (800, 600), (800, 600), (800, 600), (800, 600), (800, 600), (800, 600), (800, 600)]
input_ids shape: torch.Size([10, 809])
attention_mask shape: torch.Size([10, 809])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 809


===== Batch 1089 =====
QIDs: [264, 264, 264, 264, 264, 264, 264, 264, 264, 264]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [87, 76, 89, 90, 89, 90, 94, 74, 93, 88]
image sizes: [(285, 136), (285, 136), (285, 136), (285, 136), (285, 136), (285, 136), (285, 136), (285, 136), (285, 136), (285, 136)]
input_ids shape: torch.Size([10, 124])
attention_mask shape: torch.Size([10, 124])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 124


===== Batch 1090 =====
QIDs: [827, 827, 827, 827, 827, 827, 827, 827, 827, 827]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [227, 191, 217, 228, 207, 217, 212, 177, 225, 219]
image sizes: [(322, 385), (322, 385), (322, 385), (322, 385), (322, 385), (322, 385), (322, 385), (322, 385), (322, 385), (322, 385)]
input_ids shape: torch.Size([10, 320])
attention_mask shape: torch.Size([10, 320])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 320


===== Batch 1091 =====
QIDs: [1653, 1653, 1653, 1653, 1653, 1653, 1653, 1653, 1653, 1653]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [688, 375, 647, 718, 595, 593, 562, 327, 730, 706]
image sizes: [(676, 294), (676, 294), (676, 294), (676, 294), (676, 294), (676, 294), (676, 294), (676, 294), (676, 294), (676, 294)]
input_ids shape: torch.Size([10, 585])
attention_mask shape: torch.Size([10, 585])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 585


===== Batch 1092 =====
QIDs: [673, 673, 673, 673, 673, 673, 673, 673, 673, 673]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [684, 286, 638, 716, 580, 630, 596, 202, 758, 649]
image sizes: [(678, 564), (678, 564), (678, 564), (678, 564), (678, 564), (678, 564), (678, 564), (678, 564), (678, 564), (678, 564)]
input_ids shape: torch.Size([10, 827])
attention_mask shape: torch.Size([10, 827])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 827


===== Batch 1093 =====
QIDs: [53, 53, 53, 53, 53, 53, 53, 53, 53, 53]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [439, 245, 421, 439, 435, 391, 375, 165, 457, 429]
image sizes: [(593, 630), (593, 630), (593, 630), (593, 630), (593, 630), (593, 630), (593, 630), (593, 630), (593, 630), (593, 630)]
input_ids shape: torch.Size([10, 710])
attention_mask shape: torch.Size([10, 710])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 710


===== Batch 1094 =====
QIDs: [126, 126, 126, 126, 126, 126, 126, 126, 126, 126]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [229, 147, 229, 231, 212, 238, 194, 105, 247, 226]
image sizes: [(628, 450), (628, 450), (628, 450), (628, 450), (628, 450), (628, 450), (628, 450), (628, 450), (628, 450), (628, 450)]
input_ids shape: torch.Size([10, 499])
attention_mask shape: torch.Size([10, 499])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 499


===== Batch 1095 =====
QIDs: [824, 824, 824, 824, 824, 824, 824, 824, 824, 824]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [120, 76, 110, 142, 92, 105, 99, 54, 133, 112]
image sizes: [(706, 207), (706, 207), (706, 207), (706, 207), (706, 207), (706, 207), (706, 207), (706, 207), (706, 207), (706, 207)]
input_ids shape: torch.Size([10, 264])
attention_mask shape: torch.Size([10, 264])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 264


===== Batch 1096 =====
QIDs: [99, 99, 99, 99, 99, 99, 99, 99, 99, 99]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [716, 517, 719, 725, 726, 701, 681, 466, 710, 677]
image sizes: [(658, 149), (658, 149), (658, 149), (658, 149), (658, 149), (658, 149), (658, 149), (658, 149), (658, 149), (658, 149)]
input_ids shape: torch.Size([10, 638])
attention_mask shape: torch.Size([10, 638])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 638


===== Batch 1097 =====
QIDs: [1410, 1410, 1410, 1410, 1410, 1410, 1410, 1410, 1410, 1410]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [203, 191, 198, 206, 193, 202, 197, 182, 200, 201]
image sizes: [(383, 108), (383, 108), (383, 108), (383, 108), (383, 108), (383, 108), (383, 108), (383, 108), (383, 108), (383, 108)]
input_ids shape: torch.Size([10, 212])
attention_mask shape: torch.Size([10, 212])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 212


===== Batch 1098 =====
QIDs: [863, 863, 863, 863, 863, 863, 863, 863, 863, 863]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [793, 430, 752, 847, 718, 764, 754, 297, 874, 818]
image sizes: [(222, 230), (222, 230), (222, 230), (222, 230), (222, 230), (222, 230), (222, 230), (222, 230), (222, 230), (222, 230)]
input_ids shape: torch.Size([10, 509])
attention_mask shape: torch.Size([10, 509])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 509


===== Batch 1099 =====
QIDs: [1158, 1158, 1158, 1158, 1158, 1158, 1158, 1158, 1158, 1158]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [453, 306, 463, 483, 409, 475, 425, 222, 469, 474]
image sizes: [(628, 271), (628, 271), (628, 271), (628, 271), (628, 271), (628, 271), (628, 271), (628, 271), (628, 271), (628, 271)]
input_ids shape: torch.Size([10, 486])
attention_mask shape: torch.Size([10, 486])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 486


===== Batch 1100 =====
QIDs: [1627, 1627, 1627, 1627, 1627, 1627, 1627, 1627, 1627, 1627]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [157, 116, 145, 182, 163, 167, 133, 63, 161, 179]
image sizes: [(1530, 131), (1530, 131), (1530, 131), (1530, 131), (1530, 131), (1530, 131), (1530, 131), (1530, 131), (1530, 131), (1530, 131)]
input_ids shape: torch.Size([10, 384])
attention_mask shape: torch.Size([10, 384])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 384


===== Batch 1101 =====
QIDs: [1369, 1369, 1369, 1369, 1369, 1369, 1369, 1369, 1369, 1369]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [292, 149, 330, 316, 613, 306, 317, 114, 363, 346]
image sizes: [(782, 520), (782, 520), (782, 520), (782, 520), (782, 520), (782, 520), (782, 520), (782, 520), (782, 520), (782, 520)]
input_ids shape: torch.Size([10, 764])
attention_mask shape: torch.Size([10, 764])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 764


===== Batch 1102 =====
QIDs: [780, 780, 780, 780, 780, 780, 780, 780, 780, 780]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [435, 286, 428, 471, 348, 415, 433, 203, 424, 450]
image sizes: [(590, 606), (590, 606), (590, 606), (590, 606), (590, 606), (590, 606), (590, 606), (590, 606), (590, 606), (590, 606)]
input_ids shape: torch.Size([10, 719])
attention_mask shape: torch.Size([10, 719])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 719


===== Batch 1103 =====
QIDs: [308, 308, 308, 308, 308, 308, 308, 308, 308, 308]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [213, 104, 203, 209, 210, 205, 187, 93, 197, 216]
image sizes: [(319, 209), (319, 209), (319, 209), (319, 209), (319, 209), (319, 209), (319, 209), (319, 209), (319, 209), (319, 209)]
input_ids shape: torch.Size([10, 222])
attention_mask shape: torch.Size([10, 222])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 222


===== Batch 1104 =====
QIDs: [427, 427, 427, 427, 427, 427, 427, 427, 427, 427]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [337, 158, 348, 327, 337, 364, 323, 144, 334, 291]
image sizes: [(706, 536), (706, 536), (706, 536), (706, 536), (706, 536), (706, 536), (706, 536), (706, 536), (706, 536), (706, 536)]
input_ids shape: torch.Size([10, 690])
attention_mask shape: torch.Size([10, 690])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 690


===== Batch 1105 =====
QIDs: [890, 890, 890, 890, 890, 890, 890, 890, 890, 890]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [284, 227, 292, 273, 274, 277, 254, 194, 283, 312]
image sizes: [(608, 540), (608, 540), (608, 540), (608, 540), (608, 540), (608, 540), (608, 540), (608, 540), (608, 540), (608, 540)]
input_ids shape: torch.Size([10, 629])
attention_mask shape: torch.Size([10, 629])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 629


===== Batch 1106 =====
QIDs: [615, 615, 615, 615, 615, 615, 615, 615, 615, 615]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1291, 670, 1331, 1281, 1077, 1291, 992, 479, 1396, 1228]
image sizes: [(639, 149), (639, 149), (639, 149), (639, 149), (639, 149), (639, 149), (639, 149), (639, 149), (639, 149), (639, 149)]
input_ids shape: torch.Size([10, 864])
attention_mask shape: torch.Size([10, 864])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 864


===== Batch 1107 =====
QIDs: [1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029, 1029]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [308, 248, 297, 306, 291, 304, 284, 218, 314, 295]
image sizes: [(270, 336), (270, 336), (270, 336), (270, 336), (270, 336), (270, 336), (270, 336), (270, 336), (270, 336), (270, 336)]
input_ids shape: torch.Size([10, 305])
attention_mask shape: torch.Size([10, 305])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 305


===== Batch 1108 =====
QIDs: [891, 891, 891, 891, 891, 891, 891, 891, 891, 891]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [323, 219, 323, 311, 288, 330, 270, 163, 330, 337]
image sizes: [(868, 384), (868, 384), (868, 384), (868, 384), (868, 384), (868, 384), (868, 384), (868, 384), (868, 384), (868, 384)]
input_ids shape: torch.Size([10, 655])
attention_mask shape: torch.Size([10, 655])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 655


===== Batch 1109 =====
QIDs: [120, 120, 120, 120, 120, 120, 120, 120, 120, 120]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [202, 204, 205, 209, 204, 203, 210, 196, 201, 203]
image sizes: [(1330, 1050), (1330, 1050), (1330, 1050), (1330, 1050), (1330, 1050), (1330, 1050), (1330, 1050), (1330, 1050), (1330, 1050), (1330, 1050)]
input_ids shape: torch.Size([10, 1946])
attention_mask shape: torch.Size([10, 1946])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1946


===== Batch 1110 =====
QIDs: [548, 548, 548, 548, 548, 548, 548, 548, 548, 548]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [426, 369, 409, 417, 408, 427, 385, 343, 439, 480]
image sizes: [(460, 226), (460, 226), (460, 226), (460, 226), (460, 226), (460, 226), (460, 226), (460, 226), (460, 226), (460, 226)]
input_ids shape: torch.Size([10, 348])
attention_mask shape: torch.Size([10, 348])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 348


===== Batch 1111 =====
QIDs: [103, 103, 103, 103, 103, 103, 103, 103, 103, 103]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [439, 311, 409, 401, 402, 392, 399, 269, 457, 372]
image sizes: [(349, 361), (349, 361), (349, 361), (349, 361), (349, 361), (349, 361), (349, 361), (349, 361), (349, 361), (349, 361)]
input_ids shape: torch.Size([10, 475])
attention_mask shape: torch.Size([10, 475])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 475


===== Batch 1112 =====
QIDs: [1555, 1555, 1555, 1555, 1555, 1555, 1555, 1555, 1555, 1555]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [225, 166, 224, 233, 211, 202, 193, 151, 229, 218]
image sizes: [(358, 324), (358, 324), (358, 324), (358, 324), (358, 324), (358, 324), (358, 324), (358, 324), (358, 324), (358, 324)]
input_ids shape: torch.Size([10, 327])
attention_mask shape: torch.Size([10, 327])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 327


===== Batch 1113 =====
QIDs: [1378, 1378, 1378, 1378, 1378, 1378, 1378, 1378, 1378, 1378]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [239, 123, 247, 254, 259, 285, 275, 101, 269, 228]
image sizes: [(1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604)]
input_ids shape: torch.Size([10, 4032])
attention_mask shape: torch.Size([10, 4032])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 4032


===== Batch 1114 =====
QIDs: [882, 882, 882, 882, 882, 882, 882, 882, 882, 882]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [550, 504, 536, 566, 536, 557, 536, 483, 563, 566]
image sizes: [(538, 481), (538, 481), (538, 481), (538, 481), (538, 481), (538, 481), (538, 481), (538, 481), (538, 481), (538, 481)]
input_ids shape: torch.Size([10, 588])
attention_mask shape: torch.Size([10, 588])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 588


===== Batch 1115 =====
QIDs: [467, 467, 467, 467, 467, 467, 467, 467, 467, 467]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [251, 172, 239, 264, 233, 278, 256, 150, 301, 244]
image sizes: [(507, 201), (507, 201), (507, 201), (507, 201), (507, 201), (507, 201), (507, 201), (507, 201), (507, 201), (507, 201)]
input_ids shape: torch.Size([10, 318])
attention_mask shape: torch.Size([10, 318])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 318


===== Batch 1116 =====
QIDs: [976, 976, 976, 976, 976, 976, 976, 976, 976, 976]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [463, 307, 441, 473, 522, 454, 435, 225, 473, 499]
image sizes: [(408, 329), (408, 329), (408, 329), (408, 329), (408, 329), (408, 329), (408, 329), (408, 329), (408, 329), (408, 329)]
input_ids shape: torch.Size([10, 465])
attention_mask shape: torch.Size([10, 465])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 465


===== Batch 1117 =====
QIDs: [995, 995, 995, 995, 995, 995, 995, 995, 995, 995]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [521, 353, 484, 497, 477, 478, 525, 294, 536, 521]
image sizes: [(432, 381), (432, 381), (432, 381), (432, 381), (432, 381), (432, 381), (432, 381), (432, 381), (432, 381), (432, 381)]
input_ids shape: torch.Size([10, 581])
attention_mask shape: torch.Size([10, 581])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 581


===== Batch 1118 =====
QIDs: [501, 501, 501, 501, 501, 501, 501, 501, 501, 501]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [248, 164, 245, 257, 244, 240, 222, 134, 264, 250]
image sizes: [(363, 111), (363, 111), (363, 111), (363, 111), (363, 111), (363, 111), (363, 111), (363, 111), (363, 111), (363, 111)]
input_ids shape: torch.Size([10, 224])
attention_mask shape: torch.Size([10, 224])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 224


===== Batch 1119 =====
QIDs: [265, 265, 265, 265, 265, 265, 265, 265, 265, 265]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [617, 389, 572, 626, 497, 573, 523, 301, 615, 575]
image sizes: [(650, 580), (650, 580), (650, 580), (650, 580), (650, 580), (650, 580), (650, 580), (650, 580), (650, 580), (650, 580)]
input_ids shape: torch.Size([10, 812])
attention_mask shape: torch.Size([10, 812])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 812


===== Batch 1120 =====
QIDs: [1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025, 1025]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [243, 158, 234, 282, 235, 282, 233, 114, 235, 265]
image sizes: [(418, 262), (418, 262), (418, 262), (418, 262), (418, 262), (418, 262), (418, 262), (418, 262), (418, 262), (418, 262)]
input_ids shape: torch.Size([10, 284])
attention_mask shape: torch.Size([10, 284])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 284


===== Batch 1121 =====
QIDs: [30, 30, 30, 30, 30, 30, 30, 30, 30, 30]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [217, 162, 206, 234, 196, 214, 194, 134, 207, 195]
image sizes: [(340, 181), (340, 181), (340, 181), (340, 181), (340, 181), (340, 181), (340, 181), (340, 181), (340, 181), (340, 181)]
input_ids shape: torch.Size([10, 241])
attention_mask shape: torch.Size([10, 241])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 241


===== Batch 1122 =====
QIDs: [845, 845, 845, 845, 845, 845, 845, 845, 845, 845]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [275, 201, 257, 263, 239, 254, 257, 176, 266, 262]
image sizes: [(679, 336), (679, 336), (679, 336), (679, 336), (679, 336), (679, 336), (679, 336), (679, 336), (679, 336), (679, 336)]
input_ids shape: torch.Size([10, 466])
attention_mask shape: torch.Size([10, 466])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 466


===== Batch 1123 =====
QIDs: [257, 257, 257, 257, 257, 257, 257, 257, 257, 257]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [831, 487, 781, 862, 860, 803, 736, 370, 975, 776]
image sizes: [(562, 434), (562, 434), (562, 434), (562, 434), (562, 434), (562, 434), (562, 434), (562, 434), (562, 434), (562, 434)]
input_ids shape: torch.Size([10, 840])
attention_mask shape: torch.Size([10, 840])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 840


===== Batch 1124 =====
QIDs: [205, 205, 205, 205, 205, 205, 205, 205, 205, 205]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [538, 240, 470, 466, 428, 489, 443, 173, 613, 475]
image sizes: [(330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260)]
input_ids shape: torch.Size([10, 391])
attention_mask shape: torch.Size([10, 391])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 391


===== Batch 1125 =====
QIDs: [1032, 1032, 1032, 1032, 1032, 1032, 1032, 1032, 1032, 1032]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [661, 357, 617, 636, 498, 617, 545, 264, 629, 616]
image sizes: [(576, 174), (576, 174), (576, 174), (576, 174), (576, 174), (576, 174), (576, 174), (576, 174), (576, 174), (576, 174)]
input_ids shape: torch.Size([10, 489])
attention_mask shape: torch.Size([10, 489])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 489


===== Batch 1126 =====
QIDs: [536, 536, 536, 536, 536, 536, 536, 536, 536, 536]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [586, 548, 581, 595, 568, 583, 570, 514, 596, 573]
image sizes: [(428, 199), (428, 199), (428, 199), (428, 199), (428, 199), (428, 199), (428, 199), (428, 199), (428, 199), (428, 199)]
input_ids shape: torch.Size([10, 522])
attention_mask shape: torch.Size([10, 522])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 522


===== Batch 1127 =====
QIDs: [240, 240, 240, 240, 240, 240, 240, 240, 240, 240]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [282, 123, 277, 272, 257, 277, 261, 102, 276, 320]
image sizes: [(465, 421), (465, 421), (465, 421), (465, 421), (465, 421), (465, 421), (465, 421), (465, 421), (465, 421), (465, 421)]
input_ids shape: torch.Size([10, 436])
attention_mask shape: torch.Size([10, 436])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 436


===== Batch 1128 =====
QIDs: [604, 604, 604, 604, 604, 604, 604, 604, 604, 604]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [379, 234, 364, 366, 334, 353, 310, 179, 435, 385]
image sizes: [(430, 233), (430, 233), (430, 233), (430, 233), (430, 233), (430, 233), (430, 233), (430, 233), (430, 233), (430, 233)]
input_ids shape: torch.Size([10, 352])
attention_mask shape: torch.Size([10, 352])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 352


===== Batch 1129 =====
QIDs: [1580, 1580, 1580, 1580, 1580, 1580, 1580, 1580, 1580, 1580]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1215, 692, 1101, 1346, 864, 1049, 1076, 323, 1176, 1125]
image sizes: [(519, 302), (519, 302), (519, 302), (519, 302), (519, 302), (519, 302), (519, 302), (519, 302), (519, 302), (519, 302)]
input_ids shape: torch.Size([10, 899])
attention_mask shape: torch.Size([10, 899])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 899


===== Batch 1130 =====
QIDs: [23, 23, 23, 23, 23, 23, 23, 23, 23, 23]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [335, 224, 361, 301, 337, 322, 319, 161, 340, 348]
image sizes: [(397, 163), (397, 163), (397, 163), (397, 163), (397, 163), (397, 163), (397, 163), (397, 163), (397, 163), (397, 163)]
input_ids shape: torch.Size([10, 320])
attention_mask shape: torch.Size([10, 320])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 320


===== Batch 1131 =====
QIDs: [1288, 1288, 1288, 1288, 1288, 1288, 1288, 1288, 1288, 1288]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [120, 89, 102, 117, 108, 113, 114, 77, 123, 118]
image sizes: [(774, 320), (774, 320), (774, 320), (774, 320), (774, 320), (774, 320), (774, 320), (774, 320), (774, 320), (774, 320)]
input_ids shape: torch.Size([10, 391])
attention_mask shape: torch.Size([10, 391])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 391


===== Batch 1132 =====
QIDs: [1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [262, 210, 235, 250, 251, 241, 257, 187, 267, 264]
image sizes: [(316, 314), (316, 314), (316, 314), (316, 314), (316, 314), (316, 314), (316, 314), (316, 314), (316, 314), (316, 314)]
input_ids shape: torch.Size([10, 324])
attention_mask shape: torch.Size([10, 324])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 324


===== Batch 1133 =====
QIDs: [1523, 1523, 1523, 1523, 1523, 1523, 1523, 1523, 1523, 1523]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [341, 241, 351, 384, 324, 345, 342, 169, 343, 374]
image sizes: [(1233, 601), (1233, 601), (1233, 601), (1233, 601), (1233, 601), (1233, 601), (1233, 601), (1233, 601), (1233, 601), (1233, 601)]
input_ids shape: torch.Size([10, 1168])
attention_mask shape: torch.Size([10, 1168])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1168


===== Batch 1134 =====
QIDs: [312, 312, 312, 312, 312, 312, 312, 312, 312, 312]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [445, 227, 475, 471, 428, 442, 413, 171, 463, 495]
image sizes: [(504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330), (504, 330)]
input_ids shape: torch.Size([10, 497])
attention_mask shape: torch.Size([10, 497])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 497


===== Batch 1135 =====
QIDs: [750, 750, 750, 750, 750, 750, 750, 750, 750, 750]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [493, 215, 472, 537, 467, 496, 407, 165, 515, 518]
image sizes: [(1040, 1146), (1040, 1146), (1040, 1146), (1040, 1146), (1040, 1146), (1040, 1146), (1040, 1146), (1040, 1146), (1040, 1146), (1040, 1146)]
input_ids shape: torch.Size([10, 1784])
attention_mask shape: torch.Size([10, 1784])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1784


===== Batch 1136 =====
QIDs: [208, 208, 208, 208, 208, 208, 208, 208, 208, 208]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [356, 149, 328, 237, 237, 345, 312, 118, 367, 239]
image sizes: [(590, 290), (590, 290), (590, 290), (590, 290), (590, 290), (590, 290), (590, 290), (590, 290), (590, 290), (590, 290)]
input_ids shape: torch.Size([10, 441])
attention_mask shape: torch.Size([10, 441])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 441


===== Batch 1137 =====
QIDs: [100, 100, 100, 100, 100, 100, 100, 100, 100, 100]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1193, 813, 1129, 1161, 1112, 1361, 1246, 479, 1271, 1272]
image sizes: [(635, 70), (635, 70), (635, 70), (635, 70), (635, 70), (635, 70), (635, 70), (635, 70), (635, 70), (635, 70)]
input_ids shape: torch.Size([10, 886])
attention_mask shape: torch.Size([10, 886])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 886


===== Batch 1138 =====
QIDs: [603, 603, 603, 603, 603, 603, 603, 603, 603, 603]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1058, 528, 1044, 1066, 1240, 1098, 971, 431, 1273, 862]
image sizes: [(490, 145), (490, 145), (490, 145), (490, 145), (490, 145), (490, 145), (490, 145), (490, 145), (490, 145), (490, 145)]
input_ids shape: torch.Size([10, 755])
attention_mask shape: torch.Size([10, 755])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 755


===== Batch 1139 =====
QIDs: [1055, 1055, 1055, 1055, 1055, 1055, 1055, 1055, 1055, 1055]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [700, 380, 631, 690, 542, 650, 606, 259, 708, 630]
image sizes: [(648, 202), (648, 202), (648, 202), (648, 202), (648, 202), (648, 202), (648, 202), (648, 202), (648, 202), (648, 202)]
input_ids shape: torch.Size([10, 496])
attention_mask shape: torch.Size([10, 496])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 496


===== Batch 1140 =====
QIDs: [1714, 1714, 1714, 1714, 1714, 1714, 1714, 1714, 1714, 1714]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [366, 187, 387, 440, 297, 347, 403, 146, 461, 435]
image sizes: [(988, 1234), (988, 1234), (988, 1234), (988, 1234), (988, 1234), (988, 1234), (988, 1234), (988, 1234), (988, 1234), (988, 1234)]
input_ids shape: torch.Size([10, 1785])
attention_mask shape: torch.Size([10, 1785])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1785


===== Batch 1141 =====
QIDs: [478, 478, 478, 478, 478, 478, 478, 478, 478, 478]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [178, 102, 170, 189, 166, 171, 152, 73, 199, 171]
image sizes: [(697, 652), (697, 652), (697, 652), (697, 652), (697, 652), (697, 652), (697, 652), (697, 652), (697, 652), (697, 652)]
input_ids shape: torch.Size([10, 690])
attention_mask shape: torch.Size([10, 690])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 690


===== Batch 1142 =====
QIDs: [292, 292, 292, 292, 292, 292, 292, 292, 292, 292]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [140, 111, 128, 144, 140, 141, 136, 84, 140, 141]
image sizes: [(519, 61), (519, 61), (519, 61), (519, 61), (519, 61), (519, 61), (519, 61), (519, 61), (519, 61), (519, 61)]
input_ids shape: torch.Size([10, 132])
attention_mask shape: torch.Size([10, 132])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 132


===== Batch 1143 =====
QIDs: [1071, 1071, 1071, 1071, 1071, 1071, 1071, 1071, 1071, 1071]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [3145, 1584, 3036, 3235, 2637, 2918, 3037, 965, 3217, 3232]
image sizes: [(872, 556), (872, 556), (872, 556), (872, 556), (872, 556), (872, 556), (872, 556), (872, 556), (872, 556), (872, 556)]
input_ids shape: torch.Size([10, 2314])
attention_mask shape: torch.Size([10, 2314])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2314


===== Batch 1144 =====
QIDs: [710, 710, 710, 710, 710, 710, 710, 710, 710, 710]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [785, 328, 731, 870, 615, 727, 609, 245, 784, 838]
image sizes: [(904, 1108), (904, 1108), (904, 1108), (904, 1108), (904, 1108), (904, 1108), (904, 1108), (904, 1108), (904, 1108), (904, 1108)]
input_ids shape: torch.Size([10, 1673])
attention_mask shape: torch.Size([10, 1673])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1673


===== Batch 1145 =====
QIDs: [1551, 1551, 1551, 1551, 1551, 1551, 1551, 1551, 1551, 1551]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [353, 183, 323, 351, 264, 303, 291, 127, 314, 296]
image sizes: [(366, 285), (366, 285), (366, 285), (366, 285), (366, 285), (366, 285), (366, 285), (366, 285), (366, 285), (366, 285)]
input_ids shape: torch.Size([10, 306])
attention_mask shape: torch.Size([10, 306])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 306


===== Batch 1146 =====
QIDs: [1373, 1373, 1373, 1373, 1373, 1373, 1373, 1373, 1373, 1373]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [412, 199, 441, 409, 725, 434, 391, 156, 437, 376]
image sizes: [(1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604), (1880, 1604)]
input_ids shape: torch.Size([10, 4090])
attention_mask shape: torch.Size([10, 4090])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 4090


===== Batch 1147 =====
QIDs: [1297, 1297, 1297, 1297, 1297, 1297, 1297, 1297, 1297, 1297]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [277, 153, 284, 293, 235, 288, 256, 108, 319, 266]
image sizes: [(155, 144), (155, 144), (155, 144), (155, 144), (155, 144), (155, 144), (155, 144), (155, 144), (155, 144), (155, 144)]
input_ids shape: torch.Size([10, 212])
attention_mask shape: torch.Size([10, 212])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 212


===== Batch 1148 =====
QIDs: [1036, 1036, 1036, 1036, 1036, 1036, 1036, 1036, 1036, 1036]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [2789, 1321, 2639, 2720, 2197, 2616, 2388, 846, 2828, 2835]
image sizes: [(1226, 620), (1226, 620), (1226, 620), (1226, 620), (1226, 620), (1226, 620), (1226, 620), (1226, 620), (1226, 620), (1226, 620)]
input_ids shape: torch.Size([10, 2292])
attention_mask shape: torch.Size([10, 2292])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2292


===== Batch 1149 =====
QIDs: [379, 379, 379, 379, 379, 379, 379, 379, 379, 379]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [190, 137, 167, 183, 181, 177, 185, 118, 183, 193]
image sizes: [(506, 514), (506, 514), (506, 514), (506, 514), (506, 514), (506, 514), (506, 514), (506, 514), (506, 514), (506, 514)]
input_ids shape: torch.Size([10, 449])
attention_mask shape: torch.Size([10, 449])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 449


===== Batch 1150 =====
QIDs: [506, 506, 506, 506, 506, 506, 506, 506, 506, 506]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [599, 541, 617, 620, 583, 600, 574, 522, 619, 617]
image sizes: [(563, 213), (563, 213), (563, 213), (563, 213), (563, 213), (563, 213), (563, 213), (563, 213), (563, 213), (563, 213)]
input_ids shape: torch.Size([10, 550])
attention_mask shape: torch.Size([10, 550])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 550


===== Batch 1151 =====
QIDs: [972, 972, 972, 972, 972, 972, 972, 972, 972, 972]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [167, 98, 173, 181, 151, 170, 143, 67, 179, 164]
image sizes: [(343, 232), (343, 232), (343, 232), (343, 232), (343, 232), (343, 232), (343, 232), (343, 232), (343, 232), (343, 232)]
input_ids shape: torch.Size([10, 209])
attention_mask shape: torch.Size([10, 209])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 209


===== Batch 1152 =====
QIDs: [1709, 1709, 1709, 1709, 1709, 1709, 1709, 1709, 1709, 1709]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [563, 300, 523, 581, 480, 526, 476, 221, 577, 575]
image sizes: [(755, 195), (755, 195), (755, 195), (755, 195), (755, 195), (755, 195), (755, 195), (755, 195), (755, 195), (755, 195)]
input_ids shape: torch.Size([10, 505])
attention_mask shape: torch.Size([10, 505])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 505


===== Batch 1153 =====
QIDs: [978, 978, 978, 978, 978, 978, 978, 978, 978, 978]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [223, 116, 228, 243, 189, 238, 214, 71, 261, 211]
image sizes: [(554, 137), (554, 137), (554, 137), (554, 137), (554, 137), (554, 137), (554, 137), (554, 137), (554, 137), (554, 137)]
input_ids shape: torch.Size([10, 243])
attention_mask shape: torch.Size([10, 243])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 243


===== Batch 1154 =====
QIDs: [1165, 1165, 1165, 1165, 1165, 1165, 1165, 1165, 1165, 1165]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [496, 257, 458, 470, 410, 461, 446, 164, 535, 501]
image sizes: [(444, 112), (444, 112), (444, 112), (444, 112), (444, 112), (444, 112), (444, 112), (444, 112), (444, 112), (444, 112)]
input_ids shape: torch.Size([10, 358])
attention_mask shape: torch.Size([10, 358])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 358


===== Batch 1155 =====
QIDs: [497, 497, 497, 497, 497, 497, 497, 497, 497, 497]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [703, 621, 699, 717, 694, 715, 667, 527, 718, 710]
image sizes: [(1228, 499), (1228, 499), (1228, 499), (1228, 499), (1228, 499), (1228, 499), (1228, 499), (1228, 499), (1228, 499), (1228, 499)]
input_ids shape: torch.Size([10, 1423])
attention_mask shape: torch.Size([10, 1423])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1423


===== Batch 1156 =====
QIDs: [840, 840, 840, 840, 840, 840, 840, 840, 840, 840]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1215, 610, 1164, 1233, 1065, 1179, 1061, 449, 1365, 1318]
image sizes: [(408, 276), (408, 276), (408, 276), (408, 276), (408, 276), (408, 276), (408, 276), (408, 276), (408, 276), (408, 276)]
input_ids shape: torch.Size([10, 762])
attention_mask shape: torch.Size([10, 762])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 762


===== Batch 1157 =====
QIDs: [347, 347, 347, 347, 347, 347, 347, 347, 347, 347]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [895, 443, 902, 855, 776, 786, 739, 327, 924, 938]
image sizes: [(118, 150), (118, 150), (118, 150), (118, 150), (118, 150), (118, 150), (118, 150), (118, 150), (118, 150), (118, 150)]
input_ids shape: torch.Size([10, 496])
attention_mask shape: torch.Size([10, 496])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 496


===== Batch 1158 =====
QIDs: [260, 260, 260, 260, 260, 260, 260, 260, 260, 260]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [101, 77, 106, 111, 76, 101, 103, 52, 121, 101]
image sizes: [(628, 414), (628, 414), (628, 414), (628, 414), (628, 414), (628, 414), (628, 414), (628, 414), (628, 414), (628, 414)]
input_ids shape: torch.Size([10, 414])
attention_mask shape: torch.Size([10, 414])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 414


===== Batch 1159 =====
QIDs: [503, 503, 503, 503, 503, 503, 503, 503, 503, 503]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [586, 533, 583, 608, 573, 576, 557, 521, 587, 568]
image sizes: [(330, 215), (330, 215), (330, 215), (330, 215), (330, 215), (330, 215), (330, 215), (330, 215), (330, 215), (330, 215)]
input_ids shape: torch.Size([10, 478])
attention_mask shape: torch.Size([10, 478])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 478


===== Batch 1160 =====
QIDs: [889, 889, 889, 889, 889, 889, 889, 889, 889, 889]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1547, 1332, 1626, 1575, 1554, 1597, 1623, 1188, 1686, 1612]
image sizes: [(500, 156), (500, 156), (500, 156), (500, 156), (500, 156), (500, 156), (500, 156), (500, 156), (500, 156), (500, 156)]
input_ids shape: torch.Size([10, 1333])
attention_mask shape: torch.Size([10, 1333])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1333


===== Batch 1161 =====
QIDs: [1509, 1509, 1509, 1509, 1509, 1509, 1509, 1509, 1509, 1509]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [591, 329, 580, 604, 508, 572, 517, 237, 609, 617]
image sizes: [(566, 76), (566, 76), (566, 76), (566, 76), (566, 76), (566, 76), (566, 76), (566, 76), (566, 76), (566, 76)]
input_ids shape: torch.Size([10, 389])
attention_mask shape: torch.Size([10, 389])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 389


===== Batch 1162 =====
QIDs: [219, 219, 219, 219, 219, 219, 219, 219, 219, 219]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [227, 181, 200, 227, 232, 220, 227, 146, 278, 255]
image sizes: [(422, 90), (422, 90), (422, 90), (422, 90), (422, 90), (422, 90), (422, 90), (422, 90), (422, 90), (422, 90)]
input_ids shape: torch.Size([10, 230])
attention_mask shape: torch.Size([10, 230])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 230


===== Batch 1163 =====
QIDs: [81, 81, 81, 81, 81, 81, 81, 81, 81, 81]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [411, 236, 394, 386, 307, 358, 378, 197, 358, 319]
image sizes: [(424, 304), (424, 304), (424, 304), (424, 304), (424, 304), (424, 304), (424, 304), (424, 304), (424, 304), (424, 304)]
input_ids shape: torch.Size([10, 445])
attention_mask shape: torch.Size([10, 445])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 445


===== Batch 1164 =====
QIDs: [1091, 1091, 1091, 1091, 1091, 1091, 1091, 1091, 1091, 1091]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [383, 236, 330, 354, 370, 335, 335, 184, 392, 370]
image sizes: [(1466, 202), (1466, 202), (1466, 202), (1466, 202), (1466, 202), (1466, 202), (1466, 202), (1466, 202), (1466, 202), (1466, 202)]
input_ids shape: torch.Size([10, 594])
attention_mask shape: torch.Size([10, 594])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 594


===== Batch 1165 =====
QIDs: [1458, 1458, 1458, 1458, 1458, 1458, 1458, 1458, 1458, 1458]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [679, 356, 711, 754, 660, 738, 573, 291, 798, 651]
image sizes: [(737, 591), (737, 591), (737, 591), (737, 591), (737, 591), (737, 591), (737, 591), (737, 591), (737, 591), (737, 591)]
input_ids shape: torch.Size([10, 921])
attention_mask shape: torch.Size([10, 921])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 921


===== Batch 1166 =====
QIDs: [558, 558, 558, 558, 558, 558, 558, 558, 558, 558]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [351, 241, 340, 400, 336, 338, 390, 204, 358, 369]
image sizes: [(294, 283), (294, 283), (294, 283), (294, 283), (294, 283), (294, 283), (294, 283), (294, 283), (294, 283), (294, 283)]
input_ids shape: torch.Size([10, 408])
attention_mask shape: torch.Size([10, 408])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 408


===== Batch 1167 =====
QIDs: [1464, 1464, 1464, 1464, 1464, 1464, 1464, 1464, 1464, 1464]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [417, 229, 389, 429, 706, 394, 370, 175, 433, 425]
image sizes: [(560, 215), (560, 215), (560, 215), (560, 215), (560, 215), (560, 215), (560, 215), (560, 215), (560, 215), (560, 215)]
input_ids shape: torch.Size([10, 466])
attention_mask shape: torch.Size([10, 466])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 466


===== Batch 1168 =====
QIDs: [835, 835, 835, 835, 835, 835, 835, 835, 835, 835]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [378, 323, 374, 375, 356, 369, 374, 300, 395, 380]
image sizes: [(451, 325), (451, 325), (451, 325), (451, 325), (451, 325), (451, 325), (451, 325), (451, 325), (451, 325), (451, 325)]
input_ids shape: torch.Size([10, 438])
attention_mask shape: torch.Size([10, 438])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 438


===== Batch 1169 =====
QIDs: [465, 465, 465, 465, 465, 465, 465, 465, 465, 465]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1113, 485, 933, 1093, 772, 980, 836, 333, 1103, 889]
image sizes: [(881, 401), (881, 401), (881, 401), (881, 401), (881, 401), (881, 401), (881, 401), (881, 401), (881, 401), (881, 401)]
input_ids shape: torch.Size([10, 950])
attention_mask shape: torch.Size([10, 950])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 950


===== Batch 1170 =====
QIDs: [1268, 1268, 1268, 1268, 1268, 1268, 1268, 1268, 1268, 1268]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [125, 87, 129, 123, 124, 129, 122, 75, 163, 127]
image sizes: [(170, 186), (170, 186), (170, 186), (170, 186), (170, 186), (170, 186), (170, 186), (170, 186), (170, 186), (170, 186)]
input_ids shape: torch.Size([10, 159])
attention_mask shape: torch.Size([10, 159])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 159


===== Batch 1171 =====
QIDs: [1263, 1263, 1263, 1263, 1263, 1263, 1263, 1263, 1263, 1263]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [240, 137, 265, 211, 273, 256, 301, 124, 237, 273]
image sizes: [(330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260), (330, 260)]
input_ids shape: torch.Size([10, 302])
attention_mask shape: torch.Size([10, 302])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 302


===== Batch 1172 =====
QIDs: [376, 376, 376, 376, 376, 376, 376, 376, 376, 376]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [516, 280, 536, 508, 453, 396, 488, 212, 547, 517]
image sizes: [(429, 214), (429, 214), (429, 214), (429, 214), (429, 214), (429, 214), (429, 214), (429, 214), (429, 214), (429, 214)]
input_ids shape: torch.Size([10, 437])
attention_mask shape: torch.Size([10, 437])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 437


===== Batch 1173 =====
QIDs: [303, 303, 303, 303, 303, 303, 303, 303, 303, 303]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [711, 359, 683, 688, 606, 658, 636, 379, 688, 673]
image sizes: [(504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331), (504, 331)]
input_ids shape: torch.Size([10, 606])
attention_mask shape: torch.Size([10, 606])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 606


===== Batch 1174 =====
QIDs: [753, 753, 753, 753, 753, 753, 753, 753, 753, 753]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1148, 660, 1073, 1154, 995, 966, 863, 526, 1078, 1088]
image sizes: [(915, 318), (915, 318), (915, 318), (915, 318), (915, 318), (915, 318), (915, 318), (915, 318), (915, 318), (915, 318)]
input_ids shape: torch.Size([10, 1055])
attention_mask shape: torch.Size([10, 1055])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1055


===== Batch 1175 =====
QIDs: [798, 798, 798, 798, 798, 798, 798, 798, 798, 798]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [654, 342, 611, 694, 635, 611, 541, 259, 666, 657]
image sizes: [(1216, 421), (1216, 421), (1216, 421), (1216, 421), (1216, 421), (1216, 421), (1216, 421), (1216, 421), (1216, 421), (1216, 421)]
input_ids shape: torch.Size([10, 1012])
attention_mask shape: torch.Size([10, 1012])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1012


===== Batch 1176 =====
QIDs: [844, 844, 844, 844, 844, 844, 844, 844, 844, 844]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [314, 208, 303, 331, 380, 299, 285, 175, 311, 312]
image sizes: [(723, 208), (723, 208), (723, 208), (723, 208), (723, 208), (723, 208), (723, 208), (723, 208), (723, 208), (723, 208)]
input_ids shape: torch.Size([10, 417])
attention_mask shape: torch.Size([10, 417])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 417


===== Batch 1177 =====
QIDs: [1163, 1163, 1163, 1163, 1163, 1163, 1163, 1163, 1163, 1163]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [315, 198, 328, 335, 306, 331, 268, 162, 354, 341]
image sizes: [(843, 130), (843, 130), (843, 130), (843, 130), (843, 130), (843, 130), (843, 130), (843, 130), (843, 130), (843, 130)]
input_ids shape: torch.Size([10, 358])
attention_mask shape: torch.Size([10, 358])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 358


===== Batch 1178 =====
QIDs: [1106, 1106, 1106, 1106, 1106, 1106, 1106, 1106, 1106, 1106]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [778, 457, 819, 835, 749, 766, 702, 356, 835, 759]
image sizes: [(845, 298), (845, 298), (845, 298), (845, 298), (845, 298), (845, 298), (845, 298), (845, 298), (845, 298), (845, 298)]
input_ids shape: torch.Size([10, 861])
attention_mask shape: torch.Size([10, 861])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 861


===== Batch 1179 =====
QIDs: [879, 879, 879, 879, 879, 879, 879, 879, 879, 879]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [675, 628, 673, 745, 605, 666, 618, 430, 694, 679]
image sizes: [(507, 203), (507, 203), (507, 203), (507, 203), (507, 203), (507, 203), (507, 203), (507, 203), (507, 203), (507, 203)]
input_ids shape: torch.Size([10, 538])
attention_mask shape: torch.Size([10, 538])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 538


===== Batch 1180 =====
QIDs: [608, 608, 608, 608, 608, 608, 608, 608, 608, 608]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [511, 325, 489, 496, 452, 495, 443, 246, 510, 459]
image sizes: [(389, 332), (389, 332), (389, 332), (389, 332), (389, 332), (389, 332), (389, 332), (389, 332), (389, 332), (389, 332)]
input_ids shape: torch.Size([10, 481])
attention_mask shape: torch.Size([10, 481])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 481


===== Batch 1181 =====
QIDs: [409, 409, 409, 409, 409, 409, 409, 409, 409, 409]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1228, 579, 965, 1016, 889, 966, 809, 325, 965, 844]
image sizes: [(500, 375), (500, 375), (500, 375), (500, 375), (500, 375), (500, 375), (500, 375), (500, 375), (500, 375), (500, 375)]
input_ids shape: torch.Size([10, 772])
attention_mask shape: torch.Size([10, 772])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 772


===== Batch 1182 =====
QIDs: [647, 647, 647, 647, 647, 647, 647, 647, 647, 647]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [177, 100, 162, 165, 151, 158, 160, 82, 197, 152]
image sizes: [(187, 108), (187, 108), (187, 108), (187, 108), (187, 108), (187, 108), (187, 108), (187, 108), (187, 108), (187, 108)]
input_ids shape: torch.Size([10, 151])
attention_mask shape: torch.Size([10, 151])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 151


===== Batch 1183 =====
QIDs: [1439, 1439, 1439, 1439, 1439, 1439, 1439, 1439, 1439, 1439]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [291, 221, 286, 311, 297, 293, 259, 195, 296, 301]
image sizes: [(282, 310), (282, 310), (282, 310), (282, 310), (282, 310), (282, 310), (282, 310), (282, 310), (282, 310), (282, 310)]
input_ids shape: torch.Size([10, 318])
attention_mask shape: torch.Size([10, 318])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 318


===== Batch 1184 =====
QIDs: [1526, 1526, 1526, 1526, 1526, 1526, 1526, 1526, 1526, 1526]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [205, 112, 218, 239, 189, 200, 176, 72, 246, 204]
image sizes: [(654, 370), (654, 370), (654, 370), (654, 370), (654, 370), (654, 370), (654, 370), (654, 370), (654, 370), (654, 370)]
input_ids shape: torch.Size([10, 427])
attention_mask shape: torch.Size([10, 427])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 427


===== Batch 1185 =====
QIDs: [751, 751, 751, 751, 751, 751, 751, 751, 751, 751]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [142, 117, 128, 156, 148, 195, 178, 104, 138, 215]
image sizes: [(497, 186), (497, 186), (497, 186), (497, 186), (497, 186), (497, 186), (497, 186), (497, 186), (497, 186), (497, 186)]
input_ids shape: torch.Size([10, 259])
attention_mask shape: torch.Size([10, 259])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 259


===== Batch 1186 =====
QIDs: [534, 534, 534, 534, 534, 534, 534, 534, 534, 534]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [276, 268, 297, 305, 281, 293, 273, 253, 297, 292]
image sizes: [(374, 227), (374, 227), (374, 227), (374, 227), (374, 227), (374, 227), (374, 227), (374, 227), (374, 227), (374, 227)]
input_ids shape: torch.Size([10, 324])
attention_mask shape: torch.Size([10, 324])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 324


===== Batch 1187 =====
QIDs: [805, 805, 805, 805, 805, 805, 805, 805, 805, 805]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [413, 298, 399, 412, 369, 388, 356, 266, 414, 418]
image sizes: [(1267, 382), (1267, 382), (1267, 382), (1267, 382), (1267, 382), (1267, 382), (1267, 382), (1267, 382), (1267, 382), (1267, 382)]
input_ids shape: torch.Size([10, 931])
attention_mask shape: torch.Size([10, 931])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 931


===== Batch 1188 =====
QIDs: [1134, 1134, 1134, 1134, 1134, 1134, 1134, 1134, 1134, 1134]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [439, 242, 416, 425, 330, 406, 378, 167, 464, 439]
image sizes: [(1052, 836), (1052, 836), (1052, 836), (1052, 836), (1052, 836), (1052, 836), (1052, 836), (1052, 836), (1052, 836), (1052, 836)]
input_ids shape: torch.Size([10, 1357])
attention_mask shape: torch.Size([10, 1357])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1357


===== Batch 1189 =====
QIDs: [1040, 1040, 1040, 1040, 1040, 1040, 1040, 1040, 1040, 1040]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [280, 196, 275, 295, 243, 268, 244, 141, 281, 302]
image sizes: [(488, 294), (488, 294), (488, 294), (488, 294), (488, 294), (488, 294), (488, 294), (488, 294), (488, 294), (488, 294)]
input_ids shape: torch.Size([10, 332])
attention_mask shape: torch.Size([10, 332])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 332


===== Batch 1190 =====
QIDs: [1463, 1463, 1463, 1463, 1463, 1463, 1463, 1463, 1463, 1463]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [123, 70, 102, 122, 87, 106, 89, 50, 130, 127]
image sizes: [(1699, 214), (1699, 214), (1699, 214), (1699, 214), (1699, 214), (1699, 214), (1699, 214), (1699, 214), (1699, 214), (1699, 214)]
input_ids shape: torch.Size([10, 571])
attention_mask shape: torch.Size([10, 571])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 571


===== Batch 1191 =====
QIDs: [1525, 1525, 1525, 1525, 1525, 1525, 1525, 1525, 1525, 1525]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [213, 156, 215, 233, 212, 214, 203, 135, 228, 203]
image sizes: [(1287, 316), (1287, 316), (1287, 316), (1287, 316), (1287, 316), (1287, 316), (1287, 316), (1287, 316), (1287, 316), (1287, 316)]
input_ids shape: torch.Size([10, 675])
attention_mask shape: torch.Size([10, 675])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 675


===== Batch 1192 =====
QIDs: [984, 984, 984, 984, 984, 984, 984, 984, 984, 984]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1495, 688, 1246, 1241, 1312, 1235, 1181, 480, 1270, 1108]
image sizes: [(276, 208), (276, 208), (276, 208), (276, 208), (276, 208), (276, 208), (276, 208), (276, 208), (276, 208), (276, 208)]
input_ids shape: torch.Size([10, 831])
attention_mask shape: torch.Size([10, 831])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 831


===== Batch 1193 =====
QIDs: [457, 457, 457, 457, 457, 457, 457, 457, 457, 457]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [251, 119, 242, 245, 232, 230, 220, 94, 268, 230]
image sizes: [(790, 630), (790, 630), (790, 630), (790, 630), (790, 630), (790, 630), (790, 630), (790, 630), (790, 630), (790, 630)]
input_ids shape: torch.Size([10, 777])
attention_mask shape: torch.Size([10, 777])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 777


===== Batch 1194 =====
QIDs: [1648, 1648, 1648, 1648, 1648, 1648, 1648, 1648, 1648, 1648]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [162, 110, 175, 203, 172, 152, 134, 72, 193, 176]
image sizes: [(512, 205), (512, 205), (512, 205), (512, 205), (512, 205), (512, 205), (512, 205), (512, 205), (512, 205), (512, 205)]
input_ids shape: torch.Size([10, 237])
attention_mask shape: torch.Size([10, 237])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 237


===== Batch 1195 =====
QIDs: [1711, 1711, 1711, 1711, 1711, 1711, 1711, 1711, 1711, 1711]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1067, 489, 1039, 1065, 1072, 1005, 953, 293, 1160, 973]
image sizes: [(602, 141), (602, 141), (602, 141), (602, 141), (602, 141), (602, 141), (602, 141), (602, 141), (602, 141), (602, 141)]
input_ids shape: torch.Size([10, 700])
attention_mask shape: torch.Size([10, 700])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 700


===== Batch 1196 =====
QIDs: [787, 787, 787, 787, 787, 787, 787, 787, 787, 787]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1373, 643, 1283, 1450, 1326, 1290, 1076, 392, 1525, 1362]
image sizes: [(1216, 421), (1216, 421), (1216, 421), (1216, 421), (1216, 421), (1216, 421), (1216, 421), (1216, 421), (1216, 421), (1216, 421)]
input_ids shape: torch.Size([10, 1308])
attention_mask shape: torch.Size([10, 1308])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1308


===== Batch 1197 =====
QIDs: [1566, 1566, 1566, 1566, 1566, 1566, 1566, 1566, 1566, 1566]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [420, 279, 415, 439, 418, 431, 365, 230, 461, 430]
image sizes: [(430, 173), (430, 173), (430, 173), (430, 173), (430, 173), (430, 173), (430, 173), (430, 173), (430, 173), (430, 173)]
input_ids shape: torch.Size([10, 368])
attention_mask shape: torch.Size([10, 368])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 368


===== Batch 1198 =====
QIDs: [670, 670, 670, 670, 670, 670, 670, 670, 670, 670]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [755, 380, 755, 818, 688, 671, 644, 233, 869, 782]
image sizes: [(491, 67), (491, 67), (491, 67), (491, 67), (491, 67), (491, 67), (491, 67), (491, 67), (491, 67), (491, 67)]
input_ids shape: torch.Size([10, 438])
attention_mask shape: torch.Size([10, 438])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 438


===== Batch 1199 =====
QIDs: [864, 864, 864, 864, 864, 864, 864, 864, 864, 864]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [234, 154, 241, 280, 264, 237, 192, 188, 262, 213]
image sizes: [(398, 386), (398, 386), (398, 386), (398, 386), (398, 386), (398, 386), (398, 386), (398, 386), (398, 386), (398, 386)]
input_ids shape: torch.Size([10, 383])
attention_mask shape: torch.Size([10, 383])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 383


===== Batch 1200 =====
QIDs: [552, 552, 552, 552, 552, 552, 552, 552, 552, 552]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [467, 435, 455, 474, 463, 456, 453, 423, 456, 471]
image sizes: [(452, 363), (452, 363), (452, 363), (452, 363), (452, 363), (452, 363), (452, 363), (452, 363), (452, 363), (452, 363)]
input_ids shape: torch.Size([10, 464])
attention_mask shape: torch.Size([10, 464])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 464


===== Batch 1201 =====
QIDs: [1407, 1407, 1407, 1407, 1407, 1407, 1407, 1407, 1407, 1407]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [490, 458, 503, 516, 476, 511, 480, 444, 506, 499]
image sizes: [(466, 144), (466, 144), (466, 144), (466, 144), (466, 144), (466, 144), (466, 144), (466, 144), (466, 144), (466, 144)]
input_ids shape: torch.Size([10, 386])
attention_mask shape: torch.Size([10, 386])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 386


===== Batch 1202 =====
QIDs: [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [459, 280, 428, 456, 413, 430, 433, 238, 454, 481]
image sizes: [(343, 162), (343, 162), (343, 162), (343, 162), (343, 162), (343, 162), (343, 162), (343, 162), (343, 162), (343, 162)]
input_ids shape: torch.Size([10, 381])
attention_mask shape: torch.Size([10, 381])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 381


===== Batch 1203 =====
QIDs: [334, 334, 334, 334, 334, 334, 334, 334, 334, 334]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [452, 267, 453, 420, 423, 433, 409, 162, 478, 440]
image sizes: [(464, 480), (464, 480), (464, 480), (464, 480), (464, 480), (464, 480), (464, 480), (464, 480), (464, 480), (464, 480)]
input_ids shape: torch.Size([10, 566])
attention_mask shape: torch.Size([10, 566])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 566


===== Batch 1204 =====
QIDs: [282, 282, 282, 282, 282, 282, 282, 282, 282, 282]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [106, 76, 93, 99, 98, 103, 96, 66, 109, 101]
image sizes: [(118, 96), (118, 96), (118, 96), (118, 96), (118, 96), (118, 96), (118, 96), (118, 96), (118, 96), (118, 96)]
input_ids shape: torch.Size([10, 87])
attention_mask shape: torch.Size([10, 87])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 87


===== Batch 1205 =====
QIDs: [592, 592, 592, 592, 592, 592, 592, 592, 592, 592]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1460, 777, 1337, 1403, 1376, 1228, 1121, 501, 1363, 1201]
image sizes: [(955, 329), (955, 329), (955, 329), (955, 329), (955, 329), (955, 329), (955, 329), (955, 329), (955, 329), (955, 329)]
input_ids shape: torch.Size([10, 1168])
attention_mask shape: torch.Size([10, 1168])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1168


===== Batch 1206 =====
QIDs: [433, 433, 433, 433, 433, 433, 433, 433, 433, 433]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [760, 385, 690, 814, 597, 711, 624, 230, 824, 735]
image sizes: [(576, 372), (576, 372), (576, 372), (576, 372), (576, 372), (576, 372), (576, 372), (576, 372), (576, 372), (576, 372)]
input_ids shape: torch.Size([10, 658])
attention_mask shape: torch.Size([10, 658])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 658


===== Batch 1207 =====
QIDs: [1262, 1262, 1262, 1262, 1262, 1262, 1262, 1262, 1262, 1262]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [338, 158, 350, 360, 326, 348, 315, 128, 352, 318]
image sizes: [(251, 260), (251, 260), (251, 260), (251, 260), (251, 260), (251, 260), (251, 260), (251, 260), (251, 260), (251, 260)]
input_ids shape: torch.Size([10, 308])
attention_mask shape: torch.Size([10, 308])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 308


===== Batch 1208 =====
QIDs: [194, 194, 194, 194, 194, 194, 194, 194, 194, 194]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [646, 363, 663, 619, 594, 599, 581, 224, 711, 610]
image sizes: [(1142, 238), (1142, 238), (1142, 238), (1142, 238), (1142, 238), (1142, 238), (1142, 238), (1142, 238), (1142, 238), (1142, 238)]
input_ids shape: torch.Size([10, 703])
attention_mask shape: torch.Size([10, 703])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 703


===== Batch 1209 =====
QIDs: [451, 451, 451, 451, 451, 451, 451, 451, 451, 451]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [183, 109, 160, 174, 146, 188, 173, 78, 161, 191]
image sizes: [(722, 686), (722, 686), (722, 686), (722, 686), (722, 686), (722, 686), (722, 686), (722, 686), (722, 686), (722, 686)]
input_ids shape: torch.Size([10, 752])
attention_mask shape: torch.Size([10, 752])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 752


===== Batch 1210 =====
QIDs: [533, 533, 533, 533, 533, 533, 533, 533, 533, 533]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [756, 730, 745, 767, 743, 748, 756, 716, 796, 757]
image sizes: [(590, 283), (590, 283), (590, 283), (590, 283), (590, 283), (590, 283), (590, 283), (590, 283), (590, 283), (590, 283)]
input_ids shape: torch.Size([10, 722])
attention_mask shape: torch.Size([10, 722])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 722


===== Batch 1211 =====
QIDs: [439, 439, 439, 439, 439, 439, 439, 439, 439, 439]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [307, 132, 321, 282, 272, 298, 302, 110, 294, 272]
image sizes: [(1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548), (1916, 1548)]
input_ids shape: torch.Size([10, 3953])
attention_mask shape: torch.Size([10, 3953])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 3953


===== Batch 1212 =====
QIDs: [606, 606, 606, 606, 606, 606, 606, 606, 606, 606]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [288, 212, 312, 303, 259, 289, 269, 188, 304, 294]
image sizes: [(704, 814), (704, 814), (704, 814), (704, 814), (704, 814), (704, 814), (704, 814), (704, 814), (704, 814), (704, 814)]
input_ids shape: torch.Size([10, 931])
attention_mask shape: torch.Size([10, 931])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 931


===== Batch 1213 =====
QIDs: [396, 396, 396, 396, 396, 396, 396, 396, 396, 396]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [223, 148, 240, 248, 214, 229, 229, 108, 236, 264]
image sizes: [(375, 500), (375, 500), (375, 500), (375, 500), (375, 500), (375, 500), (375, 500), (375, 500), (375, 500), (375, 500)]
input_ids shape: torch.Size([10, 395])
attention_mask shape: torch.Size([10, 395])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 395


===== Batch 1214 =====
QIDs: [937, 937, 937, 937, 937, 937, 937, 937, 937, 937]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [93, 74, 89, 112, 82, 86, 107, 64, 93, 96]
image sizes: [(346, 112), (346, 112), (346, 112), (346, 112), (346, 112), (346, 112), (346, 112), (346, 112), (346, 112), (346, 112)]
input_ids shape: torch.Size([10, 128])
attention_mask shape: torch.Size([10, 128])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 128


===== Batch 1215 =====
QIDs: [932, 932, 932, 932, 932, 932, 932, 932, 932, 932]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [83, 56, 78, 72, 88, 82, 77, 41, 92, 85]
image sizes: [(1274, 205), (1274, 205), (1274, 205), (1274, 205), (1274, 205), (1274, 205), (1274, 205), (1274, 205), (1274, 205), (1274, 205)]
input_ids shape: torch.Size([10, 387])
attention_mask shape: torch.Size([10, 387])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 387


===== Batch 1216 =====
QIDs: [259, 259, 259, 259, 259, 259, 259, 259, 259, 259]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [409, 211, 364, 399, 315, 373, 332, 137, 376, 404]
image sizes: [(124, 138), (124, 138), (124, 138), (124, 138), (124, 138), (124, 138), (124, 138), (124, 138), (124, 138), (124, 138)]
input_ids shape: torch.Size([10, 248])
attention_mask shape: torch.Size([10, 248])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 248


===== Batch 1217 =====
QIDs: [33, 33, 33, 33, 33, 33, 33, 33, 33, 33]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [780, 396, 769, 851, 645, 744, 687, 279, 837, 878]
image sizes: [(1000, 1333), (1000, 1333), (1000, 1333), (1000, 1333), (1000, 1333), (1000, 1333), (1000, 1333), (1000, 1333), (1000, 1333), (1000, 1333)]
input_ids shape: torch.Size([10, 2147])
attention_mask shape: torch.Size([10, 2147])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2147


===== Batch 1218 =====
QIDs: [1164, 1164, 1164, 1164, 1164, 1164, 1164, 1164, 1164, 1164]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1125, 587, 1100, 1148, 972, 1069, 1030, 351, 1105, 899]
image sizes: [(409, 183), (409, 183), (409, 183), (409, 183), (409, 183), (409, 183), (409, 183), (409, 183), (409, 183), (409, 183)]
input_ids shape: torch.Size([10, 694])
attention_mask shape: torch.Size([10, 694])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 694


===== Batch 1219 =====
QIDs: [35, 35, 35, 35, 35, 35, 35, 35, 35, 35]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [228, 136, 215, 222, 173, 216, 180, 105, 239, 201]
image sizes: [(1175, 900), (1175, 900), (1175, 900), (1175, 900), (1175, 900), (1175, 900), (1175, 900), (1175, 900), (1175, 900), (1175, 900)]
input_ids shape: torch.Size([10, 1487])
attention_mask shape: torch.Size([10, 1487])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1487


===== Batch 1220 =====
QIDs: [625, 625, 625, 625, 625, 625, 625, 625, 625, 625]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [282, 172, 284, 275, 246, 269, 253, 147, 281, 271]
image sizes: [(650, 269), (650, 269), (650, 269), (650, 269), (650, 269), (650, 269), (650, 269), (650, 269), (650, 269), (650, 269)]
input_ids shape: torch.Size([10, 424])
attention_mask shape: torch.Size([10, 424])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 424


===== Batch 1221 =====
QIDs: [526, 526, 526, 526, 526, 526, 526, 526, 526, 526]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [365, 314, 396, 370, 342, 356, 362, 286, 364, 351]
image sizes: [(1090, 495), (1090, 495), (1090, 495), (1090, 495), (1090, 495), (1090, 495), (1090, 495), (1090, 495), (1090, 495), (1090, 495)]
input_ids shape: torch.Size([10, 976])
attention_mask shape: torch.Size([10, 976])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 976


===== Batch 1222 =====
QIDs: [186, 186, 186, 186, 186, 186, 186, 186, 186, 186]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [256, 121, 238, 262, 226, 234, 203, 104, 268, 229]
image sizes: [(181, 260), (181, 260), (181, 260), (181, 260), (181, 260), (181, 260), (181, 260), (181, 260), (181, 260), (181, 260)]
input_ids shape: torch.Size([10, 209])
attention_mask shape: torch.Size([10, 209])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 209


===== Batch 1223 =====
QIDs: [695, 695, 695, 695, 695, 695, 695, 695, 695, 695]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1543, 678, 1527, 1691, 1266, 1387, 1244, 421, 1546, 1451]
image sizes: [(984, 1096), (984, 1096), (984, 1096), (984, 1096), (984, 1096), (984, 1096), (984, 1096), (984, 1096), (984, 1096), (984, 1096)]
input_ids shape: torch.Size([10, 2150])
attention_mask shape: torch.Size([10, 2150])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2150


===== Batch 1224 =====
QIDs: [14, 14, 14, 14, 14, 14, 14, 14, 14, 14]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [689, 347, 649, 704, 526, 599, 565, 216, 656, 692]
image sizes: [(493, 123), (493, 123), (493, 123), (493, 123), (493, 123), (493, 123), (493, 123), (493, 123), (493, 123), (493, 123)]
input_ids shape: torch.Size([10, 416])
attention_mask shape: torch.Size([10, 416])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 416


===== Batch 1225 =====
QIDs: [1218, 1218, 1218, 1218, 1218, 1218, 1218, 1218, 1218, 1218]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [515, 343, 472, 503, 456, 472, 445, 273, 527, 478]
image sizes: [(529, 69), (529, 69), (529, 69), (529, 69), (529, 69), (529, 69), (529, 69), (529, 69), (529, 69), (529, 69)]
input_ids shape: torch.Size([10, 363])
attention_mask shape: torch.Size([10, 363])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 363


===== Batch 1226 =====
QIDs: [1072, 1072, 1072, 1072, 1072, 1072, 1072, 1072, 1072, 1072]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [4287, 2106, 4080, 4475, 3500, 3978, 3836, 1276, 4461, 4377]
image sizes: [(872, 556), (872, 556), (872, 556), (872, 556), (872, 556), (872, 556), (872, 556), (872, 556), (872, 556), (872, 556)]
input_ids shape: torch.Size([10, 2729])
attention_mask shape: torch.Size([10, 2729])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2729


===== Batch 1227 =====
QIDs: [1450, 1450, 1450, 1450, 1450, 1450, 1450, 1450, 1450, 1450]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [620, 383, 641, 546, 616, 646, 605, 309, 750, 608]
image sizes: [(502, 250), (502, 250), (502, 250), (502, 250), (502, 250), (502, 250), (502, 250), (502, 250), (502, 250), (502, 250)]
input_ids shape: torch.Size([10, 598])
attention_mask shape: torch.Size([10, 598])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 598


===== Batch 1228 =====
QIDs: [1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [712, 394, 674, 696, 660, 689, 593, 284, 761, 699]
image sizes: [(807, 259), (807, 259), (807, 259), (807, 259), (807, 259), (807, 259), (807, 259), (807, 259), (807, 259), (807, 259)]
input_ids shape: torch.Size([10, 661])
attention_mask shape: torch.Size([10, 661])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 661


===== Batch 1229 =====
QIDs: [1681, 1681, 1681, 1681, 1681, 1681, 1681, 1681, 1681, 1681]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [2317, 953, 2163, 2533, 1852, 2189, 1721, 574, 2454, 2119]
image sizes: [(660, 230), (660, 230), (660, 230), (660, 230), (660, 230), (660, 230), (660, 230), (660, 230), (660, 230), (660, 230)]
input_ids shape: torch.Size([10, 1273])
attention_mask shape: torch.Size([10, 1273])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1273


===== Batch 1230 =====
QIDs: [336, 336, 336, 336, 336, 336, 336, 336, 336, 336]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [435, 192, 403, 385, 395, 406, 361, 152, 472, 426]
image sizes: [(1141, 1281), (1141, 1281), (1141, 1281), (1141, 1281), (1141, 1281), (1141, 1281), (1141, 1281), (1141, 1281), (1141, 1281), (1141, 1281)]
input_ids shape: torch.Size([10, 2124])
attention_mask shape: torch.Size([10, 2124])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2124


===== Batch 1231 =====
QIDs: [550, 550, 550, 550, 550, 550, 550, 550, 550, 550]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [646, 594, 656, 658, 643, 638, 621, 567, 658, 658]
image sizes: [(537, 156), (537, 156), (537, 156), (537, 156), (537, 156), (537, 156), (537, 156), (537, 156), (537, 156), (537, 156)]
input_ids shape: torch.Size([10, 523])
attention_mask shape: torch.Size([10, 523])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 523


===== Batch 1232 =====
QIDs: [554, 554, 554, 554, 554, 554, 554, 554, 554, 554]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [305, 220, 299, 339, 286, 303, 291, 209, 296, 301]
image sizes: [(322, 268), (322, 268), (322, 268), (322, 268), (322, 268), (322, 268), (322, 268), (322, 268), (322, 268), (322, 268)]
input_ids shape: torch.Size([10, 363])
attention_mask shape: torch.Size([10, 363])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 363


===== Batch 1233 =====
QIDs: [906, 906, 906, 906, 906, 906, 906, 906, 906, 906]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [485, 408, 462, 501, 455, 461, 604, 380, 471, 442]
image sizes: [(417, 293), (417, 293), (417, 293), (417, 293), (417, 293), (417, 293), (417, 293), (417, 293), (417, 293), (417, 293)]
input_ids shape: torch.Size([10, 599])
attention_mask shape: torch.Size([10, 599])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 599


===== Batch 1234 =====
QIDs: [247, 247, 247, 247, 247, 247, 247, 247, 247, 247]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [83, 72, 87, 84, 80, 86, 83, 67, 102, 93]
image sizes: [(300, 292), (300, 292), (300, 292), (300, 292), (300, 292), (300, 292), (300, 292), (300, 292), (300, 292), (300, 292)]
input_ids shape: torch.Size([10, 182])
attention_mask shape: torch.Size([10, 182])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 182


===== Batch 1235 =====
QIDs: [471, 471, 471, 471, 471, 471, 471, 471, 471, 471]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [797, 474, 729, 888, 731, 839, 677, 288, 964, 807]
image sizes: [(962, 184), (962, 184), (962, 184), (962, 184), (962, 184), (962, 184), (962, 184), (962, 184), (962, 184), (962, 184)]
input_ids shape: torch.Size([10, 665])
attention_mask shape: torch.Size([10, 665])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 665


===== Batch 1236 =====
QIDs: [904, 904, 904, 904, 904, 904, 904, 904, 904, 904]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [98, 62, 101, 104, 101, 102, 98, 53, 99, 115]
image sizes: [(388, 153), (388, 153), (388, 153), (388, 153), (388, 153), (388, 153), (388, 153), (388, 153), (388, 153), (388, 153)]
input_ids shape: torch.Size([10, 146])
attention_mask shape: torch.Size([10, 146])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 146


===== Batch 1237 =====
QIDs: [768, 768, 768, 768, 768, 768, 768, 768, 768, 768]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [432, 201, 406, 478, 361, 402, 361, 133, 467, 393]
image sizes: [(915, 1356), (915, 1356), (915, 1356), (915, 1356), (915, 1356), (915, 1356), (915, 1356), (915, 1356), (915, 1356), (915, 1356)]
input_ids shape: torch.Size([10, 1814])
attention_mask shape: torch.Size([10, 1814])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1814


===== Batch 1238 =====
QIDs: [588, 588, 588, 588, 588, 588, 588, 588, 588, 588]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [419, 235, 324, 383, 292, 327, 348, 157, 338, 381]
image sizes: [(592, 124), (592, 124), (592, 124), (592, 124), (592, 124), (592, 124), (592, 124), (592, 124), (592, 124), (592, 124)]
input_ids shape: torch.Size([10, 312])
attention_mask shape: torch.Size([10, 312])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 312


===== Batch 1239 =====
QIDs: [746, 746, 746, 746, 746, 746, 746, 746, 746, 746]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [237, 132, 244, 274, 190, 228, 223, 84, 284, 230]
image sizes: [(490, 278), (490, 278), (490, 278), (490, 278), (490, 278), (490, 278), (490, 278), (490, 278), (490, 278), (490, 278)]
input_ids shape: torch.Size([10, 324])
attention_mask shape: torch.Size([10, 324])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 324


===== Batch 1240 =====
QIDs: [1028, 1028, 1028, 1028, 1028, 1028, 1028, 1028, 1028, 1028]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [209, 170, 203, 216, 189, 203, 195, 124, 191, 219]
image sizes: [(728, 438), (728, 438), (728, 438), (728, 438), (728, 438), (728, 438), (728, 438), (728, 438), (728, 438), (728, 438)]
input_ids shape: torch.Size([10, 551])
attention_mask shape: torch.Size([10, 551])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 551


===== Batch 1241 =====
QIDs: [1676, 1676, 1676, 1676, 1676, 1676, 1676, 1676, 1676, 1676]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [2354, 1154, 2267, 2368, 1927, 2191, 1973, 768, 2332, 2375]
image sizes: [(867, 521), (867, 521), (867, 521), (867, 521), (867, 521), (867, 521), (867, 521), (867, 521), (867, 521), (867, 521)]
input_ids shape: torch.Size([10, 1723])
attention_mask shape: torch.Size([10, 1723])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1723


===== Batch 1242 =====
QIDs: [621, 621, 621, 621, 621, 621, 621, 621, 621, 621]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [445, 337, 458, 451, 434, 460, 469, 247, 425, 401]
image sizes: [(395, 185), (395, 185), (395, 185), (395, 185), (395, 185), (395, 185), (395, 185), (395, 185), (395, 185), (395, 185)]
input_ids shape: torch.Size([10, 383])
attention_mask shape: torch.Size([10, 383])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 383


===== Batch 1243 =====
QIDs: [88, 88, 88, 88, 88, 88, 88, 88, 88, 88]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [448, 279, 408, 443, 355, 387, 379, 206, 419, 421]
image sizes: [(360, 240), (360, 240), (360, 240), (360, 240), (360, 240), (360, 240), (360, 240), (360, 240), (360, 240), (360, 240)]
input_ids shape: torch.Size([10, 381])
attention_mask shape: torch.Size([10, 381])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 381


===== Batch 1244 =====
QIDs: [1349, 1349, 1349, 1349, 1349, 1349, 1349, 1349, 1349, 1349]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [452, 236, 400, 424, 312, 411, 373, 162, 483, 473]
image sizes: [(524, 714), (524, 714), (524, 714), (524, 714), (524, 714), (524, 714), (524, 714), (524, 714), (524, 714), (524, 714)]
input_ids shape: torch.Size([10, 724])
attention_mask shape: torch.Size([10, 724])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 724


===== Batch 1245 =====
QIDs: [414, 414, 414, 414, 414, 414, 414, 414, 414, 414]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [286, 134, 266, 276, 275, 284, 211, 96, 294, 237]
image sizes: [(800, 780), (800, 780), (800, 780), (800, 780), (800, 780), (800, 780), (800, 780), (800, 780), (800, 780), (800, 780)]
input_ids shape: torch.Size([10, 978])
attention_mask shape: torch.Size([10, 978])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 978


===== Batch 1246 =====
QIDs: [1113, 1113, 1113, 1113, 1113, 1113, 1113, 1113, 1113, 1113]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [434, 235, 444, 445, 380, 441, 401, 182, 453, 424]
image sizes: [(606, 147), (606, 147), (606, 147), (606, 147), (606, 147), (606, 147), (606, 147), (606, 147), (606, 147), (606, 147)]
input_ids shape: torch.Size([10, 384])
attention_mask shape: torch.Size([10, 384])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 384


===== Batch 1247 =====
QIDs: [1701, 1701, 1701, 1701, 1701, 1701, 1701, 1701, 1701, 1701]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [239, 174, 234, 273, 217, 235, 266, 156, 245, 229]
image sizes: [(609, 121), (609, 121), (609, 121), (609, 121), (609, 121), (609, 121), (609, 121), (609, 121), (609, 121), (609, 121)]
input_ids shape: torch.Size([10, 307])
attention_mask shape: torch.Size([10, 307])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 307


===== Batch 1248 =====
QIDs: [1662, 1662, 1662, 1662, 1662, 1662, 1662, 1662, 1662, 1662]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [361, 210, 340, 371, 315, 324, 304, 170, 373, 368]
image sizes: [(360, 270), (360, 270), (360, 270), (360, 270), (360, 270), (360, 270), (360, 270), (360, 270), (360, 270), (360, 270)]
input_ids shape: torch.Size([10, 354])
attention_mask shape: torch.Size([10, 354])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 354


===== Batch 1249 =====
QIDs: [1638, 1638, 1638, 1638, 1638, 1638, 1638, 1638, 1638, 1638]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [195, 122, 186, 181, 182, 187, 190, 96, 181, 186]
image sizes: [(299, 283), (299, 283), (299, 283), (299, 283), (299, 283), (299, 283), (299, 283), (299, 283), (299, 283), (299, 283)]
input_ids shape: torch.Size([10, 237])
attention_mask shape: torch.Size([10, 237])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 237


===== Batch 1250 =====
QIDs: [11, 11, 11, 11, 11, 11, 11, 11, 11, 11]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [419, 251, 434, 424, 402, 422, 381, 196, 476, 488]
image sizes: [(1215, 250), (1215, 250), (1215, 250), (1215, 250), (1215, 250), (1215, 250), (1215, 250), (1215, 250), (1215, 250), (1215, 250)]
input_ids shape: torch.Size([10, 648])
attention_mask shape: torch.Size([10, 648])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 648


===== Batch 1251 =====
QIDs: [1605, 1605, 1605, 1605, 1605, 1605, 1605, 1605, 1605, 1605]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [409, 329, 398, 423, 402, 405, 370, 298, 425, 406]
image sizes: [(457, 219), (457, 219), (457, 219), (457, 219), (457, 219), (457, 219), (457, 219), (457, 219), (457, 219), (457, 219)]
input_ids shape: torch.Size([10, 399])
attention_mask shape: torch.Size([10, 399])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 399


===== Batch 1252 =====
QIDs: [597, 597, 597, 597, 597, 597, 597, 597, 597, 597]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [169, 119, 175, 179, 172, 174, 145, 82, 187, 152]
image sizes: [(987, 148), (987, 148), (987, 148), (987, 148), (987, 148), (987, 148), (987, 148), (987, 148), (987, 148), (987, 148)]
input_ids shape: torch.Size([10, 296])
attention_mask shape: torch.Size([10, 296])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 296


===== Batch 1253 =====
QIDs: [920, 920, 920, 920, 920, 920, 920, 920, 920, 920]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [478, 291, 474, 534, 406, 457, 414, 225, 507, 434]
image sizes: [(733, 341), (733, 341), (733, 341), (733, 341), (733, 341), (733, 341), (733, 341), (733, 341), (733, 341), (733, 341)]
input_ids shape: torch.Size([10, 611])
attention_mask shape: torch.Size([10, 611])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 611


===== Batch 1254 =====
QIDs: [1023, 1023, 1023, 1023, 1023, 1023, 1023, 1023, 1023, 1023]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [364, 220, 380, 370, 330, 337, 315, 168, 396, 328]
image sizes: [(606, 150), (606, 150), (606, 150), (606, 150), (606, 150), (606, 150), (606, 150), (606, 150), (606, 150), (606, 150)]
input_ids shape: torch.Size([10, 374])
attention_mask shape: torch.Size([10, 374])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 374


===== Batch 1255 =====
QIDs: [1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174, 1174]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [377, 274, 370, 400, 355, 366, 329, 256, 388, 389]
image sizes: [(1216, 286), (1216, 286), (1216, 286), (1216, 286), (1216, 286), (1216, 286), (1216, 286), (1216, 286), (1216, 286), (1216, 286)]
input_ids shape: torch.Size([10, 737])
attention_mask shape: torch.Size([10, 737])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 737


===== Batch 1256 =====
QIDs: [1667, 1667, 1667, 1667, 1667, 1667, 1667, 1667, 1667, 1667]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [372, 291, 355, 378, 312, 376, 336, 253, 379, 376]
image sizes: [(520, 160), (520, 160), (520, 160), (520, 160), (520, 160), (520, 160), (520, 160), (520, 160), (520, 160), (520, 160)]
input_ids shape: torch.Size([10, 299])
attention_mask shape: torch.Size([10, 299])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 299


===== Batch 1257 =====
QIDs: [362, 362, 362, 362, 362, 362, 362, 362, 362, 362]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [325, 191, 329, 331, 333, 307, 279, 166, 363, 315]
image sizes: [(1450, 218), (1450, 218), (1450, 218), (1450, 218), (1450, 218), (1450, 218), (1450, 218), (1450, 218), (1450, 218), (1450, 218)]
input_ids shape: torch.Size([10, 601])
attention_mask shape: torch.Size([10, 601])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 601


===== Batch 1258 =====
QIDs: [766, 766, 766, 766, 766, 766, 766, 766, 766, 766]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [615, 343, 600, 651, 576, 609, 608, 213, 700, 625]
image sizes: [(680, 357), (680, 357), (680, 357), (680, 357), (680, 357), (680, 357), (680, 357), (680, 357), (680, 357), (680, 357)]
input_ids shape: torch.Size([10, 700])
attention_mask shape: torch.Size([10, 700])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 700


===== Batch 1259 =====
QIDs: [697, 697, 697, 697, 697, 697, 697, 697, 697, 697]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [662, 274, 652, 640, 657, 612, 579, 217, 684, 670]
image sizes: [(732, 568), (732, 568), (732, 568), (732, 568), (732, 568), (732, 568), (732, 568), (732, 568), (732, 568), (732, 568)]
input_ids shape: torch.Size([10, 872])
attention_mask shape: torch.Size([10, 872])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 872


===== Batch 1260 =====
QIDs: [991, 991, 991, 991, 991, 991, 991, 991, 991, 991]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [293, 198, 290, 305, 253, 290, 271, 162, 304, 293]
image sizes: [(325, 310), (325, 310), (325, 310), (325, 310), (325, 310), (325, 310), (325, 310), (325, 310), (325, 310), (325, 310)]
input_ids shape: torch.Size([10, 332])
attention_mask shape: torch.Size([10, 332])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 332


===== Batch 1261 =====
QIDs: [624, 624, 624, 624, 624, 624, 624, 624, 624, 624]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [328, 199, 320, 322, 362, 321, 291, 160, 334, 309]
image sizes: [(635, 251), (635, 251), (635, 251), (635, 251), (635, 251), (635, 251), (635, 251), (635, 251), (635, 251), (635, 251)]
input_ids shape: torch.Size([10, 417])
attention_mask shape: torch.Size([10, 417])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 417


===== Batch 1262 =====
QIDs: [1089, 1089, 1089, 1089, 1089, 1089, 1089, 1089, 1089, 1089]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [538, 248, 463, 521, 451, 476, 452, 191, 516, 467]
image sizes: [(602, 155), (602, 155), (602, 155), (602, 155), (602, 155), (602, 155), (602, 155), (602, 155), (602, 155), (602, 155)]
input_ids shape: torch.Size([10, 448])
attention_mask shape: torch.Size([10, 448])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 448


===== Batch 1263 =====
QIDs: [220, 220, 220, 220, 220, 220, 220, 220, 220, 220]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [691, 382, 628, 703, 660, 643, 587, 257, 660, 699]
image sizes: [(620, 224), (620, 224), (620, 224), (620, 224), (620, 224), (620, 224), (620, 224), (620, 224), (620, 224), (620, 224)]
input_ids shape: torch.Size([10, 552])
attention_mask shape: torch.Size([10, 552])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 552


===== Batch 1264 =====
QIDs: [955, 955, 955, 955, 955, 955, 955, 955, 955, 955]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [220, 130, 190, 137, 121, 129, 209, 126, 149, 147]
image sizes: [(1204, 1554), (1204, 1554), (1204, 1554), (1204, 1554), (1204, 1554), (1204, 1554), (1204, 1554), (1204, 1554), (1204, 1554), (1204, 1554)]
input_ids shape: torch.Size([10, 2569])
attention_mask shape: torch.Size([10, 2569])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2569


===== Batch 1265 =====
QIDs: [56, 56, 56, 56, 56, 56, 56, 56, 56, 56]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [291, 155, 307, 306, 281, 282, 273, 115, 338, 327]
image sizes: [(800, 600), (800, 600), (800, 600), (800, 600), (800, 600), (800, 600), (800, 600), (800, 600), (800, 600), (800, 600)]
input_ids shape: torch.Size([10, 792])
attention_mask shape: torch.Size([10, 792])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 792


===== Batch 1266 =====
QIDs: [1486, 1486, 1486, 1486, 1486, 1486, 1486, 1486, 1486, 1486]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [336, 163, 306, 320, 310, 322, 248, 122, 329, 313]
image sizes: [(459, 357), (459, 357), (459, 357), (459, 357), (459, 357), (459, 357), (459, 357), (459, 357), (459, 357), (459, 357)]
input_ids shape: torch.Size([10, 390])
attention_mask shape: torch.Size([10, 390])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 390


===== Batch 1267 =====
QIDs: [1686, 1686, 1686, 1686, 1686, 1686, 1686, 1686, 1686, 1686]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [2527, 1196, 2448, 2462, 2105, 2325, 1965, 733, 2482, 2471]
image sizes: [(1176, 544), (1176, 544), (1176, 544), (1176, 544), (1176, 544), (1176, 544), (1176, 544), (1176, 544), (1176, 544), (1176, 544)]
input_ids shape: torch.Size([10, 1953])
attention_mask shape: torch.Size([10, 1953])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1953


===== Batch 1268 =====
QIDs: [1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100, 1100]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [263, 177, 259, 257, 291, 258, 239, 140, 289, 269]
image sizes: [(606, 198), (606, 198), (606, 198), (606, 198), (606, 198), (606, 198), (606, 198), (606, 198), (606, 198), (606, 198)]
input_ids shape: torch.Size([10, 348])
attention_mask shape: torch.Size([10, 348])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 348


===== Batch 1269 =====
QIDs: [1690, 1690, 1690, 1690, 1690, 1690, 1690, 1690, 1690, 1690]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [235, 110, 242, 214, 200, 223, 183, 90, 233, 195]
image sizes: [(746, 622), (746, 622), (746, 622), (746, 622), (746, 622), (746, 622), (746, 622), (746, 622), (746, 622), (746, 622)]
input_ids shape: torch.Size([10, 743])
attention_mask shape: torch.Size([10, 743])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 743


===== Batch 1270 =====
QIDs: [252, 252, 252, 252, 252, 252, 252, 252, 252, 252]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [291, 177, 251, 238, 213, 236, 262, 149, 242, 294]
image sizes: [(351, 145), (351, 145), (351, 145), (351, 145), (351, 145), (351, 145), (351, 145), (351, 145), (351, 145), (351, 145)]
input_ids shape: torch.Size([10, 268])
attention_mask shape: torch.Size([10, 268])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 268


===== Batch 1271 =====
QIDs: [1017, 1017, 1017, 1017, 1017, 1017, 1017, 1017, 1017, 1017]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [345, 166, 321, 337, 273, 322, 262, 133, 363, 329]
image sizes: [(1186, 520), (1186, 520), (1186, 520), (1186, 520), (1186, 520), (1186, 520), (1186, 520), (1186, 520), (1186, 520), (1186, 520)]
input_ids shape: torch.Size([10, 986])
attention_mask shape: torch.Size([10, 986])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 986


===== Batch 1272 =====
QIDs: [1665, 1665, 1665, 1665, 1665, 1665, 1665, 1665, 1665, 1665]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [164, 101, 156, 151, 143, 143, 137, 62, 153, 163]
image sizes: [(820, 500), (820, 500), (820, 500), (820, 500), (820, 500), (820, 500), (820, 500), (820, 500), (820, 500), (820, 500)]
input_ids shape: torch.Size([10, 625])
attention_mask shape: torch.Size([10, 625])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 625


===== Batch 1273 =====
QIDs: [870, 870, 870, 870, 870, 870, 870, 870, 870, 870]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [628, 500, 582, 634, 586, 633, 572, 467, 601, 599]
image sizes: [(770, 520), (770, 520), (770, 520), (770, 520), (770, 520), (770, 520), (770, 520), (770, 520), (770, 520), (770, 520)]
input_ids shape: torch.Size([10, 929])
attention_mask shape: torch.Size([10, 929])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 929


===== Batch 1274 =====
QIDs: [237, 237, 237, 237, 237, 237, 237, 237, 237, 237]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [99, 74, 93, 97, 94, 104, 92, 64, 108, 90]
image sizes: [(578, 436), (578, 436), (578, 436), (578, 436), (578, 436), (578, 436), (578, 436), (578, 436), (578, 436), (578, 436)]
input_ids shape: torch.Size([10, 413])
attention_mask shape: torch.Size([10, 413])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 413


===== Batch 1275 =====
QIDs: [859, 859, 859, 859, 859, 859, 859, 859, 859, 859]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [227, 157, 202, 195, 207, 206, 201, 154, 215, 236]
image sizes: [(2400, 2406), (2400, 2406), (2400, 2406), (2400, 2406), (2400, 2406), (2400, 2406), (2400, 2406), (2400, 2406), (2400, 2406), (2400, 2406)]
input_ids shape: torch.Size([10, 7543])
attention_mask shape: torch.Size([10, 7543])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 7543


===== Batch 1276 =====
QIDs: [581, 581, 581, 581, 581, 581, 581, 581, 581, 581]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [264, 201, 268, 276, 271, 260, 268, 176, 272, 303]
image sizes: [(220, 303), (220, 303), (220, 303), (220, 303), (220, 303), (220, 303), (220, 303), (220, 303), (220, 303), (220, 303)]
input_ids shape: torch.Size([10, 298])
attention_mask shape: torch.Size([10, 298])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 298


===== Batch 1277 =====
QIDs: [1198, 1198, 1198, 1198, 1198, 1198, 1198, 1198, 1198, 1198]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [239, 142, 222, 262, 196, 218, 186, 107, 237, 234]
image sizes: [(820, 172), (820, 172), (820, 172), (820, 172), (820, 172), (820, 172), (820, 172), (820, 172), (820, 172), (820, 172)]
input_ids shape: torch.Size([10, 334])
attention_mask shape: torch.Size([10, 334])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 334


===== Batch 1278 =====
QIDs: [306, 306, 306, 306, 306, 306, 306, 306, 306, 306]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [622, 295, 592, 652, 663, 605, 526, 203, 765, 594]
image sizes: [(504, 334), (504, 334), (504, 334), (504, 334), (504, 334), (504, 334), (504, 334), (504, 334), (504, 334), (504, 334)]
input_ids shape: torch.Size([10, 573])
attention_mask shape: torch.Size([10, 573])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 573


===== Batch 1279 =====
QIDs: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [525, 321, 507, 617, 494, 491, 484, 241, 523, 559]
image sizes: [(390, 115), (390, 115), (390, 115), (390, 115), (390, 115), (390, 115), (390, 115), (390, 115), (390, 115), (390, 115)]
input_ids shape: torch.Size([10, 393])
attention_mask shape: torch.Size([10, 393])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 393


===== Batch 1280 =====
QIDs: [1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376, 1376]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [249, 160, 241, 260, 326, 251, 267, 120, 256, 309]
image sizes: [(200, 109), (200, 109), (200, 109), (200, 109), (200, 109), (200, 109), (200, 109), (200, 109), (200, 109), (200, 109)]
input_ids shape: torch.Size([10, 223])
attention_mask shape: torch.Size([10, 223])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 223


===== Batch 1281 =====
QIDs: [211, 211, 211, 211, 211, 211, 211, 211, 211, 211]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [235, 130, 271, 257, 167, 236, 261, 104, 251, 232]
image sizes: [(200, 331), (200, 331), (200, 331), (200, 331), (200, 331), (200, 331), (200, 331), (200, 331), (200, 331), (200, 331)]
input_ids shape: torch.Size([10, 281])
attention_mask shape: torch.Size([10, 281])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 281


===== Batch 1282 =====
QIDs: [1643, 1643, 1643, 1643, 1643, 1643, 1643, 1643, 1643, 1643]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [294, 153, 294, 279, 238, 295, 277, 129, 286, 305]
image sizes: [(213, 130), (213, 130), (213, 130), (213, 130), (213, 130), (213, 130), (213, 130), (213, 130), (213, 130), (213, 130)]
input_ids shape: torch.Size([10, 247])
attention_mask shape: torch.Size([10, 247])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 247


===== Batch 1283 =====
QIDs: [84, 84, 84, 84, 84, 84, 84, 84, 84, 84]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [492, 231, 420, 449, 355, 414, 464, 134, 435, 417]
image sizes: [(357, 137), (357, 137), (357, 137), (357, 137), (357, 137), (357, 137), (357, 137), (357, 137), (357, 137), (357, 137)]
input_ids shape: torch.Size([10, 340])
attention_mask shape: torch.Size([10, 340])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 340


===== Batch 1284 =====
QIDs: [993, 993, 993, 993, 993, 993, 993, 993, 993, 993]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [367, 282, 381, 369, 318, 338, 335, 249, 371, 375]
image sizes: [(386, 230), (386, 230), (386, 230), (386, 230), (386, 230), (386, 230), (386, 230), (386, 230), (386, 230), (386, 230)]
input_ids shape: torch.Size([10, 388])
attention_mask shape: torch.Size([10, 388])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 388


===== Batch 1285 =====
QIDs: [160, 160, 160, 160, 160, 160, 160, 160, 160, 160]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [232, 132, 251, 216, 188, 234, 198, 97, 230, 204]
image sizes: [(568, 800), (568, 800), (568, 800), (568, 800), (568, 800), (568, 800), (568, 800), (568, 800), (568, 800), (568, 800)]
input_ids shape: torch.Size([10, 737])
attention_mask shape: torch.Size([10, 737])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 737


===== Batch 1286 =====
QIDs: [1075, 1075, 1075, 1075, 1075, 1075, 1075, 1075, 1075, 1075]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [3177, 1482, 3014, 3191, 2589, 3046, 2820, 947, 3207, 3311]
image sizes: [(674, 486), (674, 486), (674, 486), (674, 486), (674, 486), (674, 486), (674, 486), (674, 486), (674, 486), (674, 486)]
input_ids shape: torch.Size([10, 1871])
attention_mask shape: torch.Size([10, 1871])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1871


===== Batch 1287 =====
QIDs: [962, 962, 962, 962, 962, 962, 962, 962, 962, 962]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [228, 121, 198, 236, 192, 213, 173, 86, 241, 217]
image sizes: [(662, 558), (662, 558), (662, 558), (662, 558), (662, 558), (662, 558), (662, 558), (662, 558), (662, 558), (662, 558)]
input_ids shape: torch.Size([10, 604])
attention_mask shape: torch.Size([10, 604])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 604


===== Batch 1288 =====
QIDs: [85, 85, 85, 85, 85, 85, 85, 85, 85, 85]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [299, 246, 303, 312, 273, 319, 302, 228, 324, 313]
image sizes: [(401, 196), (401, 196), (401, 196), (401, 196), (401, 196), (401, 196), (401, 196), (401, 196), (401, 196), (401, 196)]
input_ids shape: torch.Size([10, 344])
attention_mask shape: torch.Size([10, 344])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 344


===== Batch 1289 =====
QIDs: [998, 998, 998, 998, 998, 998, 998, 998, 998, 998]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [204, 139, 214, 248, 201, 207, 206, 104, 236, 219]
image sizes: [(512, 406), (512, 406), (512, 406), (512, 406), (512, 406), (512, 406), (512, 406), (512, 406), (512, 406), (512, 406)]
input_ids shape: torch.Size([10, 392])
attention_mask shape: torch.Size([10, 392])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 392


===== Batch 1290 =====
QIDs: [1572, 1572, 1572, 1572, 1572, 1572, 1572, 1572, 1572, 1572]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [239, 195, 232, 245, 244, 240, 223, 188, 229, 252]
image sizes: [(474, 458), (474, 458), (474, 458), (474, 458), (474, 458), (474, 458), (474, 458), (474, 458), (474, 458), (474, 458)]
input_ids shape: torch.Size([10, 441])
attention_mask shape: torch.Size([10, 441])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 441


===== Batch 1291 =====
QIDs: [296, 296, 296, 296, 296, 296, 296, 296, 296, 296]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [103, 76, 93, 98, 96, 100, 90, 64, 105, 86]
image sizes: [(148, 84), (148, 84), (148, 84), (148, 84), (148, 84), (148, 84), (148, 84), (148, 84), (148, 84), (148, 84)]
input_ids shape: torch.Size([10, 89])
attention_mask shape: torch.Size([10, 89])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 89


===== Batch 1292 =====
QIDs: [236, 236, 236, 236, 236, 236, 236, 236, 236, 236]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [324, 229, 316, 343, 298, 315, 305, 190, 339, 305]
image sizes: [(1526, 536), (1526, 536), (1526, 536), (1526, 536), (1526, 536), (1526, 536), (1526, 536), (1526, 536), (1526, 536), (1526, 536)]
input_ids shape: torch.Size([10, 1232])
attention_mask shape: torch.Size([10, 1232])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1232


===== Batch 1293 =====
QIDs: [36, 36, 36, 36, 36, 36, 36, 36, 36, 36]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [223, 139, 251, 240, 221, 237, 198, 99, 254, 249]
image sizes: [(2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920), (2560, 1920)]
input_ids shape: torch.Size([10, 6434])
attention_mask shape: torch.Size([10, 6434])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 6434


===== Batch 1294 =====
QIDs: [472, 472, 472, 472, 472, 472, 472, 472, 472, 472]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [241, 153, 247, 280, 213, 239, 211, 104, 280, 252]
image sizes: [(550, 519), (550, 519), (550, 519), (550, 519), (550, 519), (550, 519), (550, 519), (550, 519), (550, 519), (550, 519)]
input_ids shape: torch.Size([10, 527])
attention_mask shape: torch.Size([10, 527])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 527


===== Batch 1295 =====
QIDs: [1159, 1159, 1159, 1159, 1159, 1159, 1159, 1159, 1159, 1159]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [338, 236, 324, 381, 326, 308, 367, 193, 351, 309]
image sizes: [(343, 166), (343, 166), (343, 166), (343, 166), (343, 166), (343, 166), (343, 166), (343, 166), (343, 166), (343, 166)]
input_ids shape: torch.Size([10, 372])
attention_mask shape: torch.Size([10, 372])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 372


===== Batch 1296 =====
QIDs: [1583, 1583, 1583, 1583, 1583, 1583, 1583, 1583, 1583, 1583]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [59, 46, 61, 69, 55, 58, 60, 39, 67, 59]
image sizes: [(483, 220), (483, 220), (483, 220), (483, 220), (483, 220), (483, 220), (483, 220), (483, 220), (483, 220), (483, 220)]
input_ids shape: torch.Size([10, 193])
attention_mask shape: torch.Size([10, 193])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 193


===== Batch 1297 =====
QIDs: [480, 480, 480, 480, 480, 480, 480, 480, 480, 480]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [307, 184, 319, 316, 313, 309, 273, 149, 353, 350]
image sizes: [(1063, 283), (1063, 283), (1063, 283), (1063, 283), (1063, 283), (1063, 283), (1063, 283), (1063, 283), (1063, 283), (1063, 283)]
input_ids shape: torch.Size([10, 595])
attention_mask shape: torch.Size([10, 595])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 595


===== Batch 1298 =====
QIDs: [1286, 1286, 1286, 1286, 1286, 1286, 1286, 1286, 1286, 1286]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [402, 257, 373, 442, 336, 376, 400, 191, 406, 373]
image sizes: [(1148, 568), (1148, 568), (1148, 568), (1148, 568), (1148, 568), (1148, 568), (1148, 568), (1148, 568), (1148, 568), (1148, 568)]
input_ids shape: torch.Size([10, 1082])
attention_mask shape: torch.Size([10, 1082])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1082


===== Batch 1299 =====
QIDs: [1629, 1629, 1629, 1629, 1629, 1629, 1629, 1629, 1629, 1629]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [41, 29, 48, 43, 34, 45, 39, 23, 47, 38]
image sizes: [(1912, 187), (1912, 187), (1912, 187), (1912, 187), (1912, 187), (1912, 187), (1912, 187), (1912, 187), (1912, 187), (1912, 187)]
input_ids shape: torch.Size([10, 522])
attention_mask shape: torch.Size([10, 522])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 522


===== Batch 1300 =====
QIDs: [813, 813, 813, 813, 813, 813, 813, 813, 813, 813]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [394, 226, 398, 449, 371, 386, 355, 166, 392, 437]
image sizes: [(811, 442), (811, 442), (811, 442), (811, 442), (811, 442), (811, 442), (811, 442), (811, 442), (811, 442), (811, 442)]
input_ids shape: torch.Size([10, 708])
attention_mask shape: torch.Size([10, 708])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 708


===== Batch 1301 =====
QIDs: [1267, 1267, 1267, 1267, 1267, 1267, 1267, 1267, 1267, 1267]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [181, 109, 186, 174, 193, 178, 177, 103, 172, 189]
image sizes: [(200, 153), (200, 153), (200, 153), (200, 153), (200, 153), (200, 153), (200, 153), (200, 153), (200, 153), (200, 153)]
input_ids shape: torch.Size([10, 180])
attention_mask shape: torch.Size([10, 180])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 180


===== Batch 1302 =====
QIDs: [580, 580, 580, 580, 580, 580, 580, 580, 580, 580]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [305, 220, 304, 323, 326, 298, 282, 177, 301, 283]
image sizes: [(286, 226), (286, 226), (286, 226), (286, 226), (286, 226), (286, 226), (286, 226), (286, 226), (286, 226), (286, 226)]
input_ids shape: torch.Size([10, 303])
attention_mask shape: torch.Size([10, 303])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 303


===== Batch 1303 =====
QIDs: [1646, 1646, 1646, 1646, 1646, 1646, 1646, 1646, 1646, 1646]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [871, 566, 846, 870, 864, 836, 804, 549, 834, 851]
image sizes: [(301, 651), (301, 651), (301, 651), (301, 651), (301, 651), (301, 651), (301, 651), (301, 651), (301, 651), (301, 651)]
input_ids shape: torch.Size([10, 858])
attention_mask shape: torch.Size([10, 858])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 858


===== Batch 1304 =====
QIDs: [777, 777, 777, 777, 777, 777, 777, 777, 777, 777]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [598, 354, 630, 648, 550, 575, 579, 248, 737, 590]
image sizes: [(500, 193), (500, 193), (500, 193), (500, 193), (500, 193), (500, 193), (500, 193), (500, 193), (500, 193), (500, 193)]
input_ids shape: torch.Size([10, 489])
attention_mask shape: torch.Size([10, 489])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 489


===== Batch 1305 =====
QIDs: [469, 469, 469, 469, 469, 469, 469, 469, 469, 469]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [221, 134, 200, 188, 198, 202, 167, 81, 211, 217]
image sizes: [(547, 397), (547, 397), (547, 397), (547, 397), (547, 397), (547, 397), (547, 397), (547, 397), (547, 397), (547, 397)]
input_ids shape: torch.Size([10, 404])
attention_mask shape: torch.Size([10, 404])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 404


===== Batch 1306 =====
QIDs: [1558, 1558, 1558, 1558, 1558, 1558, 1558, 1558, 1558, 1558]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [293, 176, 267, 273, 247, 263, 273, 156, 292, 265]
image sizes: [(1045, 171), (1045, 171), (1045, 171), (1045, 171), (1045, 171), (1045, 171), (1045, 171), (1045, 171), (1045, 171), (1045, 171)]
input_ids shape: torch.Size([10, 428])
attention_mask shape: torch.Size([10, 428])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 428


===== Batch 1307 =====
QIDs: [1575, 1575, 1575, 1575, 1575, 1575, 1575, 1575, 1575, 1575]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [88, 64, 79, 83, 82, 84, 107, 50, 89, 95]
image sizes: [(406, 399), (406, 399), (406, 399), (406, 399), (406, 399), (406, 399), (406, 399), (406, 399), (406, 399), (406, 399)]
input_ids shape: torch.Size([10, 273])
attention_mask shape: torch.Size([10, 273])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 273


===== Batch 1308 =====
QIDs: [91, 91, 91, 91, 91, 91, 91, 91, 91, 91]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [447, 319, 457, 485, 484, 441, 457, 278, 500, 425]
image sizes: [(314, 156), (314, 156), (314, 156), (314, 156), (314, 156), (314, 156), (314, 156), (314, 156), (314, 156), (314, 156)]
input_ids shape: torch.Size([10, 403])
attention_mask shape: torch.Size([10, 403])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 403


===== Batch 1309 =====
QIDs: [1342, 1342, 1342, 1342, 1342, 1342, 1342, 1342, 1342, 1342]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [290, 151, 287, 313, 250, 312, 249, 105, 327, 291]
image sizes: [(389, 280), (389, 280), (389, 280), (389, 280), (389, 280), (389, 280), (389, 280), (389, 280), (389, 280), (389, 280)]
input_ids shape: torch.Size([10, 315])
attention_mask shape: torch.Size([10, 315])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 315


===== Batch 1310 =====
QIDs: [25, 25, 25, 25, 25, 25, 25, 25, 25, 25]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [180, 137, 171, 198, 160, 175, 158, 123, 183, 181]
image sizes: [(409, 169), (409, 169), (409, 169), (409, 169), (409, 169), (409, 169), (409, 169), (409, 169), (409, 169), (409, 169)]
input_ids shape: torch.Size([10, 234])
attention_mask shape: torch.Size([10, 234])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 234


===== Batch 1311 =====
QIDs: [1211, 1211, 1211, 1211, 1211, 1211, 1211, 1211, 1211, 1211]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [554, 443, 550, 541, 534, 502, 502, 254, 581, 534]
image sizes: [(709, 113), (709, 113), (709, 113), (709, 113), (709, 113), (709, 113), (709, 113), (709, 113), (709, 113), (709, 113)]
input_ids shape: torch.Size([10, 485])
attention_mask shape: torch.Size([10, 485])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 485


===== Batch 1312 =====
QIDs: [1306, 1306, 1306, 1306, 1306, 1306, 1306, 1306, 1306, 1306]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [206, 113, 212, 211, 183, 218, 189, 75, 235, 199]
image sizes: [(258, 148), (258, 148), (258, 148), (258, 148), (258, 148), (258, 148), (258, 148), (258, 148), (258, 148), (258, 148)]
input_ids shape: torch.Size([10, 190])
attention_mask shape: torch.Size([10, 190])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 190


===== Batch 1313 =====
QIDs: [524, 524, 524, 524, 524, 524, 524, 524, 524, 524]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [487, 463, 486, 502, 490, 484, 475, 449, 483, 485]
image sizes: [(408, 187), (408, 187), (408, 187), (408, 187), (408, 187), (408, 187), (408, 187), (408, 187), (408, 187), (408, 187)]
input_ids shape: torch.Size([10, 451])
attention_mask shape: torch.Size([10, 451])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 451


===== Batch 1314 =====
QIDs: [1156, 1156, 1156, 1156, 1156, 1156, 1156, 1156, 1156, 1156]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [353, 221, 343, 357, 383, 359, 308, 174, 379, 389]
image sizes: [(1219, 244), (1219, 244), (1219, 244), (1219, 244), (1219, 244), (1219, 244), (1219, 244), (1219, 244), (1219, 244), (1219, 244)]
input_ids shape: torch.Size([10, 621])
attention_mask shape: torch.Size([10, 621])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 621


===== Batch 1315 =====
QIDs: [661, 661, 661, 661, 661, 661, 661, 661, 661, 661]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [181, 132, 179, 194, 175, 186, 149, 96, 188, 186]
image sizes: [(1386, 921), (1386, 921), (1386, 921), (1386, 921), (1386, 921), (1386, 921), (1386, 921), (1386, 921), (1386, 921), (1386, 921)]
input_ids shape: torch.Size([10, 1767])
attention_mask shape: torch.Size([10, 1767])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1767


===== Batch 1316 =====
QIDs: [1335, 1335, 1335, 1335, 1335, 1335, 1335, 1335, 1335, 1335]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [144, 111, 137, 141, 131, 135, 130, 104, 145, 148]
image sizes: [(1172, 924), (1172, 924), (1172, 924), (1172, 924), (1172, 924), (1172, 924), (1172, 924), (1172, 924), (1172, 924), (1172, 924)]
input_ids shape: torch.Size([10, 1492])
attention_mask shape: torch.Size([10, 1492])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1492


===== Batch 1317 =====
QIDs: [463, 463, 463, 463, 463, 463, 463, 463, 463, 463]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [205, 130, 196, 194, 191, 187, 167, 104, 207, 186]
image sizes: [(360, 353), (360, 353), (360, 353), (360, 353), (360, 353), (360, 353), (360, 353), (360, 353), (360, 353), (360, 353)]
input_ids shape: torch.Size([10, 308])
attention_mask shape: torch.Size([10, 308])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 308


===== Batch 1318 =====
QIDs: [446, 446, 446, 446, 446, 446, 446, 446, 446, 446]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [285, 132, 294, 272, 254, 311, 288, 110, 299, 242]
image sizes: [(834, 1102), (834, 1102), (834, 1102), (834, 1102), (834, 1102), (834, 1102), (834, 1102), (834, 1102), (834, 1102), (834, 1102)]
input_ids shape: torch.Size([10, 1372])
attention_mask shape: torch.Size([10, 1372])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1372


===== Batch 1319 =====
QIDs: [403, 403, 403, 403, 403, 403, 403, 403, 403, 403]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [151, 94, 169, 171, 145, 152, 156, 87, 166, 142]
image sizes: [(2000, 746), (2000, 746), (2000, 746), (2000, 746), (2000, 746), (2000, 746), (2000, 746), (2000, 746), (2000, 746), (2000, 746)]
input_ids shape: torch.Size([10, 2045])
attention_mask shape: torch.Size([10, 2045])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2045


===== Batch 1320 =====
QIDs: [596, 596, 596, 596, 596, 596, 596, 596, 596, 596]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [253, 177, 260, 262, 234, 241, 227, 141, 269, 245]
image sizes: [(727, 231), (727, 231), (727, 231), (727, 231), (727, 231), (727, 231), (727, 231), (727, 231), (727, 231), (727, 231)]
input_ids shape: torch.Size([10, 385])
attention_mask shape: torch.Size([10, 385])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 385


===== Batch 1321 =====
QIDs: [1435, 1435, 1435, 1435, 1435, 1435, 1435, 1435, 1435, 1435]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [254, 189, 241, 278, 229, 244, 242, 156, 263, 271]
image sizes: [(300, 220), (300, 220), (300, 220), (300, 220), (300, 220), (300, 220), (300, 220), (300, 220), (300, 220), (300, 220)]
input_ids shape: torch.Size([10, 273])
attention_mask shape: torch.Size([10, 273])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 273


===== Batch 1322 =====
QIDs: [1415, 1415, 1415, 1415, 1415, 1415, 1415, 1415, 1415, 1415]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [577, 358, 560, 555, 532, 535, 521, 281, 581, 527]
image sizes: [(684, 290), (684, 290), (684, 290), (684, 290), (684, 290), (684, 290), (684, 290), (684, 290), (684, 290), (684, 290)]
input_ids shape: torch.Size([10, 593])
attention_mask shape: torch.Size([10, 593])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 593


===== Batch 1323 =====
QIDs: [9, 9, 9, 9, 9, 9, 9, 9, 9, 9]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [211, 153, 214, 201, 242, 208, 188, 143, 222, 225]
image sizes: [(735, 631), (735, 631), (735, 631), (735, 631), (735, 631), (735, 631), (735, 631), (735, 631), (735, 631), (735, 631)]
input_ids shape: torch.Size([10, 764])
attention_mask shape: torch.Size([10, 764])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 764


===== Batch 1324 =====
QIDs: [1675, 1675, 1675, 1675, 1675, 1675, 1675, 1675, 1675, 1675]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [146, 101, 137, 159, 119, 155, 137, 85, 165, 138]
image sizes: [(494, 342), (494, 342), (494, 342), (494, 342), (494, 342), (494, 342), (494, 342), (494, 342), (494, 342), (494, 342)]
input_ids shape: torch.Size([10, 319])
attention_mask shape: torch.Size([10, 319])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 319


===== Batch 1325 =====
QIDs: [775, 775, 775, 775, 775, 775, 775, 775, 775, 775]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [814, 558, 882, 865, 691, 879, 706, 360, 1053, 854]
image sizes: [(601, 272), (601, 272), (601, 272), (601, 272), (601, 272), (601, 272), (601, 272), (601, 272), (601, 272), (601, 272)]
input_ids shape: torch.Size([10, 724])
attention_mask shape: torch.Size([10, 724])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 724


===== Batch 1326 =====
QIDs: [1394, 1394, 1394, 1394, 1394, 1394, 1394, 1394, 1394, 1394]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [525, 427, 570, 553, 523, 542, 462, 282, 624, 608]
image sizes: [(709, 568), (709, 568), (709, 568), (709, 568), (709, 568), (709, 568), (709, 568), (709, 568), (709, 568), (709, 568)]
input_ids shape: torch.Size([10, 831])
attention_mask shape: torch.Size([10, 831])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 831


===== Batch 1327 =====
QIDs: [1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039, 1039]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1944, 851, 1937, 2176, 1556, 1763, 1576, 557, 2222, 2037]
image sizes: [(1354, 860), (1354, 860), (1354, 860), (1354, 860), (1354, 860), (1354, 860), (1354, 860), (1354, 860), (1354, 860), (1354, 860)]
input_ids shape: torch.Size([10, 2433])
attention_mask shape: torch.Size([10, 2433])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2433


===== Batch 1328 =====
QIDs: [1546, 1546, 1546, 1546, 1546, 1546, 1546, 1546, 1546, 1546]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [330, 252, 323, 347, 306, 338, 328, 225, 353, 320]
image sizes: [(580, 429), (580, 429), (580, 429), (580, 429), (580, 429), (580, 429), (580, 429), (580, 429), (580, 429), (580, 429)]
input_ids shape: torch.Size([10, 550])
attention_mask shape: torch.Size([10, 550])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 550


===== Batch 1329 =====
QIDs: [1497, 1497, 1497, 1497, 1497, 1497, 1497, 1497, 1497, 1497]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [256, 135, 250, 256, 207, 256, 246, 105, 285, 288]
image sizes: [(300, 300), (300, 300), (300, 300), (300, 300), (300, 300), (300, 300), (300, 300), (300, 300), (300, 300), (300, 300)]
input_ids shape: torch.Size([10, 289])
attention_mask shape: torch.Size([10, 289])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 289


===== Batch 1330 =====
QIDs: [198, 198, 198, 198, 198, 198, 198, 198, 198, 198]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [198, 126, 235, 183, 188, 226, 199, 103, 237, 234]
image sizes: [(518, 228), (518, 228), (518, 228), (518, 228), (518, 228), (518, 228), (518, 228), (518, 228), (518, 228), (518, 228)]
input_ids shape: torch.Size([10, 319])
attention_mask shape: torch.Size([10, 319])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 319


===== Batch 1331 =====
QIDs: [1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700, 1700]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [960, 428, 935, 989, 784, 934, 817, 307, 1005, 935]
image sizes: [(610, 119), (610, 119), (610, 119), (610, 119), (610, 119), (610, 119), (610, 119), (610, 119), (610, 119), (610, 119)]
input_ids shape: torch.Size([10, 612])
attention_mask shape: torch.Size([10, 612])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 612


===== Batch 1332 =====
QIDs: [909, 909, 909, 909, 909, 909, 909, 909, 909, 909]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [803, 575, 761, 798, 720, 758, 725, 477, 817, 785]
image sizes: [(809, 792), (809, 792), (809, 792), (809, 792), (809, 792), (809, 792), (809, 792), (809, 792), (809, 792), (809, 792)]
input_ids shape: torch.Size([10, 1282])
attention_mask shape: torch.Size([10, 1282])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1282


===== Batch 1333 =====
QIDs: [853, 853, 853, 853, 853, 853, 853, 853, 853, 853]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [113, 63, 107, 111, 89, 105, 85, 48, 105, 83]
image sizes: [(315, 393), (315, 393), (315, 393), (315, 393), (315, 393), (315, 393), (315, 393), (315, 393), (315, 393), (315, 393)]
input_ids shape: torch.Size([10, 228])
attention_mask shape: torch.Size([10, 228])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 228


===== Batch 1334 =====
QIDs: [667, 667, 667, 667, 667, 667, 667, 667, 667, 667]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [994, 496, 925, 923, 818, 985, 734, 280, 1031, 997]
image sizes: [(265, 203), (265, 203), (265, 203), (265, 203), (265, 203), (265, 203), (265, 203), (265, 203), (265, 203), (265, 203)]
input_ids shape: torch.Size([10, 593])
attention_mask shape: torch.Size([10, 593])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 593


===== Batch 1335 =====
QIDs: [1722, 1722, 1722, 1722, 1722, 1722, 1722, 1722, 1722, 1722]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [155, 94, 137, 153, 156, 138, 161, 77, 167, 167]
image sizes: [(752, 546), (752, 546), (752, 546), (752, 546), (752, 546), (752, 546), (752, 546), (752, 546), (752, 546), (752, 546)]
input_ids shape: torch.Size([10, 665])
attention_mask shape: torch.Size([10, 665])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 665


===== Batch 1336 =====
QIDs: [73, 73, 73, 73, 73, 73, 73, 73, 73, 73]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [294, 139, 301, 294, 247, 280, 260, 111, 292, 267]
image sizes: [(800, 503), (800, 503), (800, 503), (800, 503), (800, 503), (800, 503), (800, 503), (800, 503), (800, 503), (800, 503)]
input_ids shape: torch.Size([10, 714])
attention_mask shape: torch.Size([10, 714])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 714


===== Batch 1337 =====
QIDs: [538, 538, 538, 538, 538, 538, 538, 538, 538, 538]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [214, 155, 207, 228, 200, 197, 178, 138, 220, 238]
image sizes: [(406, 190), (406, 190), (406, 190), (406, 190), (406, 190), (406, 190), (406, 190), (406, 190), (406, 190), (406, 190)]
input_ids shape: torch.Size([10, 224])
attention_mask shape: torch.Size([10, 224])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 224


===== Batch 1338 =====
QIDs: [537, 537, 537, 537, 537, 537, 537, 537, 537, 537]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [295, 223, 293, 319, 260, 288, 261, 198, 298, 286]
image sizes: [(485, 220), (485, 220), (485, 220), (485, 220), (485, 220), (485, 220), (485, 220), (485, 220), (485, 220), (485, 220)]
input_ids shape: torch.Size([10, 355])
attention_mask shape: torch.Size([10, 355])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 355


===== Batch 1339 =====
QIDs: [286, 286, 286, 286, 286, 286, 286, 286, 286, 286]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [172, 136, 168, 182, 178, 163, 166, 126, 171, 176]
image sizes: [(592, 72), (592, 72), (592, 72), (592, 72), (592, 72), (592, 72), (592, 72), (592, 72), (592, 72), (592, 72)]
input_ids shape: torch.Size([10, 172])
attention_mask shape: torch.Size([10, 172])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 172


===== Batch 1340 =====
QIDs: [1155, 1155, 1155, 1155, 1155, 1155, 1155, 1155, 1155, 1155]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [209, 180, 195, 208, 198, 205, 207, 155, 238, 205]
image sizes: [(712, 181), (712, 181), (712, 181), (712, 181), (712, 181), (712, 181), (712, 181), (712, 181), (712, 181), (712, 181)]
input_ids shape: torch.Size([10, 320])
attention_mask shape: torch.Size([10, 320])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 320


===== Batch 1341 =====
QIDs: [861, 861, 861, 861, 861, 861, 861, 861, 861, 861]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [200, 188, 201, 211, 196, 203, 195, 174, 204, 212]
image sizes: [(550, 328), (550, 328), (550, 328), (550, 328), (550, 328), (550, 328), (550, 328), (550, 328), (550, 328), (550, 328)]
input_ids shape: torch.Size([10, 390])
attention_mask shape: torch.Size([10, 390])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 390


===== Batch 1342 =====
QIDs: [1561, 1561, 1561, 1561, 1561, 1561, 1561, 1561, 1561, 1561]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [605, 593, 606, 607, 606, 582, 605, 554, 605, 605]
image sizes: [(460, 161), (460, 161), (460, 161), (460, 161), (460, 161), (460, 161), (460, 161), (460, 161), (460, 161), (460, 161)]
input_ids shape: torch.Size([10, 548])
attention_mask shape: torch.Size([10, 548])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 548


===== Batch 1343 =====
QIDs: [1721, 1721, 1721, 1721, 1721, 1721, 1721, 1721, 1721, 1721]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [381, 205, 375, 405, 366, 368, 371, 142, 422, 403]
image sizes: [(1652, 1032), (1652, 1032), (1652, 1032), (1652, 1032), (1652, 1032), (1652, 1032), (1652, 1032), (1652, 1032), (1652, 1032), (1652, 1032)]
input_ids shape: torch.Size([10, 2394])
attention_mask shape: torch.Size([10, 2394])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2394


===== Batch 1344 =====
QIDs: [530, 530, 530, 530, 530, 530, 530, 530, 530, 530]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [347, 328, 367, 373, 342, 361, 337, 304, 364, 367]
image sizes: [(427, 143), (427, 143), (427, 143), (427, 143), (427, 143), (427, 143), (427, 143), (427, 143), (427, 143), (427, 143)]
input_ids shape: torch.Size([10, 317])
attention_mask shape: torch.Size([10, 317])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 317


===== Batch 1345 =====
QIDs: [197, 197, 197, 197, 197, 197, 197, 197, 197, 197]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [676, 352, 693, 674, 595, 671, 607, 231, 724, 622]
image sizes: [(782, 250), (782, 250), (782, 250), (782, 250), (782, 250), (782, 250), (782, 250), (782, 250), (782, 250), (782, 250)]
input_ids shape: torch.Size([10, 619])
attention_mask shape: torch.Size([10, 619])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 619


===== Batch 1346 =====
QIDs: [468, 468, 468, 468, 468, 468, 468, 468, 468, 468]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [362, 190, 339, 371, 274, 312, 298, 115, 382, 339]
image sizes: [(304, 112), (304, 112), (304, 112), (304, 112), (304, 112), (304, 112), (304, 112), (304, 112), (304, 112), (304, 112)]
input_ids shape: torch.Size([10, 229])
attention_mask shape: torch.Size([10, 229])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 229


===== Batch 1347 =====
QIDs: [1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204, 1204]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [602, 373, 615, 713, 499, 627, 504, 263, 619, 609]
image sizes: [(346, 486), (346, 486), (346, 486), (346, 486), (346, 486), (346, 486), (346, 486), (346, 486), (346, 486), (346, 486)]
input_ids shape: torch.Size([10, 579])
attention_mask shape: torch.Size([10, 579])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 579


===== Batch 1348 =====
QIDs: [1702, 1702, 1702, 1702, 1702, 1702, 1702, 1702, 1702, 1702]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [958, 485, 942, 991, 844, 920, 845, 326, 1045, 941]
image sizes: [(847, 221), (847, 221), (847, 221), (847, 221), (847, 221), (847, 221), (847, 221), (847, 221), (847, 221), (847, 221)]
input_ids shape: torch.Size([10, 770])
attention_mask shape: torch.Size([10, 770])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 770


===== Batch 1349 =====
QIDs: [310, 310, 310, 310, 310, 310, 310, 310, 310, 310]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [270, 135, 305, 314, 276, 280, 309, 107, 320, 266]
image sizes: [(321, 215), (321, 215), (321, 215), (321, 215), (321, 215), (321, 215), (321, 215), (321, 215), (321, 215), (321, 215)]
input_ids shape: torch.Size([10, 308])
attention_mask shape: torch.Size([10, 308])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 308


===== Batch 1350 =====
QIDs: [810, 810, 810, 810, 810, 810, 810, 810, 810, 810]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [547, 427, 670, 552, 570, 576, 564, 369, 597, 646]
image sizes: [(1216, 616), (1216, 616), (1216, 616), (1216, 616), (1216, 616), (1216, 616), (1216, 616), (1216, 616), (1216, 616), (1216, 616)]
input_ids shape: torch.Size([10, 1343])
attention_mask shape: torch.Size([10, 1343])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1343


===== Batch 1351 =====
QIDs: [1047, 1047, 1047, 1047, 1047, 1047, 1047, 1047, 1047, 1047]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [319, 141, 313, 326, 273, 327, 248, 122, 333, 294]
image sizes: [(472, 71), (472, 71), (472, 71), (472, 71), (472, 71), (472, 71), (472, 71), (472, 71), (472, 71), (472, 71)]
input_ids shape: torch.Size([10, 252])
attention_mask shape: torch.Size([10, 252])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 252


===== Batch 1352 =====
QIDs: [1193, 1193, 1193, 1193, 1193, 1193, 1193, 1193, 1193, 1193]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [275, 140, 272, 298, 232, 277, 223, 109, 315, 258]
image sizes: [(768, 512), (768, 512), (768, 512), (768, 512), (768, 512), (768, 512), (768, 512), (768, 512), (768, 512), (768, 512)]
input_ids shape: torch.Size([10, 646])
attention_mask shape: torch.Size([10, 646])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 646


===== Batch 1353 =====
QIDs: [790, 790, 790, 790, 790, 790, 790, 790, 790, 790]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [388, 205, 345, 375, 326, 338, 295, 136, 366, 354]
image sizes: [(654, 370), (654, 370), (654, 370), (654, 370), (654, 370), (654, 370), (654, 370), (654, 370), (654, 370), (654, 370)]
input_ids shape: torch.Size([10, 505])
attention_mask shape: torch.Size([10, 505])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 505


===== Batch 1354 =====
QIDs: [448, 448, 448, 448, 448, 448, 448, 448, 448, 448]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [383, 193, 352, 419, 293, 382, 350, 120, 399, 362]
image sizes: [(200, 149), (200, 149), (200, 149), (200, 149), (200, 149), (200, 149), (200, 149), (200, 149), (200, 149), (200, 149)]
input_ids shape: torch.Size([10, 244])
attention_mask shape: torch.Size([10, 244])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 244


===== Batch 1355 =====
QIDs: [1531, 1531, 1531, 1531, 1531, 1531, 1531, 1531, 1531, 1531]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [479, 315, 482, 525, 431, 454, 416, 269, 496, 453]
image sizes: [(1267, 382), (1267, 382), (1267, 382), (1267, 382), (1267, 382), (1267, 382), (1267, 382), (1267, 382), (1267, 382), (1267, 382)]
input_ids shape: torch.Size([10, 957])
attention_mask shape: torch.Size([10, 957])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 957


===== Batch 1356 =====
QIDs: [1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440, 1440]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [415, 278, 433, 454, 383, 455, 368, 225, 430, 411]
image sizes: [(628, 410), (628, 410), (628, 410), (628, 410), (628, 410), (628, 410), (628, 410), (628, 410), (628, 410), (628, 410)]
input_ids shape: torch.Size([10, 600])
attention_mask shape: torch.Size([10, 600])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 600


===== Batch 1357 =====
QIDs: [426, 426, 426, 426, 426, 426, 426, 426, 426, 426]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [446, 238, 458, 458, 364, 457, 440, 185, 523, 481]
image sizes: [(770, 756), (770, 756), (770, 756), (770, 756), (770, 756), (770, 756), (770, 756), (770, 756), (770, 756), (770, 756)]
input_ids shape: torch.Size([10, 1041])
attention_mask shape: torch.Size([10, 1041])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1041


===== Batch 1358 =====
QIDs: [1276, 1276, 1276, 1276, 1276, 1276, 1276, 1276, 1276, 1276]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [744, 378, 672, 711, 641, 677, 696, 265, 664, 670]
image sizes: [(1504, 880), (1504, 880), (1504, 880), (1504, 880), (1504, 880), (1504, 880), (1504, 880), (1504, 880), (1504, 880), (1504, 880)]
input_ids shape: torch.Size([10, 2077])
attention_mask shape: torch.Size([10, 2077])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2077


===== Batch 1359 =====
QIDs: [622, 622, 622, 622, 622, 622, 622, 622, 622, 622]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [629, 526, 607, 635, 587, 608, 634, 477, 672, 629]
image sizes: [(482, 235), (482, 235), (482, 235), (482, 235), (482, 235), (482, 235), (482, 235), (482, 235), (482, 235), (482, 235)]
input_ids shape: torch.Size([10, 622])
attention_mask shape: torch.Size([10, 622])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 622


===== Batch 1360 =====
QIDs: [356, 356, 356, 356, 356, 356, 356, 356, 356, 356]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [179, 129, 165, 206, 197, 177, 157, 89, 168, 192]
image sizes: [(700, 192), (700, 192), (700, 192), (700, 192), (700, 192), (700, 192), (700, 192), (700, 192), (700, 192), (700, 192)]
input_ids shape: torch.Size([10, 287])
attention_mask shape: torch.Size([10, 287])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 287


===== Batch 1361 =====
QIDs: [867, 867, 867, 867, 867, 867, 867, 867, 867, 867]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [485, 401, 479, 494, 496, 472, 500, 411, 506, 489]
image sizes: [(612, 386), (612, 386), (612, 386), (612, 386), (612, 386), (612, 386), (612, 386), (612, 386), (612, 386), (612, 386)]
input_ids shape: torch.Size([10, 565])
attention_mask shape: torch.Size([10, 565])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 565


===== Batch 1362 =====
QIDs: [1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114, 1114]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [383, 233, 434, 402, 357, 393, 384, 179, 417, 438]
image sizes: [(802, 143), (802, 143), (802, 143), (802, 143), (802, 143), (802, 143), (802, 143), (802, 143), (802, 143), (802, 143)]
input_ids shape: torch.Size([10, 432])
attention_mask shape: torch.Size([10, 432])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 432


===== Batch 1363 =====
QIDs: [459, 459, 459, 459, 459, 459, 459, 459, 459, 459]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [91, 82, 90, 90, 91, 94, 83, 71, 122, 96]
image sizes: [(532, 365), (532, 365), (532, 365), (532, 365), (532, 365), (532, 365), (532, 365), (532, 365), (532, 365), (532, 365)]
input_ids shape: torch.Size([10, 323])
attention_mask shape: torch.Size([10, 323])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 323


===== Batch 1364 =====
QIDs: [1233, 1233, 1233, 1233, 1233, 1233, 1233, 1233, 1233, 1233]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [265, 148, 311, 290, 248, 269, 270, 115, 310, 304]
image sizes: [(1134, 648), (1134, 648), (1134, 648), (1134, 648), (1134, 648), (1134, 648), (1134, 648), (1134, 648), (1134, 648), (1134, 648)]
input_ids shape: torch.Size([10, 1112])
attention_mask shape: torch.Size([10, 1112])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1112


===== Batch 1365 =====
QIDs: [1677, 1677, 1677, 1677, 1677, 1677, 1677, 1677, 1677, 1677]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [113, 69, 95, 98, 92, 95, 77, 56, 102, 105]
image sizes: [(504, 638), (504, 638), (504, 638), (504, 638), (504, 638), (504, 638), (504, 638), (504, 638), (504, 638), (504, 638)]
input_ids shape: torch.Size([10, 487])
attention_mask shape: torch.Size([10, 487])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 487


===== Batch 1366 =====
QIDs: [1188, 1188, 1188, 1188, 1188, 1188, 1188, 1188, 1188, 1188]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [575, 271, 586, 551, 483, 544, 528, 190, 607, 606]
image sizes: [(1200, 1600), (1200, 1600), (1200, 1600), (1200, 1600), (1200, 1600), (1200, 1600), (1200, 1600), (1200, 1600), (1200, 1600), (1200, 1600)]
input_ids shape: torch.Size([10, 2778])
attention_mask shape: torch.Size([10, 2778])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2778


===== Batch 1367 =====
QIDs: [869, 869, 869, 869, 869, 869, 869, 869, 869, 869]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [400, 235, 374, 469, 365, 336, 342, 207, 390, 386]
image sizes: [(530, 364), (530, 364), (530, 364), (530, 364), (530, 364), (530, 364), (530, 364), (530, 364), (530, 364), (530, 364)]
input_ids shape: torch.Size([10, 506])
attention_mask shape: torch.Size([10, 506])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 506


===== Batch 1368 =====
QIDs: [1460, 1460, 1460, 1460, 1460, 1460, 1460, 1460, 1460, 1460]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [507, 386, 459, 513, 448, 482, 450, 384, 519, 494]
image sizes: [(251, 117), (251, 117), (251, 117), (251, 117), (251, 117), (251, 117), (251, 117), (251, 117), (251, 117), (251, 117)]
input_ids shape: torch.Size([10, 340])
attention_mask shape: torch.Size([10, 340])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 340


===== Batch 1369 =====
QIDs: [838, 838, 838, 838, 838, 838, 838, 838, 838, 838]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [534, 315, 496, 547, 448, 511, 478, 258, 555, 497]
image sizes: [(439, 190), (439, 190), (439, 190), (439, 190), (439, 190), (439, 190), (439, 190), (439, 190), (439, 190), (439, 190)]
input_ids shape: torch.Size([10, 412])
attention_mask shape: torch.Size([10, 412])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 412


===== Batch 1370 =====
QIDs: [1398, 1398, 1398, 1398, 1398, 1398, 1398, 1398, 1398, 1398]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [399, 252, 377, 407, 344, 391, 390, 182, 394, 400]
image sizes: [(309, 243), (309, 243), (309, 243), (309, 243), (309, 243), (309, 243), (309, 243), (309, 243), (309, 243), (309, 243)]
input_ids shape: torch.Size([10, 366])
attention_mask shape: torch.Size([10, 366])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 366


===== Batch 1371 =====
QIDs: [680, 680, 680, 680, 680, 680, 680, 680, 680, 680]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [276, 150, 275, 295, 313, 269, 254, 112, 289, 268]
image sizes: [(810, 1076), (810, 1076), (810, 1076), (810, 1076), (810, 1076), (810, 1076), (810, 1076), (810, 1076), (810, 1076), (810, 1076)]
input_ids shape: torch.Size([10, 1282])
attention_mask shape: torch.Size([10, 1282])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1282


===== Batch 1372 =====
QIDs: [985, 985, 985, 985, 985, 985, 985, 985, 985, 985]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [195, 117, 175, 193, 172, 185, 162, 82, 194, 214]
image sizes: [(572, 280), (572, 280), (572, 280), (572, 280), (572, 280), (572, 280), (572, 280), (572, 280), (572, 280), (572, 280)]
input_ids shape: torch.Size([10, 307])
attention_mask shape: torch.Size([10, 307])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 307


===== Batch 1373 =====
QIDs: [1414, 1414, 1414, 1414, 1414, 1414, 1414, 1414, 1414, 1414]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [504, 288, 489, 491, 480, 476, 446, 245, 519, 478]
image sizes: [(439, 195), (439, 195), (439, 195), (439, 195), (439, 195), (439, 195), (439, 195), (439, 195), (439, 195), (439, 195)]
input_ids shape: torch.Size([10, 439])
attention_mask shape: torch.Size([10, 439])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 439


===== Batch 1374 =====
QIDs: [738, 738, 738, 738, 738, 738, 738, 738, 738, 738]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [270, 129, 267, 288, 193, 253, 254, 101, 275, 283]
image sizes: [(275, 183), (275, 183), (275, 183), (275, 183), (275, 183), (275, 183), (275, 183), (275, 183), (275, 183), (275, 183)]
input_ids shape: torch.Size([10, 243])
attention_mask shape: torch.Size([10, 243])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 243


===== Batch 1375 =====
QIDs: [363, 363, 363, 363, 363, 363, 363, 363, 363, 363]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [377, 239, 353, 399, 346, 398, 346, 189, 416, 384]
image sizes: [(922, 529), (922, 529), (922, 529), (922, 529), (922, 529), (922, 529), (922, 529), (922, 529), (922, 529), (922, 529)]
input_ids shape: torch.Size([10, 875])
attention_mask shape: torch.Size([10, 875])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 875


===== Batch 1376 =====
QIDs: [602, 602, 602, 602, 602, 602, 602, 602, 602, 602]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [374, 280, 387, 428, 694, 371, 343, 240, 378, 438]
image sizes: [(599, 293), (599, 293), (599, 293), (599, 293), (599, 293), (599, 293), (599, 293), (599, 293), (599, 293), (599, 293)]
input_ids shape: torch.Size([10, 553])
attention_mask shape: torch.Size([10, 553])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 553


===== Batch 1377 =====
QIDs: [1519, 1519, 1519, 1519, 1519, 1519, 1519, 1519, 1519, 1519]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [470, 268, 421, 455, 413, 442, 399, 205, 448, 456]
image sizes: [(1225, 601), (1225, 601), (1225, 601), (1225, 601), (1225, 601), (1225, 601), (1225, 601), (1225, 601), (1225, 601), (1225, 601)]
input_ids shape: torch.Size([10, 1230])
attention_mask shape: torch.Size([10, 1230])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1230


===== Batch 1378 =====
QIDs: [934, 934, 934, 934, 934, 934, 934, 934, 934, 934]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [218, 135, 209, 225, 185, 207, 221, 113, 242, 225]
image sizes: [(1557, 2057), (1557, 2057), (1557, 2057), (1557, 2057), (1557, 2057), (1557, 2057), (1557, 2057), (1557, 2057), (1557, 2057), (1557, 2057)]
input_ids shape: torch.Size([10, 4251])
attention_mask shape: torch.Size([10, 4251])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 4251


===== Batch 1379 =====
QIDs: [1347, 1347, 1347, 1347, 1347, 1347, 1347, 1347, 1347, 1347]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [353, 215, 391, 450, 327, 406, 332, 142, 429, 410]
image sizes: [(495, 300), (495, 300), (495, 300), (495, 300), (495, 300), (495, 300), (495, 300), (495, 300), (495, 300), (495, 300)]
input_ids shape: torch.Size([10, 433])
attention_mask shape: torch.Size([10, 433])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 433


===== Batch 1380 =====
QIDs: [837, 837, 837, 837, 837, 837, 837, 837, 837, 837]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [137, 102, 130, 142, 136, 120, 113, 92, 137, 133]
image sizes: [(264, 240), (264, 240), (264, 240), (264, 240), (264, 240), (264, 240), (264, 240), (264, 240), (264, 240), (264, 240)]
input_ids shape: torch.Size([10, 205])
attention_mask shape: torch.Size([10, 205])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 205


===== Batch 1381 =====
QIDs: [1419, 1419, 1419, 1419, 1419, 1419, 1419, 1419, 1419, 1419]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [289, 237, 298, 309, 286, 290, 296, 220, 282, 284]
image sizes: [(507, 325), (507, 325), (507, 325), (507, 325), (507, 325), (507, 325), (507, 325), (507, 325), (507, 325), (507, 325)]
input_ids shape: torch.Size([10, 457])
attention_mask shape: torch.Size([10, 457])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 457


===== Batch 1382 =====
QIDs: [653, 653, 653, 653, 653, 653, 653, 653, 653, 653]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [758, 362, 667, 696, 615, 690, 632, 239, 749, 653]
image sizes: [(245, 105), (245, 105), (245, 105), (245, 105), (245, 105), (245, 105), (245, 105), (245, 105), (245, 105), (245, 105)]
input_ids shape: torch.Size([10, 440])
attention_mask shape: torch.Size([10, 440])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 440


===== Batch 1383 =====
QIDs: [1596, 1596, 1596, 1596, 1596, 1596, 1596, 1596, 1596, 1596]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [307, 243, 289, 346, 274, 314, 287, 190, 358, 293]
image sizes: [(656, 391), (656, 391), (656, 391), (656, 391), (656, 391), (656, 391), (656, 391), (656, 391), (656, 391), (656, 391)]
input_ids shape: torch.Size([10, 546])
attention_mask shape: torch.Size([10, 546])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 546


===== Batch 1384 =====
QIDs: [1177, 1177, 1177, 1177, 1177, 1177, 1177, 1177, 1177, 1177]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [223, 169, 231, 238, 194, 213, 206, 141, 219, 211]
image sizes: [(343, 192), (343, 192), (343, 192), (343, 192), (343, 192), (343, 192), (343, 192), (343, 192), (343, 192), (343, 192)]
input_ids shape: torch.Size([10, 260])
attention_mask shape: torch.Size([10, 260])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 260


===== Batch 1385 =====
QIDs: [899, 899, 899, 899, 899, 899, 899, 899, 899, 899]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [282, 151, 212, 281, 218, 211, 230, 126, 212, 269]
image sizes: [(1078, 376), (1078, 376), (1078, 376), (1078, 376), (1078, 376), (1078, 376), (1078, 376), (1078, 376), (1078, 376), (1078, 376)]
input_ids shape: torch.Size([10, 647])
attention_mask shape: torch.Size([10, 647])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 647


===== Batch 1386 =====
QIDs: [62, 62, 62, 62, 62, 62, 62, 62, 62, 62]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [386, 196, 403, 356, 318, 376, 374, 142, 418, 367]
image sizes: [(1080, 1920), (1080, 1920), (1080, 1920), (1080, 1920), (1080, 1920), (1080, 1920), (1080, 1920), (1080, 1920), (1080, 1920), (1080, 1920)]
input_ids shape: torch.Size([10, 2925])
attention_mask shape: torch.Size([10, 2925])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2925


===== Batch 1387 =====
QIDs: [794, 794, 794, 794, 794, 794, 794, 794, 794, 794]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [471, 279, 435, 477, 395, 437, 433, 209, 469, 442]
image sizes: [(1224, 309), (1224, 309), (1224, 309), (1224, 309), (1224, 309), (1224, 309), (1224, 309), (1224, 309), (1224, 309), (1224, 309)]
input_ids shape: torch.Size([10, 811])
attention_mask shape: torch.Size([10, 811])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 811


===== Batch 1388 =====
QIDs: [1679, 1679, 1679, 1679, 1679, 1679, 1679, 1679, 1679, 1679]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [2087, 992, 2043, 2231, 1809, 2066, 1805, 595, 2268, 2186]
image sizes: [(1204, 928), (1204, 928), (1204, 928), (1204, 928), (1204, 928), (1204, 928), (1204, 928), (1204, 928), (1204, 928), (1204, 928)]
input_ids shape: torch.Size([10, 2437])
attention_mask shape: torch.Size([10, 2437])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2437


===== Batch 1389 =====
QIDs: [523, 523, 523, 523, 523, 523, 523, 523, 523, 523]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [298, 238, 278, 320, 266, 277, 269, 211, 283, 290]
image sizes: [(420, 167), (420, 167), (420, 167), (420, 167), (420, 167), (420, 167), (420, 167), (420, 167), (420, 167), (420, 167)]
input_ids shape: torch.Size([10, 322])
attention_mask shape: torch.Size([10, 322])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 322


===== Batch 1390 =====
QIDs: [1416, 1416, 1416, 1416, 1416, 1416, 1416, 1416, 1416, 1416]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [416, 263, 390, 403, 381, 386, 336, 196, 477, 386]
image sizes: [(569, 598), (569, 598), (569, 598), (569, 598), (569, 598), (569, 598), (569, 598), (569, 598), (569, 598), (569, 598)]
input_ids shape: torch.Size([10, 670])
attention_mask shape: torch.Size([10, 670])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 670


===== Batch 1391 =====
QIDs: [611, 611, 611, 611, 611, 611, 611, 611, 611, 611]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1915, 1273, 1902, 1947, 1755, 1999, 1668, 833, 2057, 1933]
image sizes: [(427, 180), (427, 180), (427, 180), (427, 180), (427, 180), (427, 180), (427, 180), (427, 180), (427, 180), (427, 180)]
input_ids shape: torch.Size([10, 1151])
attention_mask shape: torch.Size([10, 1151])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1151


===== Batch 1392 =====
QIDs: [847, 847, 847, 847, 847, 847, 847, 847, 847, 847]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [176, 121, 148, 173, 126, 150, 133, 71, 179, 166]
image sizes: [(409, 397), (409, 397), (409, 397), (409, 397), (409, 397), (409, 397), (409, 397), (409, 397), (409, 397), (409, 397)]
input_ids shape: torch.Size([10, 316])
attention_mask shape: torch.Size([10, 316])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 316


===== Batch 1393 =====
QIDs: [590, 590, 590, 590, 590, 590, 590, 590, 590, 590]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1527, 859, 1533, 1616, 1393, 1468, 1416, 717, 1960, 1666]
image sizes: [(550, 293), (550, 293), (550, 293), (550, 293), (550, 293), (550, 293), (550, 293), (550, 293), (550, 293), (550, 293)]
input_ids shape: torch.Size([10, 1122])
attention_mask shape: torch.Size([10, 1122])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1122


===== Batch 1394 =====
QIDs: [1391, 1391, 1391, 1391, 1391, 1391, 1391, 1391, 1391, 1391]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [432, 234, 399, 470, 345, 400, 373, 178, 449, 469]
image sizes: [(695, 278), (695, 278), (695, 278), (695, 278), (695, 278), (695, 278), (695, 278), (695, 278), (695, 278), (695, 278)]
input_ids shape: torch.Size([10, 506])
attention_mask shape: torch.Size([10, 506])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 506


===== Batch 1395 =====
QIDs: [45, 45, 45, 45, 45, 45, 45, 45, 45, 45]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [382, 243, 394, 402, 332, 378, 336, 143, 422, 393]
image sizes: [(800, 600), (800, 600), (800, 600), (800, 600), (800, 600), (800, 600), (800, 600), (800, 600), (800, 600), (800, 600)]
input_ids shape: torch.Size([10, 848])
attention_mask shape: torch.Size([10, 848])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 848


===== Batch 1396 =====
QIDs: [432, 432, 432, 432, 432, 432, 432, 432, 432, 432]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [425, 188, 426, 460, 330, 408, 463, 164, 469, 382]
image sizes: [(200, 104), (200, 104), (200, 104), (200, 104), (200, 104), (200, 104), (200, 104), (200, 104), (200, 104), (200, 104)]
input_ids shape: torch.Size([10, 324])
attention_mask shape: torch.Size([10, 324])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 324


===== Batch 1397 =====
QIDs: [834, 834, 834, 834, 834, 834, 834, 834, 834, 834]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [680, 581, 676, 705, 644, 643, 689, 523, 681, 667]
image sizes: [(496, 384), (496, 384), (496, 384), (496, 384), (496, 384), (496, 384), (496, 384), (496, 384), (496, 384), (496, 384)]
input_ids shape: torch.Size([10, 654])
attention_mask shape: torch.Size([10, 654])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 654


===== Batch 1398 =====
QIDs: [214, 214, 214, 214, 214, 214, 214, 214, 214, 214]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [591, 288, 528, 571, 614, 536, 543, 204, 561, 594]
image sizes: [(572, 553), (572, 553), (572, 553), (572, 553), (572, 553), (572, 553), (572, 553), (572, 553), (572, 553), (572, 553)]
input_ids shape: torch.Size([10, 722])
attention_mask shape: torch.Size([10, 722])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 722


===== Batch 1399 =====
QIDs: [528, 528, 528, 528, 528, 528, 528, 528, 528, 528]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [607, 578, 607, 625, 595, 608, 591, 560, 609, 609]
image sizes: [(511, 218), (511, 218), (511, 218), (511, 218), (511, 218), (511, 218), (511, 218), (511, 218), (511, 218), (511, 218)]
input_ids shape: torch.Size([10, 577])
attention_mask shape: torch.Size([10, 577])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 577


===== Batch 1400 =====
QIDs: [1336, 1336, 1336, 1336, 1336, 1336, 1336, 1336, 1336, 1336]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [104, 92, 107, 100, 94, 101, 98, 84, 101, 111]
image sizes: [(348, 341), (348, 341), (348, 341), (348, 341), (348, 341), (348, 341), (348, 341), (348, 341), (348, 341), (348, 341)]
input_ids shape: torch.Size([10, 223])
attention_mask shape: torch.Size([10, 223])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 223


===== Batch 1401 =====
QIDs: [1149, 1149, 1149, 1149, 1149, 1149, 1149, 1149, 1149, 1149]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [211, 134, 227, 219, 196, 216, 224, 100, 234, 230]
image sizes: [(684, 776), (684, 776), (684, 776), (684, 776), (684, 776), (684, 776), (684, 776), (684, 776), (684, 776), (684, 776)]
input_ids shape: torch.Size([10, 847])
attention_mask shape: torch.Size([10, 847])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 847


===== Batch 1402 =====
QIDs: [343, 343, 343, 343, 343, 343, 343, 343, 343, 343]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [353, 201, 467, 427, 315, 410, 338, 140, 446, 434]
image sizes: [(996, 608), (996, 608), (996, 608), (996, 608), (996, 608), (996, 608), (996, 608), (996, 608), (996, 608), (996, 608)]
input_ids shape: torch.Size([10, 1047])
attention_mask shape: torch.Size([10, 1047])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1047


===== Batch 1403 =====
QIDs: [1325, 1325, 1325, 1325, 1325, 1325, 1325, 1325, 1325, 1325]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [577, 298, 544, 560, 495, 561, 524, 201, 599, 576]
image sizes: [(504, 336), (504, 336), (504, 336), (504, 336), (504, 336), (504, 336), (504, 336), (504, 336), (504, 336), (504, 336)]
input_ids shape: torch.Size([10, 550])
attention_mask shape: torch.Size([10, 550])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 550


===== Batch 1404 =====
QIDs: [430, 430, 430, 430, 430, 430, 430, 430, 430, 430]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [256, 144, 259, 265, 334, 241, 316, 108, 266, 262]
image sizes: [(461, 467), (461, 467), (461, 467), (461, 467), (461, 467), (461, 467), (461, 467), (461, 467), (461, 467), (461, 467)]
input_ids shape: torch.Size([10, 494])
attention_mask shape: torch.Size([10, 494])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 494


===== Batch 1405 =====
QIDs: [771, 771, 771, 771, 771, 771, 771, 771, 771, 771]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [573, 310, 612, 655, 456, 624, 533, 214, 659, 616]
image sizes: [(416, 338), (416, 338), (416, 338), (416, 338), (416, 338), (416, 338), (416, 338), (416, 338), (416, 338), (416, 338)]
input_ids shape: torch.Size([10, 482])
attention_mask shape: torch.Size([10, 482])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 482


===== Batch 1406 =====
QIDs: [1201, 1201, 1201, 1201, 1201, 1201, 1201, 1201, 1201, 1201]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [455, 303, 497, 475, 446, 475, 385, 234, 514, 446]
image sizes: [(423, 366), (423, 366), (423, 366), (423, 366), (423, 366), (423, 366), (423, 366), (423, 366), (423, 366), (423, 366)]
input_ids shape: torch.Size([10, 509])
attention_mask shape: torch.Size([10, 509])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 509


===== Batch 1407 =====
QIDs: [1270, 1270, 1270, 1270, 1270, 1270, 1270, 1270, 1270, 1270]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [666, 279, 519, 546, 616, 682, 528, 248, 663, 608]
image sizes: [(200, 133), (200, 133), (200, 133), (200, 133), (200, 133), (200, 133), (200, 133), (200, 133), (200, 133), (200, 133)]
input_ids shape: torch.Size([10, 378])
attention_mask shape: torch.Size([10, 378])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 378


===== Batch 1408 =====
QIDs: [217, 217, 217, 217, 217, 217, 217, 217, 217, 217]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [169, 105, 183, 177, 172, 198, 190, 85, 201, 183]
image sizes: [(740, 561), (740, 561), (740, 561), (740, 561), (740, 561), (740, 561), (740, 561), (740, 561), (740, 561), (740, 561)]
input_ids shape: torch.Size([10, 666])
attention_mask shape: torch.Size([10, 666])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 666


===== Batch 1409 =====
QIDs: [331, 331, 331, 331, 331, 331, 331, 331, 331, 331]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [233, 139, 244, 280, 262, 242, 261, 108, 245, 267]
image sizes: [(188, 240), (188, 240), (188, 240), (188, 240), (188, 240), (188, 240), (188, 240), (188, 240), (188, 240), (188, 240)]
input_ids shape: torch.Size([10, 245])
attention_mask shape: torch.Size([10, 245])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 245


===== Batch 1410 =====
QIDs: [314, 314, 314, 314, 314, 314, 314, 314, 314, 314]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [562, 271, 528, 508, 513, 513, 522, 188, 592, 565]
image sizes: [(504, 329), (504, 329), (504, 329), (504, 329), (504, 329), (504, 329), (504, 329), (504, 329), (504, 329), (504, 329)]
input_ids shape: torch.Size([10, 547])
attention_mask shape: torch.Size([10, 547])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 547


===== Batch 1411 =====
QIDs: [102, 102, 102, 102, 102, 102, 102, 102, 102, 102]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [408, 291, 400, 433, 429, 403, 379, 228, 418, 463]
image sizes: [(609, 261), (609, 261), (609, 261), (609, 261), (609, 261), (609, 261), (609, 261), (609, 261), (609, 261), (609, 261)]
input_ids shape: torch.Size([10, 469])
attention_mask shape: torch.Size([10, 469])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 469


===== Batch 1412 =====
QIDs: [449, 449, 449, 449, 449, 449, 449, 449, 449, 449]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [236, 127, 209, 239, 196, 205, 211, 129, 218, 275]
image sizes: [(694, 534), (694, 534), (694, 534), (694, 534), (694, 534), (694, 534), (694, 534), (694, 534), (694, 534), (694, 534)]
input_ids shape: torch.Size([10, 628])
attention_mask shape: torch.Size([10, 628])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 628


===== Batch 1413 =====
QIDs: [1090, 1090, 1090, 1090, 1090, 1090, 1090, 1090, 1090, 1090]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [185, 132, 180, 209, 168, 181, 182, 115, 191, 177]
image sizes: [(609, 121), (609, 121), (609, 121), (609, 121), (609, 121), (609, 121), (609, 121), (609, 121), (609, 121), (609, 121)]
input_ids shape: torch.Size([10, 246])
attention_mask shape: torch.Size([10, 246])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 246


===== Batch 1414 =====
QIDs: [1597, 1597, 1597, 1597, 1597, 1597, 1597, 1597, 1597, 1597]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [149, 115, 149, 149, 148, 148, 143, 98, 161, 157]
image sizes: [(793, 317), (793, 317), (793, 317), (793, 317), (793, 317), (793, 317), (793, 317), (793, 317), (793, 317), (793, 317)]
input_ids shape: torch.Size([10, 434])
attention_mask shape: torch.Size([10, 434])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 434


===== Batch 1415 =====
QIDs: [32, 32, 32, 32, 32, 32, 32, 32, 32, 32]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [272, 144, 264, 292, 222, 278, 222, 102, 279, 234]
image sizes: [(357, 166), (357, 166), (357, 166), (357, 166), (357, 166), (357, 166), (357, 166), (357, 166), (357, 166), (357, 166)]
input_ids shape: torch.Size([10, 229])
attention_mask shape: torch.Size([10, 229])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 229


===== Batch 1416 =====
QIDs: [916, 916, 916, 916, 916, 916, 916, 916, 916, 916]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [601, 392, 612, 598, 525, 597, 525, 291, 639, 609]
image sizes: [(857, 651), (857, 651), (857, 651), (857, 651), (857, 651), (857, 651), (857, 651), (857, 651), (857, 651), (857, 651)]
input_ids shape: torch.Size([10, 1093])
attention_mask shape: torch.Size([10, 1093])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1093


===== Batch 1417 =====
QIDs: [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [330, 187, 357, 366, 287, 384, 308, 143, 393, 322]
image sizes: [(622, 180), (622, 180), (622, 180), (622, 180), (622, 180), (622, 180), (622, 180), (622, 180), (622, 180), (622, 180)]
input_ids shape: torch.Size([10, 356])
attention_mask shape: torch.Size([10, 356])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 356


===== Batch 1418 =====
QIDs: [885, 885, 885, 885, 885, 885, 885, 885, 885, 885]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [348, 320, 329, 412, 326, 321, 372, 238, 376, 364]
image sizes: [(475, 402), (475, 402), (475, 402), (475, 402), (475, 402), (475, 402), (475, 402), (475, 402), (475, 402), (475, 402)]
input_ids shape: torch.Size([10, 478])
attention_mask shape: torch.Size([10, 478])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 478


===== Batch 1419 =====
QIDs: [1049, 1049, 1049, 1049, 1049, 1049, 1049, 1049, 1049, 1049]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [3526, 1776, 3406, 3819, 2943, 3295, 2974, 1092, 3516, 3351]
image sizes: [(702, 579), (702, 579), (702, 579), (702, 579), (702, 579), (702, 579), (702, 579), (702, 579), (702, 579), (702, 579)]
input_ids shape: torch.Size([10, 2252])
attention_mask shape: torch.Size([10, 2252])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2252


===== Batch 1420 =====
QIDs: [764, 764, 764, 764, 764, 764, 764, 764, 764, 764]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [429, 204, 419, 436, 368, 408, 369, 140, 439, 379]
image sizes: [(1017, 962), (1017, 962), (1017, 962), (1017, 962), (1017, 962), (1017, 962), (1017, 962), (1017, 962), (1017, 962), (1017, 962)]
input_ids shape: torch.Size([10, 1460])
attention_mask shape: torch.Size([10, 1460])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1460


===== Batch 1421 =====
QIDs: [748, 748, 748, 748, 748, 748, 748, 748, 748, 748]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [111, 103, 97, 105, 109, 99, 103, 88, 103, 105]
image sizes: [(497, 186), (497, 186), (497, 186), (497, 186), (497, 186), (497, 186), (497, 186), (497, 186), (497, 186), (497, 186)]
input_ids shape: torch.Size([10, 235])
attention_mask shape: torch.Size([10, 235])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 235


===== Batch 1422 =====
QIDs: [1589, 1589, 1589, 1589, 1589, 1589, 1589, 1589, 1589, 1589]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [81, 53, 73, 77, 66, 80, 71, 44, 77, 67]
image sizes: [(416, 109), (416, 109), (416, 109), (416, 109), (416, 109), (416, 109), (416, 109), (416, 109), (416, 109), (416, 109)]
input_ids shape: torch.Size([10, 127])
attention_mask shape: torch.Size([10, 127])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 127


===== Batch 1423 =====
QIDs: [1052, 1052, 1052, 1052, 1052, 1052, 1052, 1052, 1052, 1052]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [216, 114, 223, 214, 283, 226, 190, 109, 237, 312]
image sizes: [(388, 788), (388, 788), (388, 788), (388, 788), (388, 788), (388, 788), (388, 788), (388, 788), (388, 788), (388, 788)]
input_ids shape: torch.Size([10, 543])
attention_mask shape: torch.Size([10, 543])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 543


===== Batch 1424 =====
QIDs: [1569, 1569, 1569, 1569, 1569, 1569, 1569, 1569, 1569, 1569]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [275, 189, 265, 281, 274, 279, 250, 161, 266, 272]
image sizes: [(463, 361), (463, 361), (463, 361), (463, 361), (463, 361), (463, 361), (463, 361), (463, 361), (463, 361), (463, 361)]
input_ids shape: torch.Size([10, 422])
attention_mask shape: torch.Size([10, 422])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 422


===== Batch 1425 =====
QIDs: [323, 323, 323, 323, 323, 323, 323, 323, 323, 323]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [660, 318, 660, 655, 637, 647, 621, 222, 693, 600]
image sizes: [(504, 336), (504, 336), (504, 336), (504, 336), (504, 336), (504, 336), (504, 336), (504, 336), (504, 336), (504, 336)]
input_ids shape: torch.Size([10, 631])
attention_mask shape: torch.Size([10, 631])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 631


===== Batch 1426 =====
QIDs: [407, 407, 407, 407, 407, 407, 407, 407, 407, 407]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [591, 285, 499, 669, 460, 525, 491, 220, 603, 576]
image sizes: [(240, 117), (240, 117), (240, 117), (240, 117), (240, 117), (240, 117), (240, 117), (240, 117), (240, 117), (240, 117)]
input_ids shape: torch.Size([10, 358])
attention_mask shape: torch.Size([10, 358])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 358


===== Batch 1427 =====
QIDs: [1340, 1340, 1340, 1340, 1340, 1340, 1340, 1340, 1340, 1340]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [195, 161, 196, 196, 170, 193, 176, 144, 222, 219]
image sizes: [(604, 478), (604, 478), (604, 478), (604, 478), (604, 478), (604, 478), (604, 478), (604, 478), (604, 478), (604, 478)]
input_ids shape: torch.Size([10, 517])
attention_mask shape: torch.Size([10, 517])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 517


===== Batch 1428 =====
QIDs: [1191, 1191, 1191, 1191, 1191, 1191, 1191, 1191, 1191, 1191]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [222, 118, 217, 213, 206, 185, 171, 99, 237, 202]
image sizes: [(2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536), (2048, 1536)]
input_ids shape: torch.Size([10, 4160])
attention_mask shape: torch.Size([10, 4160])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 4160


===== Batch 1429 =====
QIDs: [196, 196, 196, 196, 196, 196, 196, 196, 196, 196]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [433, 293, 433, 462, 448, 488, 415, 227, 466, 472]
image sizes: [(532, 416), (532, 416), (532, 416), (532, 416), (532, 416), (532, 416), (532, 416), (532, 416), (532, 416), (532, 416)]
input_ids shape: torch.Size([10, 545])
attention_mask shape: torch.Size([10, 545])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 545


===== Batch 1430 =====
QIDs: [473, 473, 473, 473, 473, 473, 473, 473, 473, 473]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [499, 237, 510, 496, 533, 416, 471, 180, 467, 502]
image sizes: [(607, 450), (607, 450), (607, 450), (607, 450), (607, 450), (607, 450), (607, 450), (607, 450), (607, 450), (607, 450)]
input_ids shape: torch.Size([10, 663])
attention_mask shape: torch.Size([10, 663])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 663


===== Batch 1431 =====
QIDs: [601, 601, 601, 601, 601, 601, 601, 601, 601, 601]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [716, 436, 777, 635, 699, 691, 617, 271, 658, 589]
image sizes: [(599, 213), (599, 213), (599, 213), (599, 213), (599, 213), (599, 213), (599, 213), (599, 213), (599, 213), (599, 213)]
input_ids shape: torch.Size([10, 605])
attention_mask shape: torch.Size([10, 605])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 605


===== Batch 1432 =====
QIDs: [1612, 1612, 1612, 1612, 1612, 1612, 1612, 1612, 1612, 1612]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [130, 90, 110, 125, 113, 111, 107, 72, 123, 117]
image sizes: [(818, 222), (818, 222), (818, 222), (818, 222), (818, 222), (818, 222), (818, 222), (818, 222), (818, 222), (818, 222)]
input_ids shape: torch.Size([10, 313])
attention_mask shape: torch.Size([10, 313])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 313


===== Batch 1433 =====
QIDs: [225, 225, 225, 225, 225, 225, 225, 225, 225, 225]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [174, 164, 177, 176, 165, 184, 165, 85, 185, 175]
image sizes: [(630, 1042), (630, 1042), (630, 1042), (630, 1042), (630, 1042), (630, 1042), (630, 1042), (630, 1042), (630, 1042), (630, 1042)]
input_ids shape: torch.Size([10, 924])
attention_mask shape: torch.Size([10, 924])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 924


===== Batch 1434 =====
QIDs: [235, 235, 235, 235, 235, 235, 235, 235, 235, 235]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [241, 105, 237, 225, 238, 230, 226, 97, 244, 243]
image sizes: [(1077, 1113), (1077, 1113), (1077, 1113), (1077, 1113), (1077, 1113), (1077, 1113), (1077, 1113), (1077, 1113), (1077, 1113), (1077, 1113)]
input_ids shape: torch.Size([10, 1680])
attention_mask shape: torch.Size([10, 1680])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1680


===== Batch 1435 =====
QIDs: [1064, 1064, 1064, 1064, 1064, 1064, 1064, 1064, 1064, 1064]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [1129, 497, 1144, 1117, 888, 1117, 995, 335, 1156, 1078]
image sizes: [(1444, 590), (1444, 590), (1444, 590), (1444, 590), (1444, 590), (1444, 590), (1444, 590), (1444, 590), (1444, 590), (1444, 590)]
input_ids shape: torch.Size([10, 1652])
attention_mask shape: torch.Size([10, 1652])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1652


===== Batch 1436 =====
QIDs: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [377, 242, 384, 415, 361, 382, 330, 206, 399, 389]
image sizes: [(730, 177), (730, 177), (730, 177), (730, 177), (730, 177), (730, 177), (730, 177), (730, 177), (730, 177), (730, 177)]
input_ids shape: torch.Size([10, 419])
attention_mask shape: torch.Size([10, 419])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 419


===== Batch 1437 =====
QIDs: [1383, 1383, 1383, 1383, 1383, 1383, 1383, 1383, 1383, 1383]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [315, 159, 322, 300, 333, 330, 356, 133, 330, 261]
image sizes: [(688, 482), (688, 482), (688, 482), (688, 482), (688, 482), (688, 482), (688, 482), (688, 482), (688, 482), (688, 482)]
input_ids shape: torch.Size([10, 656])
attention_mask shape: torch.Size([10, 656])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 656


===== Batch 1438 =====
QIDs: [868, 868, 868, 868, 868, 868, 868, 868, 868, 868]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [195, 151, 182, 207, 176, 173, 185, 122, 189, 181]
image sizes: [(963, 471), (963, 471), (963, 471), (963, 471), (963, 471), (963, 471), (963, 471), (963, 471), (963, 471), (963, 471)]
input_ids shape: torch.Size([10, 748])
attention_mask shape: torch.Size([10, 748])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 748


===== Batch 1439 =====
QIDs: [1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [420, 198, 439, 458, 352, 418, 399, 145, 507, 429]
image sizes: [(607, 117), (607, 117), (607, 117), (607, 117), (607, 117), (607, 117), (607, 117), (607, 117), (607, 117), (607, 117)]
input_ids shape: torch.Size([10, 334])
attention_mask shape: torch.Size([10, 334])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 334


===== Batch 1440 =====
QIDs: [975, 975, 975, 975, 975, 975, 975, 975, 975, 975]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [440, 212, 421, 408, 379, 402, 364, 136, 450, 385]
image sizes: [(341, 367), (341, 367), (341, 367), (341, 367), (341, 367), (341, 367), (341, 367), (341, 367), (341, 367), (341, 367)]
input_ids shape: torch.Size([10, 388])
attention_mask shape: torch.Size([10, 388])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 388


===== Batch 1441 =====
QIDs: [1571, 1571, 1571, 1571, 1571, 1571, 1571, 1571, 1571, 1571]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [529, 492, 515, 498, 496, 543, 511, 470, 553, 530]
image sizes: [(564, 416), (564, 416), (564, 416), (564, 416), (564, 416), (564, 416), (564, 416), (564, 416), (564, 416), (564, 416)]
input_ids shape: torch.Size([10, 598])
attention_mask shape: torch.Size([10, 598])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 598


===== Batch 1442 =====
QIDs: [957, 957, 957, 957, 957, 957, 957, 957, 957, 957]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [415, 171, 385, 388, 365, 359, 378, 141, 422, 383]
image sizes: [(906, 1190), (906, 1190), (906, 1190), (906, 1190), (906, 1190), (906, 1190), (906, 1190), (906, 1190), (906, 1190), (906, 1190)]
input_ids shape: torch.Size([10, 1607])
attention_mask shape: torch.Size([10, 1607])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1607


===== Batch 1443 =====
QIDs: [571, 571, 571, 571, 571, 571, 571, 571, 571, 571]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [268, 197, 274, 274, 335, 265, 247, 179, 274, 269]
image sizes: [(342, 229), (342, 229), (342, 229), (342, 229), (342, 229), (342, 229), (342, 229), (342, 229), (342, 229), (342, 229)]
input_ids shape: torch.Size([10, 310])
attention_mask shape: torch.Size([10, 310])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 310


===== Batch 1444 =====
QIDs: [924, 924, 924, 924, 924, 924, 924, 924, 924, 924]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [411, 325, 401, 440, 388, 403, 376, 293, 420, 397]
image sizes: [(436, 200), (436, 200), (436, 200), (436, 200), (436, 200), (436, 200), (436, 200), (436, 200), (436, 200), (436, 200)]
input_ids shape: torch.Size([10, 348])
attention_mask shape: torch.Size([10, 348])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 348


===== Batch 1445 =====
QIDs: [570, 570, 570, 570, 570, 570, 570, 570, 570, 570]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [324, 219, 320, 344, 262, 315, 318, 171, 353, 326]
image sizes: [(262, 144), (262, 144), (262, 144), (262, 144), (262, 144), (262, 144), (262, 144), (262, 144), (262, 144), (262, 144)]
input_ids shape: torch.Size([10, 271])
attention_mask shape: torch.Size([10, 271])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 271


===== Batch 1446 =====
QIDs: [1429, 1429, 1429, 1429, 1429, 1429, 1429, 1429, 1429, 1429]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [387, 288, 384, 409, 368, 381, 383, 232, 420, 403]
image sizes: [(387, 175), (387, 175), (387, 175), (387, 175), (387, 175), (387, 175), (387, 175), (387, 175), (387, 175), (387, 175)]
input_ids shape: torch.Size([10, 367])
attention_mask shape: torch.Size([10, 367])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 367


===== Batch 1447 =====
QIDs: [989, 989, 989, 989, 989, 989, 989, 989, 989, 989]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [160, 98, 157, 154, 134, 141, 144, 75, 152, 162]
image sizes: [(408, 94), (408, 94), (408, 94), (408, 94), (408, 94), (408, 94), (408, 94), (408, 94), (408, 94), (408, 94)]
input_ids shape: torch.Size([10, 151])
attention_mask shape: torch.Size([10, 151])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 151


===== Batch 1448 =====
QIDs: [749, 749, 749, 749, 749, 749, 749, 749, 749, 749]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [327, 185, 281, 305, 266, 278, 277, 127, 302, 321]
image sizes: [(959, 679), (959, 679), (959, 679), (959, 679), (959, 679), (959, 679), (959, 679), (959, 679), (959, 679), (959, 679)]
input_ids shape: torch.Size([10, 998])
attention_mask shape: torch.Size([10, 998])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 998


===== Batch 1449 =====
QIDs: [946, 946, 946, 946, 946, 946, 946, 946, 946, 946]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [41, 37, 41, 41, 41, 44, 49, 41, 41, 41]
image sizes: [(1207, 294), (1207, 294), (1207, 294), (1207, 294), (1207, 294), (1207, 294), (1207, 294), (1207, 294), (1207, 294), (1207, 294)]
input_ids shape: torch.Size([10, 483])
attention_mask shape: torch.Size([10, 483])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 483


===== Batch 1450 =====
QIDs: [1181, 1181, 1181, 1181, 1181, 1181, 1181, 1181, 1181, 1181]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [304, 137, 303, 299, 276, 300, 287, 108, 321, 275]
image sizes: [(782, 776), (782, 776), (782, 776), (782, 776), (782, 776), (782, 776), (782, 776), (782, 776), (782, 776), (782, 776)]
input_ids shape: torch.Size([10, 971])
attention_mask shape: torch.Size([10, 971])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 971


===== Batch 1451 =====
QIDs: [227, 227, 227, 227, 227, 227, 227, 227, 227, 227]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [264, 145, 271, 253, 221, 251, 258, 100, 296, 262]
image sizes: [(684, 668), (684, 668), (684, 668), (684, 668), (684, 668), (684, 668), (684, 668), (684, 668), (684, 668), (684, 668)]
input_ids shape: torch.Size([10, 751])
attention_mask shape: torch.Size([10, 751])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 751


===== Batch 1452 =====
QIDs: [1603, 1603, 1603, 1603, 1603, 1603, 1603, 1603, 1603, 1603]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [988, 733, 934, 964, 919, 943, 884, 654, 1013, 948]
image sizes: [(449, 177), (449, 177), (449, 177), (449, 177), (449, 177), (449, 177), (449, 177), (449, 177), (449, 177), (449, 177)]
input_ids shape: torch.Size([10, 654])
attention_mask shape: torch.Size([10, 654])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 654


===== Batch 1453 =====
QIDs: [769, 769, 769, 769, 769, 769, 769, 769, 769, 769]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [562, 318, 542, 596, 476, 579, 470, 205, 610, 539]
image sizes: [(970, 253), (970, 253), (970, 253), (970, 253), (970, 253), (970, 253), (970, 253), (970, 253), (970, 253), (970, 253)]
input_ids shape: torch.Size([10, 645])
attention_mask shape: torch.Size([10, 645])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 645


===== Batch 1454 =====
QIDs: [583, 583, 583, 583, 583, 583, 583, 583, 583, 583]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [376, 285, 411, 457, 383, 415, 367, 239, 435, 423]
image sizes: [(322, 246), (322, 246), (322, 246), (322, 246), (322, 246), (322, 246), (322, 246), (322, 246), (322, 246), (322, 246)]
input_ids shape: torch.Size([10, 399])
attention_mask shape: torch.Size([10, 399])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 399


===== Batch 1455 =====
QIDs: [970, 970, 970, 970, 970, 970, 970, 970, 970, 970]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [230, 130, 224, 304, 226, 235, 206, 101, 276, 214]
image sizes: [(358, 344), (358, 344), (358, 344), (358, 344), (358, 344), (358, 344), (358, 344), (358, 344), (358, 344), (358, 344)]
input_ids shape: torch.Size([10, 324])
attention_mask shape: torch.Size([10, 324])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 324


===== Batch 1456 =====
QIDs: [1408, 1408, 1408, 1408, 1408, 1408, 1408, 1408, 1408, 1408]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [916, 891, 926, 919, 914, 925, 911, 887, 944, 939]
image sizes: [(528, 287), (528, 287), (528, 287), (528, 287), (528, 287), (528, 287), (528, 287), (528, 287), (528, 287), (528, 287)]
input_ids shape: torch.Size([10, 817])
attention_mask shape: torch.Size([10, 817])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 817


===== Batch 1457 =====
QIDs: [950, 950, 950, 950, 950, 950, 950, 950, 950, 950]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [432, 254, 391, 377, 375, 417, 403, 175, 484, 493]
image sizes: [(626, 156), (626, 156), (626, 156), (626, 156), (626, 156), (626, 156), (626, 156), (626, 156), (626, 156), (626, 156)]
input_ids shape: torch.Size([10, 424])
attention_mask shape: torch.Size([10, 424])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 424


===== Batch 1458 =====
QIDs: [974, 974, 974, 974, 974, 974, 974, 974, 974, 974]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [882, 446, 957, 925, 865, 918, 862, 316, 974, 860]
image sizes: [(363, 643), (363, 643), (363, 643), (363, 643), (363, 643), (363, 643), (363, 643), (363, 643), (363, 643), (363, 643)]
input_ids shape: torch.Size([10, 852])
attention_mask shape: torch.Size([10, 852])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 852


===== Batch 1459 =====
QIDs: [1226, 1226, 1226, 1226, 1226, 1226, 1226, 1226, 1226, 1226]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [124, 90, 153, 121, 116, 148, 111, 62, 144, 103]
image sizes: [(1090, 428), (1090, 428), (1090, 428), (1090, 428), (1090, 428), (1090, 428), (1090, 428), (1090, 428), (1090, 428), (1090, 428)]
input_ids shape: torch.Size([10, 690])
attention_mask shape: torch.Size([10, 690])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 690


===== Batch 1460 =====
QIDs: [1059, 1059, 1059, 1059, 1059, 1059, 1059, 1059, 1059, 1059]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [2559, 1222, 2495, 2734, 2295, 2398, 2260, 745, 2561, 2662]
image sizes: [(968, 600), (968, 600), (968, 600), (968, 600), (968, 600), (968, 600), (968, 600), (968, 600), (968, 600), (968, 600)]
input_ids shape: torch.Size([10, 1953])
attention_mask shape: torch.Size([10, 1953])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1953


===== Batch 1461 =====
QIDs: [595, 595, 595, 595, 595, 595, 595, 595, 595, 595]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [620, 380, 641, 548, 628, 647, 680, 319, 750, 602]
image sizes: [(502, 250), (502, 250), (502, 250), (502, 250), (502, 250), (502, 250), (502, 250), (502, 250), (502, 250), (502, 250)]
input_ids shape: torch.Size([10, 659])
attention_mask shape: torch.Size([10, 659])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 659


===== Batch 1462 =====
QIDs: [1656, 1656, 1656, 1656, 1656, 1656, 1656, 1656, 1656, 1656]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [557, 344, 539, 543, 461, 533, 459, 282, 550, 486]
image sizes: [(484, 362), (484, 362), (484, 362), (484, 362), (484, 362), (484, 362), (484, 362), (484, 362), (484, 362), (484, 362)]
input_ids shape: torch.Size([10, 526])
attention_mask shape: torch.Size([10, 526])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 526


===== Batch 1463 =====
QIDs: [740, 740, 740, 740, 740, 740, 740, 740, 740, 740]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [256, 142, 238, 281, 194, 252, 211, 108, 253, 233]
image sizes: [(387, 420), (387, 420), (387, 420), (387, 420), (387, 420), (387, 420), (387, 420), (387, 420), (387, 420), (387, 420)]
input_ids shape: torch.Size([10, 365])
attention_mask shape: torch.Size([10, 365])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 365


===== Batch 1464 =====
QIDs: [1003, 1003, 1003, 1003, 1003, 1003, 1003, 1003, 1003, 1003]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [259, 143, 271, 262, 211, 253, 215, 90, 277, 272]
image sizes: [(490, 322), (490, 322), (490, 322), (490, 322), (490, 322), (490, 322), (490, 322), (490, 322), (490, 322), (490, 322)]
input_ids shape: torch.Size([10, 364])
attention_mask shape: torch.Size([10, 364])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 364


===== Batch 1465 =====
QIDs: [51, 51, 51, 51, 51, 51, 51, 51, 51, 51]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [516, 343, 496, 575, 535, 498, 549, 202, 529, 566]
image sizes: [(405, 274), (405, 274), (405, 274), (405, 274), (405, 274), (405, 274), (405, 274), (405, 274), (405, 274), (405, 274)]
input_ids shape: torch.Size([10, 496])
attention_mask shape: torch.Size([10, 496])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 496


===== Batch 1466 =====
QIDs: [1102, 1102, 1102, 1102, 1102, 1102, 1102, 1102, 1102, 1102]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [402, 209, 410, 406, 361, 362, 349, 153, 416, 398]
image sizes: [(1462, 604), (1462, 604), (1462, 604), (1462, 604), (1462, 604), (1462, 604), (1462, 604), (1462, 604), (1462, 604), (1462, 604)]
input_ids shape: torch.Size([10, 1377])
attention_mask shape: torch.Size([10, 1377])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1377


===== Batch 1467 =====
QIDs: [1308, 1308, 1308, 1308, 1308, 1308, 1308, 1308, 1308, 1308]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [178, 114, 151, 177, 142, 151, 167, 114, 159, 178]
image sizes: [(868, 728), (868, 728), (868, 728), (868, 728), (868, 728), (868, 728), (868, 728), (868, 728), (868, 728), (868, 728)]
input_ids shape: torch.Size([10, 942])
attention_mask shape: torch.Size([10, 942])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 942


===== Batch 1468 =====
QIDs: [724, 724, 724, 724, 724, 724, 724, 724, 724, 724]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [429, 218, 441, 513, 395, 440, 397, 154, 484, 437]
image sizes: [(350, 350), (350, 350), (350, 350), (350, 350), (350, 350), (350, 350), (350, 350), (350, 350), (350, 350), (350, 350)]
input_ids shape: torch.Size([10, 405])
attention_mask shape: torch.Size([10, 405])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 405


===== Batch 1469 =====
QIDs: [662, 662, 662, 662, 662, 662, 662, 662, 662, 662]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [201, 116, 227, 193, 250, 145, 207, 103, 153, 132]
image sizes: [(396, 480), (396, 480), (396, 480), (396, 480), (396, 480), (396, 480), (396, 480), (396, 480), (396, 480), (396, 480)]
input_ids shape: torch.Size([10, 392])
attention_mask shape: torch.Size([10, 392])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 392


===== Batch 1470 =====
QIDs: [641, 641, 641, 641, 641, 641, 641, 641, 641, 641]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [86, 67, 89, 92, 91, 93, 92, 53, 93, 98]
image sizes: [(746, 719), (746, 719), (746, 719), (746, 719), (746, 719), (746, 719), (746, 719), (746, 719), (746, 719), (746, 719)]
input_ids shape: torch.Size([10, 770])
attention_mask shape: torch.Size([10, 770])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 770


===== Batch 1471 =====
QIDs: [640, 640, 640, 640, 640, 640, 640, 640, 640, 640]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [434, 411, 444, 439, 432, 435, 431, 375, 464, 440]
image sizes: [(724, 326), (724, 326), (724, 326), (724, 326), (724, 326), (724, 326), (724, 326), (724, 326), (724, 326), (724, 326)]
input_ids shape: torch.Size([10, 554])
attention_mask shape: torch.Size([10, 554])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 554


===== Batch 1472 =====
QIDs: [634, 634, 634, 634, 634, 634, 634, 634, 634, 634]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [182, 89, 159, 177, 136, 156, 134, 80, 167, 164]
image sizes: [(734, 676), (734, 676), (734, 676), (734, 676), (734, 676), (734, 676), (734, 676), (734, 676), (734, 676), (734, 676)]
input_ids shape: torch.Size([10, 731])
attention_mask shape: torch.Size([10, 731])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 731


===== Batch 1473 =====
QIDs: [123, 123, 123, 123, 123, 123, 123, 123, 123, 123]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [469, 267, 421, 488, 399, 436, 379, 173, 493, 443]
image sizes: [(712, 950), (712, 950), (712, 950), (712, 950), (712, 950), (712, 950), (712, 950), (712, 950), (712, 950), (712, 950)]
input_ids shape: torch.Size([10, 1098])
attention_mask shape: torch.Size([10, 1098])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1098


===== Batch 1474 =====
QIDs: [628, 628, 628, 628, 628, 628, 628, 628, 628, 628]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [87, 56, 80, 81, 85, 75, 77, 46, 87, 89]
image sizes: [(866, 861), (866, 861), (866, 861), (866, 861), (866, 861), (866, 861), (866, 861), (866, 861), (866, 861), (866, 861)]
input_ids shape: torch.Size([10, 1030])
attention_mask shape: torch.Size([10, 1030])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1030


===== Batch 1475 =====
QIDs: [742, 742, 742, 742, 742, 742, 742, 742, 742, 742]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [233, 114, 217, 227, 202, 212, 235, 75, 250, 227]
image sizes: [(702, 967), (702, 967), (702, 967), (702, 967), (702, 967), (702, 967), (702, 967), (702, 967), (702, 967), (702, 967)]
input_ids shape: torch.Size([10, 1024])
attention_mask shape: torch.Size([10, 1024])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1024


===== Batch 1476 =====
QIDs: [648, 648, 648, 648, 648, 648, 648, 648, 648, 648]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [276, 132, 318, 307, 260, 320, 238, 112, 371, 293]
image sizes: [(1446, 879), (1446, 879), (1446, 879), (1446, 879), (1446, 879), (1446, 879), (1446, 879), (1446, 879), (1446, 879), (1446, 879)]
input_ids shape: torch.Size([10, 1801])
attention_mask shape: torch.Size([10, 1801])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1801


===== Batch 1477 =====
QIDs: [639, 639, 639, 639, 639, 639, 639, 639, 639, 639]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [311, 123, 335, 280, 484, 374, 220, 109, 423, 255]
image sizes: [(651, 479), (651, 479), (651, 479), (651, 479), (651, 479), (651, 479), (651, 479), (651, 479), (651, 479), (651, 479)]
input_ids shape: torch.Size([10, 599])
attention_mask shape: torch.Size([10, 599])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 599


===== Batch 1478 =====
QIDs: [644, 644, 644, 644, 644, 644, 644, 644, 644, 644]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [205, 189, 203, 215, 201, 205, 195, 184, 207, 212]
image sizes: [(476, 829), (476, 829), (476, 829), (476, 829), (476, 829), (476, 829), (476, 829), (476, 829), (476, 829), (476, 829)]
input_ids shape: torch.Size([10, 641])
attention_mask shape: torch.Size([10, 641])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 641


===== Batch 1479 =====
QIDs: [650, 650, 650, 650, 650, 650, 650, 650, 650, 650]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [132, 84, 120, 126, 133, 117, 161, 79, 156, 149]
image sizes: [(1289, 1096), (1289, 1096), (1289, 1096), (1289, 1096), (1289, 1096), (1289, 1096), (1289, 1096), (1289, 1096), (1289, 1096), (1289, 1096)]
input_ids shape: torch.Size([10, 1928])
attention_mask shape: torch.Size([10, 1928])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1928


===== Batch 1480 =====
QIDs: [642, 642, 642, 642, 642, 642, 642, 642, 642, 642]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [405, 173, 379, 455, 398, 367, 344, 145, 390, 411]
image sizes: [(1401, 1006), (1401, 1006), (1401, 1006), (1401, 1006), (1401, 1006), (1401, 1006), (1401, 1006), (1401, 1006), (1401, 1006), (1401, 1006)]
input_ids shape: torch.Size([10, 2035])
attention_mask shape: torch.Size([10, 2035])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2035


===== Batch 1481 =====
QIDs: [140, 140, 140, 140, 140, 140, 140, 140, 140, 140]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [117, 81, 111, 127, 109, 94, 94, 58, 101, 122]
image sizes: [(1528, 738), (1528, 738), (1528, 738), (1528, 738), (1528, 738), (1528, 738), (1528, 738), (1528, 738), (1528, 738), (1528, 738)]
input_ids shape: torch.Size([10, 1513])
attention_mask shape: torch.Size([10, 1513])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1513


===== Batch 1482 =====
QIDs: [632, 632, 632, 632, 632, 632, 632, 632, 632, 632]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [183, 107, 158, 180, 157, 167, 181, 94, 206, 157]
image sizes: [(1414, 1171), (1414, 1171), (1414, 1171), (1414, 1171), (1414, 1171), (1414, 1171), (1414, 1171), (1414, 1171), (1414, 1171), (1414, 1171)]
input_ids shape: torch.Size([10, 2232])
attention_mask shape: torch.Size([10, 2232])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 2232


===== Batch 1483 =====
QIDs: [654, 654, 654, 654, 654, 654, 654, 654, 654, 654]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [239, 121, 260, 229, 260, 249, 247, 96, 285, 277]
image sizes: [(1109, 1131), (1109, 1131), (1109, 1131), (1109, 1131), (1109, 1131), (1109, 1131), (1109, 1131), (1109, 1131), (1109, 1131), (1109, 1131)]
input_ids shape: torch.Size([10, 1770])
attention_mask shape: torch.Size([10, 1770])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1770


===== Batch 1484 =====
QIDs: [652, 652, 652, 652, 652, 652, 652, 652, 652, 652]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [502, 220, 470, 516, 448, 484, 443, 174, 525, 519]
image sizes: [(1246, 1059), (1246, 1059), (1246, 1059), (1246, 1059), (1246, 1059), (1246, 1059), (1246, 1059), (1246, 1059), (1246, 1059), (1246, 1059)]
input_ids shape: torch.Size([10, 1944])
attention_mask shape: torch.Size([10, 1944])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1944


===== Batch 1485 =====
QIDs: [643, 643, 643, 643, 643, 643, 643, 643, 643, 643]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [381, 173, 394, 306, 353, 310, 328, 163, 345, 367]
image sizes: [(480, 370), (480, 370), (480, 370), (480, 370), (480, 370), (480, 370), (480, 370), (480, 370), (480, 370), (480, 370)]
input_ids shape: torch.Size([10, 443])
attention_mask shape: torch.Size([10, 443])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 443


===== Batch 1486 =====
QIDs: [638, 638, 638, 638, 638, 638, 638, 638, 638, 638]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [115, 78, 112, 119, 103, 119, 92, 56, 140, 111]
image sizes: [(811, 979), (811, 979), (811, 979), (811, 979), (811, 979), (811, 979), (811, 979), (811, 979), (811, 979), (811, 979)]
input_ids shape: torch.Size([10, 1094])
attention_mask shape: torch.Size([10, 1094])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1094


===== Batch 1487 =====
QIDs: [729, 729, 729, 729, 729, 729, 729, 729, 729, 729]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [233, 105, 227, 246, 175, 220, 178, 90, 250, 215]
image sizes: [(1120, 825), (1120, 825), (1120, 825), (1120, 825), (1120, 825), (1120, 825), (1120, 825), (1120, 825), (1120, 825), (1120, 825)]
input_ids shape: torch.Size([10, 1297])
attention_mask shape: torch.Size([10, 1297])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1297


===== Batch 1488 =====
QIDs: [637, 637, 637, 637, 637, 637, 637, 637, 637, 637]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [225, 106, 207, 225, 225, 211, 219, 95, 211, 223]
image sizes: [(358, 399), (358, 399), (358, 399), (358, 399), (358, 399), (358, 399), (358, 399), (358, 399), (358, 399), (358, 399)]
input_ids shape: torch.Size([10, 339])
attention_mask shape: torch.Size([10, 339])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 339


===== Batch 1489 =====
QIDs: [135, 135, 135, 135, 135, 135, 135, 135, 135, 135]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [117, 82, 110, 108, 111, 111, 92, 58, 101, 134]
image sizes: [(1242, 614), (1242, 614), (1242, 614), (1242, 614), (1242, 614), (1242, 614), (1242, 614), (1242, 614), (1242, 614), (1242, 614)]
input_ids shape: torch.Size([10, 1048])
attention_mask shape: torch.Size([10, 1048])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 1048


===== Batch 1490 =====
QIDs: [664, 664, 664, 664, 664, 664, 664, 664, 664, 664]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [384, 217, 376, 391, 345, 368, 392, 168, 393, 399]
image sizes: [(484, 268), (484, 268), (484, 268), (484, 268), (484, 268), (484, 268), (484, 268), (484, 268), (484, 268), (484, 268)]
input_ids shape: torch.Size([10, 422])
attention_mask shape: torch.Size([10, 422])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 422


===== Batch 0 =====
QIDs: [669, 669, 669, 669, 669, 669, 669, 669, 669, 669]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [531, 245, 491, 536, 450, 520, 529, 166, 561, 567]
image sizes: [(276, 269), (276, 269), (276, 269), (276, 269), (276, 269), (276, 269), (276, 269), (276, 269), (276, 269), (276, 269)]
input_ids shape: torch.Size([10, 394])
attention_mask shape: torch.Size([10, 394])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 394


===== Batch 1 =====
QIDs: [1356, 1356, 1356, 1356, 1356, 1356, 1356, 1356, 1356, 1356]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [186, 105, 157, 191, 129, 162, 167, 101, 177, 233]
image sizes: [(490, 600), (490, 600), (490, 600), (490, 600), (490, 600), (490, 600), (490, 600), (490, 600), (490, 600), (490, 600)]
input_ids shape: torch.Size([10, 500])
attention_mask shape: torch.Size([10, 500])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 500


===== Batch 2 =====
QIDs: [677, 677, 677, 677, 677, 677, 677, 677, 677, 677]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [486, 196, 440, 468, 371, 417, 438, 141, 438, 449]
image sizes: [(488, 532), (488, 532), (488, 532), (488, 532), (488, 532), (488, 532), (488, 532), (488, 532), (488, 532), (488, 532)]
input_ids shape: torch.Size([10, 594])
attention_mask shape: torch.Size([10, 594])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 594


===== Batch 3 =====
QIDs: [736, 736, 736, 736, 736, 736, 736, 736, 736, 736]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [330, 168, 338, 354, 305, 328, 266, 120, 387, 368]
image sizes: [(470, 366), (470, 366), (470, 366), (470, 366), (470, 366), (470, 366), (470, 366), (470, 366), (470, 366), (470, 366)]
input_ids shape: torch.Size([10, 417])
attention_mask shape: torch.Size([10, 417])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 417


===== Batch 4 =====
QIDs: [52, 52, 52, 52, 52, 52, 52, 52, 52, 52]
LANGs: ['EN', 'KO', 'SR', 'HU', 'AR', 'CS', 'TH', 'ZH', 'RU', 'VI']
text lens: [240, 130, 236, 223, 228, 213, 207, 94, 232, 216]
image sizes: [(1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560), (1920, 2560)]
input_ids shape: torch.Size([10, 6432])
attention_mask shape: torch.Size([10, 6432])
vision_start count = 10, vision_end count = 10
NaN in input_ids: False
Sequence length = 6432
[ERROR] Batch 4 crash!
Exception: OutOfMemoryError('CUDA out of memory. Tried to allocate 3.31 GiB. GPU 0 has a total capacity of 139.81 GiB of which 1.10 GiB is free. Including non-PyTorch memory, this process has 138.71 GiB memory in use. Of the allocated memory 131.43 GiB is allocated by PyTorch, and 6.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)')
First 50 input_ids: [151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643]
vision token positions: [66, 6346]
Sample text preview: <image 1> Which nutrient seems to be absent given the signs of deficiency in this brassica seedling?\nA. Zinc\nB. Potassium\nC. Magnesium\nD. Molybdenum\nE. Calcium\nF. Sulfur\nG. Don't know and don't want to guess\nH. Iron\nI. Nitrogen\nJ. Phosphate
